{"posts":[{"title":"CNI工作流程","text":"CNI 即容器网络接口，通过 CNI 能够使 K8s 支持不同的网络模式。 CNI 常见的实现模式大致分为两种: overlay: 通过隧道打通网络，不依赖底层网络，如 calico underlay: 通过底层打通网络，强依赖底层网络，如 macvlan CNI 如何使用CNI 的实现通常需要两个部分: CNI 二进制文件去配置 Pod 的网卡和 IP DaemonSet 进程去管理 Pod 之间的网络打通 配置 CNI 配置文件(/etc/cni/net.d/*.conf) 安装 CNI 二进制文件(/opt/cni/bin/*) 节点上创建 Pod kubelet 根据 CNI 配置文件执行 CNI 二进制文件 Pod 网络配置完成","link":"/2024/03/30/CNI%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"},{"title":"CSI工作流程","text":"CSI 即容器存储接口，借助 CSI 就可以很容易的给工作负载提供存储使用。K8s 的存储插件又分为 in-tree 和 out-of-tree 两种类型，前者是与 K8s 主库共同迭代维护，后者则是独立维护的存储插件。 out-of-tree 类型的插件则是通过 gRPC 与 K8s 的组件进行交互，为了简化 CSI 的开发与部署，K8s 也提供了多个 sidecar 组件: node-driver-registrar: 监听来自 kubelet 的 gRPC 请求，从 CSI driver 获取驱动程序信息（通过 NodeGetInfo 方法），并使用 kubelet 插件注册机制在该节点上的 kubelet 中对其进行注册 provisioner: 监听来自 kube-apiserver 的 gRPC 请求，监听 PVC 的创建和删除，调用 CSI driver 创建和删除 PV（通过 CreateVolume 和 Delete Volume）方法 attacher: 监听来自 kube-apiserver 的 gRPC 请求，监听 volumeAttachment 对象并触发 CSI 执行 ControllerPublishVolume 和 ControllerUnpublishVolume 的操作 resizer: 监听来自 kube-apiserver 的 gRPC 请求，监听 PVC 的修改，调用 CSI Controller 执行 ExpandVolume 方法，来调整 Volume 的大小 livenessProbe: 检查 CSI 程序的健康状态，如不健康则会进行重启 CSI 工作流程CSI 的工作流分为三个阶段: Provision/Delete Attach/Detach Mount/Unmount 这三个阶段会用到 Sidecar 组件，也会用到 K8s 的 PV Controller 和 AD Controller 组件 当然并不是所有的 CSI 都会经历这三个阶段，如 NFS CSI 的工作流就没有涉及 volumeAttachment Privision 阶段在此阶段，Sidecar provisioner 和 PV Controller 都会监听 PVC 资源 当 PV Controller 观察到新的 PVC 被创建时，就会去判断是否有与之匹配的 in-tree 插件，如果没有则判定为 out-of-tree，并为该 PVC 添加 annotation provisioner 观察到 PVC 的 annotation 与自己的 CSI 是相匹配的时候，就会去调用 CreateVolume 方法 当 CreateVolume 调用返回成功时，provisioner 就会创建 PV PV Controller 监听到该 PV 时，就会将其与 PVC 做绑定 Attach 阶段在此阶段，会将数据卷附在一个节点上 AD Controller 监听到 Pod 被调度到某个节点后，会调用 in-tree 内部接口创建 volumeAttachment 资源 attacher 监听到 volumeAttachment 就会调用 ControllerPublishVolume 接口 当接口返回成功时就会将 volumeAttachment 资源的 status.attached 设置为 true Mount 阶段在此阶段，会将数据卷挂载到 Pod 上 kubelet 观察到 volumeAttachment 资源的 status.attached 设置为 true 时，就会调用 in-tree 内部接口进行实际的卷挂载操作 Unmount 阶段在此阶段，会将数据卷从 Pod 上取消挂载 kubelet 监听到节点上的 Pod 被删除，就会调用 in-tree 内部接口进行实际的卷卸载操作 Detach 阶段在此阶段，会将对应的 volumeAttachment 资源删除 attacher 会讲被删除 Pod 对应的 volumeAttachment 进行删除 AD Controller 监听到 volumeAttachment 被删除后，会去更新节点的 status.volumesInUse，将对应的卷信息摘除 Delete 阶段在此阶段，会判断 PV 的回收策略进行不同的操作 provisioner 观察 PV 的 persistentVolumeReclaimPolicy，如果为 Retain 则保留，Delete 则删除","link":"/2024/03/30/CSI%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"},{"title":"CoreDNS和NodeLocalDNS的域名解析","text":"在 K8s 中，DNS 的解析主要用这两个工具: CoreDNS: 主要负责集群内部域名解析 NodeLocalDNS: 提供 DNS 缓存 首先看一下集群中节点的 /etc/resolv.conf 配置文件 123456789101112131415# 指定了搜索域，当使用域名解析主机名时，如果主机名没有完全限定，系统会依次尝试在指定的搜索域中追加搜索后缀进行解析# 例如解析 test，则会尝试解析 test.default.svc.cluster.local 和 test.svc.cluster.localsearch default.svc.cluster.local svc.cluster.local# nodelocaldns 服务器的地址，如果集群中没有 nodelocaldns，那么这个地址就会替换成 CoreDNS 的 SVC ClusterIPnameserver 169.254.25.10# 其他 DNS 服务器地址nameserver 192.168.0.5nameserver 223.5.5.5# 解析的域名最多包涵 2 个点，最多超时 2s，最多重试 2 次# 假设解析a，会尝试追加 default.svc.cluster.local 和 svc.cluster.local# 假设解析a.b，则不会，除非 ndots &gt; 2options ndots:2 timeout:2 attempts:2 在 K8s 中，workload 的 dnsPolicy 有四种类型: ClusterFirst: 与配置的集群域后缀不匹配的任何 DNS 查询（例如 “","link":"/2024/03/30/CoreDNS%E5%92%8CNodeLocalDNS%E7%9A%84%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/"},{"title":"Docker 部署 NeuVector","text":"Docker 部署 NeuVector 适用于做简单的测试。 部署 allinone 容器： 12345678910111213141516171819docker run -d --name allinone \\--pid=host \\--privileged \\ -e CLUSTER_JOIN_ADDR=172.16.0.1 \\ -e NV_PLATFORM_INFO=platform=Docker \\ -e CTRL_PERSIST_CONFIG=1 \\ -p 18300:18300 \\ -p 18301:18301 \\ -p 18400:18400 \\ -p 18401:18401 \\ -p 10443:10443 \\ -p 18301:18301/udp \\ -p 8443:8443 \\ -v /lib/modules:/lib/modules:ro \\ -v /var/neuvector:/var/neuvector \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\neuvector/allinone:5.4.0 部署 scanner 容器： 12345docker run -td --name scanner \\-e CLUSTER_JOIN_ADDR=172.16.0.1 \\-e NV_PLATFORM_INFO=platform=Docker \\-p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro \\registry.cn-hangzhou.aliyuncs.com/rancher/mirrored-neuvector-scanner:latest","link":"/2024/11/07/Docker-%E9%83%A8%E7%BD%B2-NeuVector/"},{"title":"Elasticsearch 使用随记","text":"部署 ECK Operator： 12kubectl create -f https://download.elastic.co/downloads/eck/2.16.1/crds.yamlkubectl apply -f https://download.elastic.co/downloads/eck/2.16.1/operator.yaml 部署单点 ES 集群： 12345678910111213cat &lt;&lt;EOF | kubectl apply -f -apiVersion: elasticsearch.k8s.elastic.co/v1kind: Elasticsearchmetadata: name: quickstartspec: version: 8.17.1 nodeSets: - name: default count: 1 config: node.store.allow_mmap: falseEOF 对集群发起请求： 12PASSWORD=$(kubectl get secret quickstart-es-elastic-user -o go-template='{{.data.elastic | base64decode}}')curl -u &quot;elastic:$PASSWORD&quot; -k &quot;https://quickstart-es-http:9200&quot; 部署高可用 ES 集群： 123456789101112131415161718192021cat &lt;&lt;EOF | kubectl apply -f -apiVersion: elasticsearch.k8s.elastic.co/v1kind: Elasticsearchmetadata: name: quickstartspec: version: 8.17.1 nodeSets: - name: data-nodes count: 3 config: node.store.allow_mmap: false index.number_of_replicas: 1 - name: master-nodes count: 3 config: node.master: true node.data: false node.ingest: false heap.size: 2gEOF","link":"/2025/01/31/Elasticsearch-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"ETCD 出现高碎片率事件解析","text":"集群频繁触发 etcdDatabaseHighFragmentationRatio 告警, PrometheusRule 内容如下： 123456789101112131415- alert: etcdDatabaseHighFragmentationRatio annotations: description: 'etcd cluster &quot;{{ $labels.job }}&quot;: database size in use on instance {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.' runbook_url: https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation summary: etcd database size in use is less than 50% of the actual allocated storage. expr: (last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes{job=~&quot;.*etcd.*&quot;}[5m]) / last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~&quot;.*etcd.*&quot;}[5m])) &lt; 0.5 and etcd_mvcc_db_total_size_in_use_in_bytes{job=~&quot;.*etcd.*&quot;} &gt; 104857600 for: 10m labels: severity: warning 相关指标: etcd_server_quota_backend_bytes：当前后端存储配额大小（字节），默认为 2GB etcd_mvcc_db_total_size_in_bytes：物理分配的底层数据库总大小（字节），包含了数据（如 keyspace）和碎片，即 DB SIZE etcd_mvcc_db_total_size_in_use_in_bytes：逻辑上正在使用的底层数据库的总大小（以字节为单位），不包含碎片 也就是说 quota-backend-bytes 配置后，etcd_mvcc_db_total_size_in_bytes 的大小并不会根据这个值而变化，会变的是 etcd_server_quota_backend_bytes，etcd_mvcc_db_total_size_in_bytes 的值指的是 DB SIZE，可以通过以下方式获取 DB SIZE 12ls -lrth ${etcd-data-dir}/member/snapetcdctl endpoint status -w table 为什么会产生碎片? ETCD 支持多版本并发控制(MVCC)，同时会精确记录其 keyspace 的历史 压缩操作是清除历史记录的唯一方法，通常用 –auto-compaction-mode 和 –auto-compaction-retention 来实现自动压缩 但压缩操作后的空闲空间并不会真正在文件系统中释放，而是会被 ETCD 标记为可使用的空闲空间，也就是说压缩操作后仍然会占用磁盘空间 要真正释放，就需要进行碎片整理，即 etcdctl defrag 1etcdctl defrag 那么针对 etcdDatabaseHighFragmentationRatio 告警的触发，要怎么判断需不需要进行碎片清理? 通过 etcd_server_quota_backend_bytes 指标查看实际配额 通过指标 etcd_mvcc_db_total_size_in_bytes 或者命令检查 DB SIZE，看是否真的很大且接近于 quota-backend-bytes，如果不是则无需担心 如果是，获取每种资源的数量，查看是什么资源导致 DB Size 这么大，然后通过碎片清理尝试释放空间，如果释放后仍接近于 quota-backend-bytes，那么需要考虑增加配额 12etcdctl get /registry --prefix --keys-only | grep -v ^$ | awk -F '/' '{ h[$3]++ } END {for (k in h) print h[k], k}' | sort -nretcdctl defrag","link":"/2024/09/14/ETCD-%E5%87%BA%E7%8E%B0%E9%AB%98%E7%A2%8E%E7%89%87%E7%8E%87%E4%BA%8B%E4%BB%B6%E8%A7%A3%E6%9E%90/"},{"title":"Harvester 使用随记","text":"Harvester 是 SUSE 的一款开源 HCI 解决方案，基于 Kubernetes 的现代化 HCI 平台，旨在简化虚拟化管理，同时与云原生技术无缝集成。 Harvester 安装且对接 Rancher 后，就可以直接在 Rancher 进行虚拟机的创建/删除等动作。 如果 Harvester 集群只有一个节点，因为默认的 StorageClass 副本为 3，需要创建一个 StorageClass 用于代替。 1234567891011121314151617cat &lt;&lt;EOF | kubectl apply -f -allowVolumeExpansion: trueapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: annotations: storageclass.beta.kubernetes.io/is-default-class: &quot;true&quot; storageclass.kubernetes.io/is-default-class: &quot;true&quot; name: harvester-longhorn-single-replicasparameters: migratable: &quot;true&quot; numberOfReplicas: &quot;1&quot; staleReplicaTimeout: &quot;30&quot;provisioner: driver.longhorn.ioreclaimPolicy: DeletevolumeBindingMode: ImmediateEOF 上传 ISO 文件虚拟机的创建的前提是需要有 ISO 文件，Harvester 支持本地上传和通过互联网下载两种方式： 清华 Ubuntu Cloud IMG 创建 Cluster Network需要给 Harvester 节点准备多一张网卡给虚拟机用： 创建 VM Network Bridge使用刚刚创建的 Cluster Network： 创建 VM Cloud Config TemplateUser Data： 123456789101112131415161718192021222324252627282930#cloud-configpackage_update: truepackages: - qemu-guest-agentwrite_files: - path: /etc/sysctl.conf permissions: 0644 owner: root content: | net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1runcmd: - systemctl enable --now qemu-guest-agent.service - systemctl disable --now systemd-resolved.service - rm -rf /etc/resolv.conf - echo 'nameserver 172.16.16.12' &gt; /etc/resolv.conf - timedatectl set-timezone Asia/Shanghai - sysctl -pssh_authorized_keys: - ssh-rsa xxxsystem_info: default_user: name: ubuntu groups: users lock_passwd: false sudo: ALL=(ALL) NOPASSWD:ALL plain_text_passwd: 'ubuntu' homedir: /home/ubuntu shell: /bin/bash Network Data： 1234567891011network: version: 2 ethernets: enp1s0: dhcp4: false dhcp6: false addresses: - 172.16.16.132/24 routes: - to: default via: 172.16.16.1 创建 VM 创建后，即可 ssh 到虚拟机中： 使用 Harvester 创建 RKE2 集群","link":"/2025/01/14/Harvester-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"Istio 使用随记","text":"Istio 是一种开源服务网格，使用代理拦截所有网络流量，可根据配置提供广泛的应用程序感知功能。 常见的 CRD： VirtualService：定义服务间或服务至外部流量的路由规则，支持请求匹配、路由选择、流量分割、重试、超时等策略。 DestinationRule：定义目标服务的流量策略，比如负载均衡、连接池、TLS 配置等。每个目标服务的流量策略通过 DestinationRule 配置后，对所有请求生效。 Gateway：配置边缘网关的入口流量规则，例如定义外部流量如何进入服务网格。它可以用于 HTTP、HTTPS、TLS 和 TCP 流量。 ServiceEntry：将外部服务引入到 Istio 网格中，允许 Istio 管理和监控这些服务的流量。例如，可以使用 ServiceEntry 来引入外部 API，使网格中的服务可以透明地与外部服务交互。 创建一个 Nginx，用于测试： 12kubectl create deployment nginx --image=nginx:mainlinekubectl expose deployment nginx --port=80 创建 Gateway： selector 匹配 istio-ingressgateway 部署的 Pod，通过该网关来接收外部流量。 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: Gatewaymetadata: name: http-gateway namespace: istio-systemspec: selector: # select the istio ingressgateway istio: ingressgateway servers: - hosts: - test.nginx.com port: name: http number: 80 protocol: HTTP 创建 VirtualService： hosts 字段指定了可以通过 test.nginx.com 主机名访问该服务的外部请求，流量通过定义的 Gateway 进入。VirtualService 的 route 字段将流量路由到 nginx.default.svc.cluster.local。 1234567891011121314151617apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: nginx-virtualservice namespace: defaultspec: gateways: # select the gateway created above - http-gateway.istio-system.svc.cluster.local hosts: - test.nginx.com http: - route: - destination: host: nginx.default.svc.cluster.local port: number: 80 创建 DestinationRule： trafficPolicy 中定义了负载均衡策略为 LEAST_CONN，即最小连接数策略。DestinationRule 的 host 必须匹配 VirtualService 中的 destination。 12345678910apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: nginx-destination-rule namespace: defaultspec: host: nginx.default.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_CONN 通过 Istio Ingress Gateway 访问 Nginx： 1curl -H &quot;Host: test.nginx.com&quot; http://&lt;istio-ingressgateway-cluster-ip&gt;/","link":"/2024/11/08/Istio-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"Docker 部署 Rancher 指定镜像仓库","text":"docker 启动的 rancher 默认会走公网获取镜像，添加了 CATTLE_SYSTEM_DEFAULT_REGISTRY 的话，helm-operation 使用的 rancher/shell 等还是会走到公网，如果要所有镜像都是用 private registry，可以通过下面的方式。 准备 private registry 认证的配置文件和 k3s 配置文件： 123456789101112131415mkdir -p /etc/rancher/k3scat &lt;&lt;EOF &gt; /etc/rancher/k3s/registries.yamlconfigs: &quot;harbor.warnerchen.com&quot;: auth: username: xxx password: xxx tls: insecure_skip_verify: trueEOFcat &lt;&lt;EOF &gt; /etc/rancher/k3s/config.yamlsystem-default-registry: harbor.warnerchen.comEOF 启动 rancher： 123456789docker run -d --restart=unless-stopped --name rancher \\ -v /var/lib/rancher:/var/lib/rancher \\ -v /etc/rancher/k3s/registries.yaml:/etc/rancher/k3s/registries.yaml:ro \\ -v /etc/rancher/k3s/config.yaml:/etc/rancher/k3s/config.yaml:ro \\ -e CATTLE_BOOTSTRAP_PASSWORD=xxx \\ -e CATTLE_SYSTEM_DEFAULT_REGISTRY=harbor.warnerchen.com \\ -p 80:80 -p 443:443 \\ --privileged \\ harbor.warnerchen.com/prime/rancher:v2.7.15-ent","link":"/2024/11/28/Docker-%E9%83%A8%E7%BD%B2-Rancher-%E6%8C%87%E5%AE%9A%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93/"},{"title":"NeuVector 的 Zero Draft 与 Basic 模式","text":"NeuVector 有 Zero-drift 和 Basic 两种模式，而 Zero-drift 模式是默认模式，根据一个 Nginx 来作为测试案例，观察两种模式下 Process Profile Rules 的效果。 Zero-drift 模式Discover在 Discover 下，NV 会自动学习容器运行中的进程并生成 Process Profile Rules 若发现未授权进程，会触发告警 但不自动生成 File Access Rules，只有在系统默认监控的目录内进行文件增删改查等动作才会触发告警 MonitorMonitor 与 Discover 类似，任何不符合 Process Profile Rules 的活动都会发出警告，但不会阻止操作 Protect在 Protect 下，对未被授权的进程和文件活动进行强制阻止并发出告警 123root@test:~# kubectl exec -it nginx-57b989859-bbh9j -- bashexec /usr/bin/bash: operation not permittedcommand terminated with exit code 1 Basic 模式Discover与 Zero-drift 类似，会学习容器进程，但不会对容器中的新进程发出告警，反而会自动学习这些新进程，也就是说在该模式下所有的进程活动都是被允许的；文件监控的规则与 Zero-drift 相同不会自动生成 Monitor不会学习新进程，任何未被允许的进程活动都会触发告警 Protect行为与 Zero-drift Protect 相同，禁止任何未被允许的进程活动，并触发告警 结论NeuVector 的 Zero-drift 模式和 Basic 模式主要区别在于 Discover 模式下的行为。Zero-drift 更为严格，确保容器仅运行镜像中定义的进程，不允许任何新的进程运行。而 Basic 模式则更灵活，允许 NeuVector 学习容器内的新进程，并根据这些新活动自动生成规则。两种模式在 Monitor 和 Protect 模式下都会对容器的进程和文件活动进行监控和防护。 Zero-drift 模式 Discover：Zero-drift 会自动分析和学习容器镜像中允许的进程，并自动生成 Process Profile Rules，确保容器只运行在镜像内定义的进程。如果容器内有其他非镜像定义的进程启动，NeuVector 会发出告警。文件访问监控则只针对 NV 默认的监控目录，且不会自动生成 File Access Rules。这种模式更严格地控制容器行为，尤其适用于需要高安全性的场景。 Monitor：和 Discover 相似，NeuVector 会持续监控容器内的进程和文件活动，但不会阻止活动，只会发出警告。如果有任何与规则不匹配的进程或文件操作，都会触发告警。 Protect：在 Protect 下，Zero-drift 会阻止任何与规则不匹配的进程或文件操作，确保容器运行环境的安全。如果发现未授权的进程或文件活动，NeuVector 会立即阻止并发出告警。 Basic 模式 Discover：Basic 模式也会自动学习容器运行的进程，并生成 Process Profile Rules，但与 Zero-drift 模式不同，Basic 模式不会限制新进程的运行，即便有非镜像定义的进程启动，NeuVector 也不会立即发出告警，反而会自动学习这些新进程，创建相应的规则。文件监控的规则则与 Zero-drift 模式相同，依赖于系统默认的监控目录。该模式更灵活，适用于需要动态调整容器进程的场景。 Monitor：在 Monitor 下，Basic 模式不再继续学习新的进程活动，任何未经授权的进程操作都会触发告警。 Protect：在 Protect 下，Basic 模式和 Zero-drift 模式的行为一致，NeuVector 会阻止任何未被授权的进程和文件活动，并发出告警。","link":"/2024/10/21/NeuVector-%E7%9A%84-Zero-Draft-%E4%B8%8E-Basic-%E6%A8%A1%E5%BC%8F/"},{"title":"Ollama + Open WebUI 使用随记","text":"Ollama 是简化本地设备上大型语言模型 (LLM) 安装和管理的平台，而 Open WebUI 是 Ollama LLM 运行程序的可扩展网络用户界面。 Ollama在 Kubernetes 上部署 Ollama： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566helm repo add ollama-helm https://otwld.github.io/ollama-helm/helm repo updatecat &lt;&lt;EOF &gt; ollama-values.yamlreplicaCount: 1knative: enabled: falseimage: repository: ollama/ollama pullPolicy: IfNotPresentollama: gpu: enabled: true type: 'nvidia' number: 1 nvidiaResource: &quot;nvidia.com/gpu&quot; mig: enabled: false models: pull: [] run: [] create: [] insecure: falseserviceAccount: create: true automount: trueruntimeClassName: &quot;nvidia&quot;service: type: ClusterIP port: 11434 nodePort: 31434 loadBalancerIP:ingress: enabled: falselivenessProbe: enabled: true path: / initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 6 successThreshold: 1readinessProbe: enabled: true path: / initialDelaySeconds: 30 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 6 successThreshold: 1autoscaling: enabled: falsepersistentVolume: enabled: true accessModes: - ReadWriteOnce size: 10Gi storageClass: &quot;longhorn&quot;updateStrategy: type: &quot;Recreate&quot;hostIPC: falsehostPID: falsehostNetwork: falseEOFhelm -n ollama upgrade --install ollama ollama-helm/ollama -f ollama-values.yaml --version 1.7.0 下载 deepseek-r1:1.5b 模型并运行： 调用 API 运行： 1234567891011121314151617181920212223242526272829303132333435root@rke2-cilium-01:~/ai-related# curl -s -X POST http://&lt;ollama-svc-ip&gt;:11434/api/generate -d '{ &quot;model&quot;: &quot;deepseek-r1:1.5b&quot;, &quot;prompt&quot;: &quot;你好?&quot;, &quot;stream&quot;: false }' | jq{ &quot;model&quot;: &quot;deepseek-r1:1.5b&quot;, &quot;created_at&quot;: &quot;2025-02-27T04:46:43.19616833Z&quot;, &quot;response&quot;: &quot;&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\n您好！很高兴为您服务。请问有什么可以帮助您的？&quot;, &quot;done&quot;: true, &quot;done_reason&quot;: &quot;stop&quot;, &quot;context&quot;: [ 151644, 108386, 30, 151645, 151648, 271, 151649, 271, 111308, 6313, 112169, 102804, 47874, 1773, 109194, 104139, 111728, 101214, 11319 ], &quot;total_duration&quot;: 3243229672, &quot;load_duration&quot;: 2905513629, &quot;prompt_eval_count&quot;: 5, &quot;prompt_eval_duration&quot;: 82000000, &quot;eval_count&quot;: 16, &quot;eval_duration&quot;: 253000000} Open WebUI在 Kubernetes 上部署 Open WebUI： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152helm repo add open-webui https://helm.openwebui.com/helm repo updatecat &lt;&lt;EOF &gt; open-webui-values.yamlollama: enabled: false fullnameOverride: &quot;open-webui-ollama&quot;pipelines: enabled: falsetika: enabled: falseollamaUrls: - http://ollama.ollama.svc.cluster.local:11434ollamaUrlsFromExtraEnv: falsewebsocket: enabled: falseredis-cluster: enabled: falseclusterDomain: cluster.localreplicaCount: 1image: repository: harbor.warnerchen.com/open-webui/open-webui pullPolicy: &quot;IfNotPresent&quot;serviceAccount: enable: true automountServiceAccountToken: falsemanagedCertificate: enabled: falseingress: enabled: true class: &quot;nginx&quot; tls: false host: &quot;open-webui.warnerchen.com&quot;persistence: enabled: true size: 2Gi accessModes: - ReadWriteOnce storageClass: &quot;longhorn&quot;service: type: ClusterIP port: 80 containerPort: 8080openaiBaseApiUrl: &quot;https://api.openai.com/v1&quot;extraEnvVars: - name: OPENAI_API_KEY value: &quot;0p3n-w3bu!&quot; - name: WEBUI_AUTH value: &quot;False&quot;EOFhelm -n ollama upgrade --install open-webui open-webui/open-webui -f open-webui-values.yaml --version 5.20.0 访问 Open WebUI：","link":"/2025/02/27/Ollama-Open-WebUI-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"K8s给命名空间设置专享节点","text":"是指在特定命名空间下部署的 Pod 都会被调度到指定标签的节点上，这个功能需要在 kube-apiserver 添加 PodNodeSelector 参数： 1234567891011apiVersion: v1kind: Podmetadata: labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system... - --enable-admission-plugins=NodeRestriction,PodNodeSelector... 然后给命名空间添加 annotation 12345apiVersion: v1kind: Namespacemetadata: annotations: scheduler.alpha.kubernetes.io/node-selector: test.io/app=true","link":"/2024/03/30/K8s%E7%BB%99%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%E8%AE%BE%E7%BD%AE%E4%B8%93%E4%BA%AB%E8%8A%82%E7%82%B9/"},{"title":"Pod是怎么诞生的","text":"K8s 本身不提供容器创建的实现，容器的创建是通过 CRI 接口调用外部插件实现的，常见的 CRI 有这几种: docker containerd cri-dockerd Pod 是如何被创建出来的 客户端通过 kubectl 等将创建 pod 的请求发送给 kube-apiserver kube-apiserver 将 pod 信息写入 etcd，etcd 将写入的结果返回给 kube-apiserver，然后 kube-apiserver 再返回给客户端 kube-scheduler 通过 kube-apiserver 的 watch 接口，获取到未被调度的 pod 信息，根据调度算法选择集群内的一个节点，然后将节点信息发送给 kube-apiserver kube-apiserver 将这个 pod 和节点的绑定信息写入到 etcd，etcd 将结果返回给 kube-apiserver kubelet 通过 kube-apiserver 的 watch 接口，获取到本节点有创建 pod 的信息，然后会调用 CRI 创建容器，并将 pod 的运行状态发送给 kube-apiserver kube-apiserver 将 pod 状态信息更新到 etcd","link":"/2024/03/11/Pod%E6%98%AF%E6%80%8E%E4%B9%88%E8%AF%9E%E7%94%9F%E7%9A%84/"},{"title":"RKE1 ETCD 出现 request cluster id mismatch 问题修复记录","text":"当集群中三个 Control Plane 节点的 ETCD 出现 request cluster ID mismatch 问题时，可以保留一个 ETCD 实例通过 --force-new-cluster 参数重建集群，然后再将其他两个节点的 ETCD 实例加入集群。 通过 docker rename 的方式保留第二/三台 Control Plane 节点的 ETCD 12docker stop etcddocker rename etcd etcd-old 备份第一台 Control Plane 节点的 ETCD 启动命令 1234docker run --rm -v /var/run/docker.sock:/var/run/docker.sock assaflavie/runlike:latest etcd# 以下为 ETCD 启动命令docker run --name=etcd --hostname=test001 --env=ETCDCTL_API=3 --env=ETCDCTL_CACERT=/etc/kubernetes/ssl/kube-ca.pem --env=ETCDCTL_CERT=/etc/kubernetes/ssl/kube-etcd-172-16-0-106.pem --env=ETCDCTL_KEY=/etc/kubernetes/ssl/kube-etcd-172-16-0-106-key.pem --env=ETCDCTL_ENDPOINTS=https://127.0.0.1:2379 --env=ETCD_UNSUPPORTED_ARCH=x86_64 --volume=/var/lib/etcd:/var/lib/rancher/etcd/:z --volume=/etc/kubernetes:/etc/kubernetes:z --network=host --restart=always --label='io.rancher.rke.container.name=etcd' --runtime=runc --detach=true registry.cn-hangzhou.aliyuncs.com/rancher/mirrored-coreos-etcd:v3.4.15-rancher1 /usr/local/bin/etcd --listen-peer-urls=https://0.0.0.0:2380 --trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem --key-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106-key.pem --peer-cert-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106.pem --peer-key-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106-key.pem --peer-client-cert-auth=true --initial-advertise-peer-urls=https://172.16.0.106:2380 --heartbeat-interval=500 --cert-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106.pem --advertise-client-urls=https://172.16.0.106:2379 --cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --initial-cluster=etcd-rke1-server-0=https://172.16.0.106:2380,etcd-rke1-server-1=https://172.16.0.105:2380,etcd-rke1-server-2=https://172.16.0.104:2380 --initial-cluster-state=new --client-cert-auth=true --listen-client-urls=https://0.0.0.0:2379 --initial-cluster-token=etcd-cluster-1 --name=etcd-rke1-server-0 --enable-v2=true --election-timeout=5000 --data-dir=/var/lib/rancher/etcd/ 停止第一台 Control Plane 节点的 ETCD 12docker stop etcddocker rename etcd etcd-old 修改先前保存的 ETCD 启动命令，在 initial-cluster 参数中删除第二/三台 Control Plane 节点的 ETCD 信息，并在最后添加 --force-new-cluster 参数，然后执行，如果启动后仍然报 request cluster ID mismatch 的错误，可以重复多几次 1docker run --name=etcd --hostname=test001 --env=ETCDCTL_API=3 --env=ETCDCTL_CACERT=/etc/kubernetes/ssl/kube-ca.pem --env=ETCDCTL_CERT=/etc/kubernetes/ssl/kube-etcd-172-16-0-106.pem --env=ETCDCTL_KEY=/etc/kubernetes/ssl/kube-etcd-172-16-0-106-key.pem --env=ETCDCTL_ENDPOINTS=https://127.0.0.1:2379 --env=ETCD_UNSUPPORTED_ARCH=x86_64 --volume=/var/lib/etcd:/var/lib/rancher/etcd/:z --volume=/etc/kubernetes:/etc/kubernetes:z --network=host --restart=always --label='io.rancher.rke.container.name=etcd' --runtime=runc --detach=true registry.cn-hangzhou.aliyuncs.com/rancher/mirrored-coreos-etcd:v3.4.15-rancher1 /usr/local/bin/etcd --listen-peer-urls=https://0.0.0.0:2380 --trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem --key-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106-key.pem --peer-cert-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106.pem --peer-key-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106-key.pem --peer-client-cert-auth=true --initial-advertise-peer-urls=https://172.16.0.106:2380 --heartbeat-interval=500 --cert-file=/etc/kubernetes/ssl/kube-etcd-172-16-0-106.pem --advertise-client-urls=https://172.16.0.106:2379 --cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --initial-cluster=etcd-rke1-server-0=https://172.16.0.106:2380 --initial-cluster-state=new --client-cert-auth=true --listen-client-urls=https://0.0.0.0:2379 --initial-cluster-token=etcd-cluster-1 --name=etcd-rke1-server-0 --enable-v2=true --election-timeout=5000 --data-dir=/var/lib/rancher/etcd/ --force-new-cluster 启动完毕后检查 ETCD 集群状态 12docker exec -it -e ETCDCTL_API=3 etcd etcdctl member list -w tabledocker exec -it -e ETCDCTL_API=3 etcd etcdctl endpoint status --cluster -w table 在第一台 Control Plane 节点上添加 ETCD Member 123456789MEMBER_IP=172.16.0.105MEMBER_NAME=&quot;rke1-server-1&quot;docker exec -it etcd etcdctl member add etcd-$MEMBER_NAME --peer-urls=https://$MEMBER_IP:2380# 执行完命令后，下面的配置需要保留，后续节点启动 ETCD 时需要使用ETCD_NAME=&quot;etcd-rke1-server-1&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-rke1-server-0=https://172.16.0.106:2380,etcd-rke1-server-1=https://172.16.0.105:2380&quot;ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://172.16.0.105:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;existing&quot; 然后在第二台 Control Plane 节点，进行恢复 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 备份数据mv /var/lib/etcd /var/lib/etcd_bak# 设置变量NODE_IP=172.16.0.105ETCD_IMAGE=registry.cn-hangzhou.aliyuncs.com/rancher/mirrored-coreos-etcd:v3.4.15-rancher1ETCD_NAME=&quot;etcd-rke1-server-1&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-rke1-server-0=https://172.16.0.106:2380,etcd-rke1-server-1=https://172.16.0.105:2380&quot;ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://172.16.0.105:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;existing&quot;# 启动 ETCDdocker run --name=etcd --hostname=`hostname` \\--env=&quot;ETCDCTL_API=3&quot; \\--env=&quot;ETCDCTL_CACERT=/etc/kubernetes/ssl/kube-ca.pem&quot; \\--env=&quot;ETCDCTL_CERT=/etc/kubernetes/ssl/kube-etcd-`echo $NODE_IP|sed 's/\\./-/g'`.pem&quot; \\--env=&quot;ETCDCTL_KEY=/etc/kubernetes/ssl/kube-etcd-`echo $NODE_IP|sed 's/\\./-/g'`-key.pem&quot; \\--env=&quot;ETCDCTL_ENDPOINTS=https://127.0.0.1:2379&quot; \\--env=&quot;ETCD_UNSUPPORTED_ARCH=x86_64&quot; \\--env=&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; \\--volume=&quot;/var/lib/etcd:/var/lib/rancher/etcd/:z&quot; \\--volume=&quot;/etc/kubernetes:/etc/kubernetes:z&quot; \\--network=host \\--restart=always \\--label io.rancher.rke.container.name=&quot;etcd&quot; \\--detach=true \\$ETCD_IMAGE \\/usr/local/bin/etcd \\--peer-client-cert-auth \\--client-cert-auth \\--peer-cert-file=/etc/kubernetes/ssl/kube-etcd-`echo $NODE_IP|sed 's/\\./-/g'`.pem \\--peer-key-file=/etc/kubernetes/ssl/kube-etcd-`echo $NODE_IP|sed 's/\\./-/g'`-key.pem \\--cert-file=/etc/kubernetes/ssl/kube-etcd-`echo $NODE_IP|sed 's/\\./-/g'`.pem \\--trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \\--initial-cluster-token=etcd-cluster-1 \\--peer-trusted-ca-file=/etc/kubernetes/ssl/kube-ca.pem \\--key-file=/etc/kubernetes/ssl/kube-etcd-`echo $NODE_IP|sed 's/\\./-/g'`-key.pem \\--data-dir=/var/lib/rancher/etcd/ \\--advertise-client-urls=https://$NODE_IP:2379 \\--listen-client-urls=https://0.0.0.0:2379 \\--listen-peer-urls=https://0.0.0.0:2380 \\--initial-advertise-peer-urls=https://$NODE_IP:2380 \\--election-timeout=5000 \\--heartbeat-interval=500 \\--name=$ETCD_NAME \\--initial-cluster=$ETCD_INITIAL_CLUSTER \\--initial-cluster-state=$ETCD_INITIAL_CLUSTER_STATE 启动完后检查状态，如果没问题则可以重复上面步骤添加第三台节点 12docker exec -it -e ETCDCTL_API=3 etcd etcdctl member list -w tabledocker exec -it -e ETCDCTL_API=3 etcd etcdctl endpoint status --cluster -w table 集群状态正常后，恢复第一台 Control Plane 节点的 etcd 1234docker stop etcddocker rename etcd etcd-restoredocker rename etcd-old etcddocker start etcd","link":"/2024/10/24/RKE1-ETCD-%E5%87%BA%E7%8E%B0-request-cluster-id-mismatch-%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D%E8%AE%B0%E5%BD%95/"},{"title":"NeuVector v5.4 为用户设置 Fed 角色","text":"NeuVector 从 v5.4 版本起，通过 Rancher 登录到 NeuVector 的用户无法在 NeuVector 被设置为 Fed 角色，需要在 Rancher 中设置对应的角色，从而映射到 NeuVector 中。 对于 NV 在 Primary 集群中的 SSO 映射： Global Role：get/nv-perm.all-permissions 映射到 fedReader 角色，*/nv-perm.all-permissions 映射到 fedAdmin 角色。 Cluster Role：get/nv-perm.all-permissions 映射到 reader 角色，*/nv-perm.all-permissions 映射到 admin 角色，get/nv-perm.all-permissions,nv-perm.fed 映射到 fedReader 角色，*/nv-perm.all-permissions,nv-perm.fed 映射到 fedAdmin 角色。 Project Role：没有项目角色会映射到 fedReader/fedAdmin 角色。 如果是非主 NV 集群，映射的 fedReader 角色会自动降级为 reader 角色，fedAdmin 角色会自动降级为 admin 角色。 Global Role如果 NV 部署在下游集群，由于 Global Role 不会自动同步到下游集群中，所以先要创建一个 Cluster Role，然后再通过 inheritedClusterRoles 字段引用。 以 fedAdmin 为例，创建 Cluster Role： 123456789101112131415161718192021222324252627administrative: falseapiVersion: management.cattle.io/v3builtin: falseclusterCreatorDefault: falsecontext: clusterdisplayName: NeuVector Fed Administrator Cluster Roleexternal: falsehidden: falsekind: RoleTemplatelocked: falsemetadata: name: cluster-role-neuvector-fed-adminprojectCreatorDefault: falseroleTemplateNames: []rules: - apiGroups: - api.neuvector.com resources: - nv-perm.all-permissions verbs: - '*' - apiGroups: - api.neuvector.com resources: - nv-perm.fed verbs: - '*' 创建 Global Role： 12345678910apiVersion: management.cattle.io/v3description: NeuVector Fed Administrator Global RoledisplayName: NeuVector Fed Administratorkind: GlobalRolemetadata: name: global-role-neuvector-fed-admininheritedClusterRoles: - cluster-role-neuvector-fed-adminnewUserDefault: falserules: [] 然后给用户授予 Global Role 权限： 在 NV 中可以看到角色为 fedAdmin： Cluster Role如果不通过 Global Role 给用户进行授权，可以直接在下游集群使用刚刚创建的 Cluster Role 进行授权：","link":"/2025/01/03/NeuVector-v5-4-%E4%B8%BA%E7%94%A8%E6%88%B7%E8%AE%BE%E7%BD%AE-Fed-%E8%A7%92%E8%89%B2/"},{"title":"Cert Manager 使用随记","text":"在 Kubernetes 中常用 Cert Manager 生成并管理自签名证书，常见的 CR 有👇 Issuer: 用于定义如何生成证书 ClusterIssuer: 用于定义如何生成集群级别的证书 Certificate: 用来请求和管理证书的主要资源 CertificateRequest: 是用于手动请求证书的资源 Order: 当使用 ACME 协议时会生成， Challenge: 是 ACME 协议中的一部分，用于表示 ACME 服务对域名所有权的验证 一个自签的简单实用例子自签名生成 CA 证书和私钥 123456789# 生成 CA 的私钥openssl genrsa -out ca.key 2048# 生成自签名的 CA 证书openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=nginx-ca&quot; -days 3650 -out ca.crt -extensions v3_ca -config &lt;(echo &quot;[ v3_ca ]&quot;; echo &quot;basicConstraints=CA:TRUE&quot;)kubectl create secret tls tls-nginx \\ --cert=ca.crt \\ --key=ca.key 通过 Issue 创建自签名 12345678910cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cert-manager.io/v1kind: Issuermetadata: name: nginx-issuer namespace: defaultspec: ca: secretName: tls-nginxEOF 通过 Certificate 创建证书请求 1234567891011121314151617cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cert-manager.io/v1kind: Certificatemetadata: name: nginx-certificate namespace: defaultspec: secretName: tls-nginx duration: 24h renewBefore: 12h commonName: nginx.warnerchen.io dnsNames: - nginx.warnerchen.io issuerRef: name: nginx-issuer kind: IssuerEOF 给 Ingress 挂载后，尝试请求","link":"/2024/09/07/Cert-Manager-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"RKE2 Calico 指定网卡","text":"calico-node 的 IP_AUTODETECTION_METHOD 默认使用 first-found，如果节点存在多张网卡的时候，可能导致 calico-node 绑定到错误的网卡上，导致跨节点网络不通。 可以通过修改 IP_AUTODETECTION_METHOD 为 interface 或者 cidrs 来指定 calico-node 绑定到指定网卡上。 12345678910111213141516171819cat &lt;&lt;EOF | kubectl apply -f -apiVersion: helm.cattle.io/v1kind: HelmChartConfigmetadata: name: rke2-calico namespace: kube-systemspec: valuesContent: |- global: cattle: clusterId: &quot;xxx&quot; installation: calicoNetwork: nodeAddressAutodetectionV4: interface: &quot;ens34&quot; # 也可以使用 cidrs，interface 和 cidrs 不能同时使用 cidrs: - &quot;172.16.16.0/24&quot;EOF","link":"/2024/11/27/RKE2-Calico-%E6%8C%87%E5%AE%9A%E7%BD%91%E5%8D%A1/"},{"title":"Harvester VM OOM 问题排查","text":"Harvester 版本：v1.3.2 问题现象：环境中的 VM 发生 OOM，对应的 virt-launcher Pod 重启。 排查过程Kernel 中捕获到的 OOM 日志： 1234567...Mar 17 13:06:36 xxx kernel: CPU 1/KVM invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=990Mar 17 13:06:36 xxx kernel: Memory cgroup out of memory: Killed process 15621 (qemu-system-x86) total-vm:17962836kB, anon-rss:15698676kB, file-rss:24744kB, shmem-rss:4kB, UID:107 pgtables:31552kB oom_score_adj:990...Mar 20 03:00:55 xxx kernel: virt-launcher invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=990Mar 20 03:00:55 xxx kernel: Memory cgroup out of memory: Killed process 37685 (qemu-system-x86) total-vm:19403216kB, anon-rss:16439716kB, file-rss:24368kB, shmem-rss:4kB, UID:107 pgtables:33732kB oom_score_adj:990... CPU 1/KVM 是 qemu-system-x86 进程的一个线程，它向 Guest OS 模拟 vCPU。该日志表示 CPU 1/KVM 线程在执行分配内存操作的时候触发了 OOM；virt-launcher 是一个与 KubeVirt 相关的进程，运行在虚拟机 Pod 的相关 cgroup 中。该日志表示 virt-launcher 进程在分配内存时触发了 OOM。virt-launcher 也在 qemu-system-x86 进程中运行，所以 Kernel 日志中记录的触发 OOM 进程也是 qemu-system-x86。 内存资源使用情况： 12345678910# Guest OS 视角sles@xxx:~&gt; free -gtotal used free shared buff/cache availableMem: 15 4 3 0 8 11Swap: 0 0 0# Host OS 视角kubectl top podNAME CPU(cores) MEMORY(bytes)virt-launcher-xxx 622m 13138Mi 从 Guest OS 的内存使用情况来看，Guest OS 有 11GiB 可用，但 buff/cache 仍有 8GiB；而 virt-launcher Pod 占用内存约为 13GiB。由此可见从 Host OS 角度来看，这些缓存仍然占用了 qemu-system-x86 进程的 cgroup 资源。 即使 Guest OS 认为它有 11GiB 可用，但 Host OS 认为 VM 仍然持有 13GiB 内存，那么就有可能触发 OOM。 Memory Overhead在 Harvester 中，每个 VM 都运行在一个 Kubernetes Pod 内，主要进程为 qemu-system-x86。Pod 的内存分配受 Kubernetes 的 requests 和 limits 机制控制。除了 Guest OS 本身使用的内存外，Harvester 和 KubeVirt 还需要额外分配一部分内存来管理 VM 的 CPU、存储、网络等资源，这部分称为 Memory Overhead（额外内存）。 OOM 发生的原因尽管 Harvester 采用了一定的计算公式来为 VM 预留额外内存，但在某些情况下，仍可能出现 OOM 问题，导致 qemu-system-x86 被 cgroup OOM killer 终结。 可能导致 VM OOM 的原因有： VM 运行在 Kubernetes Pod 中，受 cgroup 机制限制，所有进程 (如 virt-launcher, virtlogd, virtqemud) 共享 Pod 级别的内存配额。 Harvester 默认 Reserved Memory 过低 (100Mi)，不足以应对不同类型的 VM 及其操作系统。 VM 在长时间运行后，Guest OS 层面看仍是较低的内存使用率，但从 Host OS 层面上看，相关进程几乎耗尽了所有资源，导致 Host OS 认为 VM 内存已经耗尽，进而触发 cgroup OOM。 Reserved Memory为了降低 OOM 发生的概率，Harvester 引入 Reserved Memory 机制，默认 Reserved Memory 为 100Mi。该机制使用户可以手动调整 Guest OS 之外的 Total Memory Overhead，从而避免 VM 进程因 cgroup OOM 而崩溃。 如下为 Total Memory Overhead 计算公式：Total Memory Overhead (总的额外内存) = 自动计算的 Memory Overhead (根据 VM 配置自动计算的额外内存) + Harvester Reserved Memory (预留内存) 解决方案以下解决方案均需要重启 VM： v1.4.x 版本前，可以通过增加 Reserved Memory，提高 Total Memory Overhead 避免 OOM 发生。 v1.4.x 版本开始，在 Harvester -&gt; Settings 中可以调整 additional-guest-memory-overhead-ratio，提高 KubeVirt 计算的 Memory Overhead，减少 VM 被 OOM 杀死的概率。 v1.4.x 版本前可以通过修改 CR kubevirts.kubevirt.io 添加 spec.configuration.additionalGuestMemoryOverheadRatio 配置。且升级至 v1.4.x 后会自动转换为 Harvester -&gt; Settings 中的 additional-guest-memory-overhead-ratio。 移除 cgroup 内存限制，VM 可能无限制占用 Host 资源，影响其他 Pod/VM。 临时解决方案 手动释放 Guest OS 内的 buff/cache 内存，属于临时解决方案，未来仍有可能再次发生 OOM。 1sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches","link":"/2025/03/25/Harvester-VM-OOM-%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"title":"RKE2 Cilium without kube-proxy","text":"集群如果使用 Cilium 作为 cni 的话，可以实现 Kubernetes Without kube-proxy。 Cilium 的 kube-proxy 替代程序依赖于 socket-LB 功能，需要使用 v4.19.57、v5.1.16、v5.2.0 或更高版本的 Linux 内核。Linux 内核 v5.3 和 v5.8 增加了其他功能，Cilium 可利用这些功能进一步优化 kube-proxy 替代实现。 已有的 RKE2 Cilium 集群，可以通过下面的步骤开启此功能。 在 Rancher 通过 Yaml 编辑集群： 1234567891011121314spec: kubernetesVersion: v1.27.16+rke2r2 rkeConfig: chartValues: rke2-cilium: # 如果有外部 LB 指向 kube-apiserver，可以设置为 VIP 地址 k8sServiceHost: 127.0.0.1 k8sServicePort: 6443 # 关键参数，开启 Cilium kube-proxy 替代功能 kubeProxyReplacement: true machineGlobalConfig: cni: cilium # 关闭 kube-proxy disable-kube-proxy: true 待 Provisioning 结束后，需要重启所有节点的 rke2-server or rke2-agent： 12systemctl restart rke2-serversystemctl restart rke2-agent 这个时候 agent 节点就不会有 kube-proxy pod 了，但 server 节点的需要手动移除 kube-proxy Yaml 文件： 1mv /var/lib/rancher/rke2/agent/pod-manifests/kube-proxy.yaml ~/kube-proxy.yaml 这个时候集群已经没有 kube-proxy pod，然后清除之前生成的 iptables 规则： 1iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -Z &amp;&amp; iptables -F -t nat &amp;&amp; iptables -X -t nat &amp;&amp; iptables -Z -t nat 最后，重启 Cilium： 1kubectl -n kube-system rollout restart ds cilium 检查 KubeProxyReplacement 配置是否生效： 123root@test001:~# kubectl -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg status | grep KubeProxyReplacementKubeProxyReplacement: True [eth0 172.16.0.2 fe80::216:3eff:fe08:5140 (Direct Routing)]root@test001:~# 获取更多细节配置： 123456789101112131415161718192021root@test001:~# kubectl -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg status --verbose...KubeProxyReplacement Details: Status: True Socket LB: Enabled Socket LB Tracing: Enabled Socket LB Coverage: Full Devices: eth0 172.16.0.2 fe80::216:3eff:fe08:5140 (Direct Routing) Mode: SNAT Backend Selection: Random Session Affinity: Enabled Graceful Termination: Enabled NAT46/64 Support: Disabled XDP Acceleration: Disabled Services: - ClusterIP: Enabled - NodePort: Enabled (Range: 30000-32767) - LoadBalancer: Enabled - externalIPs: Enabled - HostPort: Enabled... 查看是否还有 kube-proxy 的 iptables 规则： 1iptables-save | grep KUBE-SVC 创建一个 Workload 和 Service 进行 ClusterIP/NodePort 测试，能正常通信即可： 12345678910111213root@test001:~# kubectl get pod -n kube-system | grep kube-proxyroot@test001:~# kubectl get svc nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx NodePort 10.43.38.171 &lt;none&gt; 80:32048/TCP 26hroot@test001:~# curl 10.43.38.171 -IHTTP/1.1 200 OK...root@test001:~# curl 127.0.0.1:32048 -IHTTP/1.1 200 OK...","link":"/2024/10/31/RKE2-Cilium-without-kube-proxy/"},{"title":"RKE1&#x2F;RKE2 Nginx Ingress Controller 自定义配置","text":"RKE1 通过 cluster.yaml 中配置，RKE2 通过 HelmChartConfig 配置。 RKE1参考：https://rke.docs.rancher.com/config-options/add-ons/ingress-controllers#configuring-nginx-ingress-controller 1234ingress: provider: nginx options: allow-snippet-annotations: &quot;true&quot; RKE2参考：https://github.com/rancher/rke2-charts/tree/main/charts/rke2-ingress-nginx/rke2-ingress-nginx 1234567891011cat &lt;&lt;EOF | kubectl apply -f -apiVersion: helm.cattle.io/v1kind: HelmChartConfigmetadata: name: rke2-ingress-nginx namespace: kube-systemspec: valuesContent: |- controller: allowSnippetAnnotations: &quot;true&quot;EOF","link":"/2025/03/17/RKE1-RKE2-Nginx-Ingress-Controller-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AE/"},{"title":"RKE2 Cilium 配置 Cluster Mesh 实现跨集群通信","text":"通过 Cilium 的能力实现 Kubernetes 集群连接在一起来构建一个网状集群，在所有集群之间启用 pod-to-pod 连接，定义全局服务来平衡集群之间的负载，并执行安全策略来限制访问。 官方文档：https://docs.cilium.io/en/stable/network/clustermesh/clustermesh/#enable-clustermesh 首先需要创建两个 Cilium 集群，两个集群的 Cluster CIDR 和 Service CIDR 不能冲突： 在附加配置中，设置两个集群的 Cilium Cluster ID 和 Name，也不能够冲突： 集群创建好后，在两个集群创建一个 SVC，用于将该集群的 API server 对外暴露： 12345678910111213141516cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: name: apiserver namespace: kube-systemspec: ports: - name: port-6443 port: 6443 protocol: TCP targetPort: 6443 selector: component: kube-apiserver type: NodePortEOF 分别在两个集群的 Control Plane 节点配置 kubeconfig，添加对端集群的配置，server 地址可以配置刚刚创建好的 Service NodePort，例如： 12345678910111213141516171819202122232425262728293031apiVersion: v1clusters:- cluster: certificate-authority-data: xxx server: https://172.16.16.140:30645 name: rke2-cilium-1- cluster: certificate-authority-data: xxx server: https://172.16.16.141:32460 name: rke2-cilium-2contexts:- context: cluster: rke2-cilium-1 user: rke2-cilium-1 name: rke2-cilium-1- context: cluster: rke2-cilium-2 user: rke2-cilium-2 name: rke2-cilium-2current-context: rke2-cilium-1kind: Configpreferences: {}users:- name: rke2-cilium-1 user: client-certificate-data: xxx client-key-data: xxx- name: rke2-cilium-2 user: client-certificate-data: xxx client-key-data: xxx 安装 Cilium CLI： 1234567CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)CLI_ARCH=amd64if [ &quot;$(uname -m)&quot; = &quot;aarch64&quot; ]; then CLI_ARCH=arm64; ficurl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sumsudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/binrm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} 分别在两个集群开启 Cluster Mesh： 12345# Cluster 1cilium clustermesh enable --context rke2-cilium-1 --service-type NodePort --helm-release-name rke2-cilium# Cluster 2cilium clustermesh enable --context rke2-cilium-2 --service-type NodePort --helm-release-name rke2-cilium 执行该命令后在 kube-system 下会生成 Deployment clustermesh-apiserver，其状态正常则代表开启成功，也可以通过下面的命令检查： 12345678910111213141516171819root@test-0:~# cilium status --context rke2-cilium-1 /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¯¯\\__/ Hubble Relay: disabled \\__/ ClusterMesh: OKDaemonSet cilium Desired: 1, Ready: 1/1, Available: 1/1Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1Deployment clustermesh-apiserver Desired: 1, Ready: 1/1, Available: 1/1Containers: cilium Running: 1 cilium-operator Running: 1 clustermesh-apiserver Running: 1Cluster Pods: 15/15 managed by CiliumHelm chart version:Image versions cilium harbor.warnerchen.com/rancher/mirrored-cilium-cilium:v1.16.2: 1 cilium-operator harbor.warnerchen.com/rancher/mirrored-cilium-operator-generic:v1.16.2: 1 clustermesh-apiserver harbor.warnerchen.com/rancher/mirrored-cilium-clustermesh-apiserver:v1.16.2: 3 建立集群连接： 12345# Cluster 1cilium clustermesh connect --context rke2-cilium-1 --destination-context rke2-cilium-2 --helm-release-name rke2-cilium# Cluster 2cilium clustermesh connect --context rke2-cilium-2 --destination-context rke2-cilium-1 --helm-release-name rke2-cilium 日志如下则代表开启成功： 也可以通过下面的命令检查： 123456789101112131415root@test-0:~# cilium clustermesh status rke2-cilium-1⚠️ Service type NodePort detected! Service may fail when nodes are removed from the cluster!✅ Service &quot;clustermesh-apiserver&quot; of type &quot;NodePort&quot; found✅ Cluster access information is available: - 172.16.16.140:32379✅ Deployment clustermesh-apiserver is readyℹ️ KVStoreMesh is enabled✅ All 1 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]✅ All 1 KVStoreMesh replicas are connected to all clusters [min:1 / avg:1.0 / max:1]🔌 Cluster Connections: - rke2-cilium-2: 1/1 configured, 1/1 connected - KVStoreMesh: 1/1 configured, 1/1 connected🔀 Global services: [ min:1 / avg:1.0 / max:1 ] 如果连接 Cilium Pod 失败可以尝试重启 Cilium 解决： 1kubectl -n kube-system rollout restart ds cilium 部署 Demo 进行测试： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: annotations: service.cilium.io/global: 'true' name: global-service-videospec: ports: - name: port-80 port: 80 protocol: TCP targetPort: 80 selector: name: rebel-base---apiVersion: apps/v1kind: Deploymentmetadata: name: rebel-basespec: selector: matchLabels: name: rebel-base replicas: 2 template: metadata: labels: name: rebel-base spec: containers: - name: rebel-base image: harbor.warnerchen.com/library/nginx:mainline volumeMounts: - name: html mountPath: /usr/share/nginx/html/ livenessProbe: httpGet: path: / port: 80 periodSeconds: 1 readinessProbe: httpGet: path: / port: 80 volumes: - name: html configMap: name: rebel-base-response items: - key: message path: index.html---apiVersion: v1kind: ConfigMapmetadata: name: rebel-base-responsedata: # 需要替换此处的这里的 Cluster 为对应的集群名称 message: &quot;{\\&quot;Name\\&quot;: \\&quot;Warner\\&quot;, \\&quot;Cluster\\&quot;: \\&quot;rke2-cilium-1\\&quot;}\\n&quot;---apiVersion: apps/v1kind: Deploymentmetadata: name: x-wingspec: selector: matchLabels: name: x-wing replicas: 2 template: metadata: labels: name: x-wing spec: containers: - name: x-wing-container image: quay.io/cilium/json-mock:v1.3.3@sha256:f26044a2b8085fcaa8146b6b8bb73556134d7ec3d5782c6a04a058c945924ca0 livenessProbe: exec: command: - curl - -sS - -o - /dev/null - localhost readinessProbe: exec: command: - curl - -sS - -o - /dev/null - localhostEOF 可以直接通过 Pod IP/Global Service 进行通信：","link":"/2025/02/13/RKE2-Cilium-%E9%85%8D%E7%BD%AE-Cluster-Mesh-%E5%AE%9E%E7%8E%B0%E8%B7%A8%E9%9B%86%E7%BE%A4%E9%80%9A%E4%BF%A1/"},{"title":"Kubernetes MCP Server 使用随记","text":"MCP 全称为 Model Context Protocol，中文译为模型上下文协议，是一个开放标准，使开发人员能够在数据源和人工智能驱动的工具之间建立安全的双向连接。 在没有 MCP 之前，AI 要读去分析资料需要先手动将文件进行上传，或者是将内容复制到对话框内，使用过程相对繁杂；如果使用了 MCP，则可以直接让 AI 对本地文件进行分析。 此处使用开源项目 Kubernetes MCP Server 进行测试。 环境信息： 12345678910111213node --versionv23.10.0npm --version11.2.0npx --version11.2.0bun --version1.2.5kubectl versionClient Version: v1.31.0Kustomize Version: v5.4.2helm versionversion.BuildInfo{Version:&quot;v3.15.4&quot;, GitCommit:&quot;fa9efb07d9d8debbb4306d72af76a383895aa8c4&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.22.6&quot;} 构建 mcp-server-kubernetes： 12345git clone https://github.com/Flux159/mcp-server-kubernetes.gitcd mcp-server-kubernetesgit checkout v0.2.5bun installbun run build 启动 mcp-server-kubernetes： 1npx @modelcontextprotocol/inspector node dist/index.js 测试是否能够通过 MCP 连接到 Kubernetes： 通过下面的 claude_desktop_config.json 配置，与 Claude 进行对接： 1234567891011121314{ &quot;mcpServers&quot;: { &quot;kubernetes&quot;: { &quot;command&quot;: &quot;npx&quot;, &quot;args&quot;: [&quot;mcp-server-kubernetes&quot;], &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 3000, &quot;timeout&quot;: 30000, &quot;env&quot;: { &quot;KUBECONFIG&quot;: &quot;/Users/warchen/Desktop/mcp-server-kubernetes/kubeconfig&quot; } } }} 尝试通过 Claude 获取某个 namespace 下的 pod 信息，并创建一个 nginx pod：","link":"/2025/03/18/Kubernetes-MCP-Server-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"RKE1&#x2F;2 节点配置 Nvidia Container Toolkit","text":"GPU 节点安装 Nvidia 驱动官方文档：NVIDIA CUDA Installation Guide for Linux 安装前准备查看是否有可用的 GPU： 12root@gpu-0:~# lspci | grep -i nvidia03:00.0 3D controller: NVIDIA Corporation GP104GL [Tesla P4] (rev a1) 安装依赖项： 12apt updateapt -y install gcc make Nouveau 是开源的 NVIDIA 驱动，会与官方闭源驱动冲突，需要禁用： 1234echo -e &quot;blacklist nouveau\\noptions nouveau modeset=0&quot; | sudo tee /etc/modprobe.d/blacklist-nouveau.confupdate-initramfs -urebootlsmod | grep nouveau 关闭 Secure Boot： 12mokutil --disable-validationmokutil --sb-state 通过 Nvidia 官方 .run 程序安装驱动下载对应的安装程序：Nvidia Driver Downloads 1wget -c &quot;https://cn.download.nvidia.com/tesla/570.86.15/NVIDIA-Linux-x86_64-570.86.15.run&quot; 执行安装： 123chmod +x NVIDIA-Linux-x86_64-570.86.15.run./NVIDIA-Linux-x86_64-570.86.15.runreboot 如果执行 nvidia-smi 是 No devices were found or NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.，可以尝试用下面的命令解决： 1234apt -y install dkmsVERSION=$(ls /usr/src | awk -F - '/nvidia/{ print $2 }')dkms install -m nvidia -v $VERSIONreboot 重启节点会进入 Perform MOK management 页面，选择 EnrollMOK 并输入密码，完成后会再次重启节点。 通过 Nvidia 官方 local repo 安装下载对应的 local repo 文件： 1234wget -c &quot;https://cn.download.nvidia.com/tesla/570.86.15/nvidia-driver-local-repo-ubuntu2204-570.86.15_1.0-1_amd64.deb&quot;dpkg -i nvidia-driver-local-repo-ubuntu2204-570.86.15_1.0-1_amd64.debcp /var/nvidia-driver-local-repo-ubuntu2204-570.86.15/nvidia-driver-local-081EF1BD-keyring.gpg /usr/share/keyrings/apt update 查看驱动版本： 1apt list | grep nvidia-driver 安装驱动： 12apt -y install nvidia-driver-570reboot 重启节点会进入 Perform MOK management 页面，选择 EnrollMOK 并输入密码，完成后会再次重启节点。 重启完成，可以用 nvidia-smi 命令查看 GPU 信息： 通过 Ubuntu PPA 安装驱动可以通过 Ubuntu PPA 进行安装，但推荐安装的驱动版本较低： 1234567891011root@gpu-0:~# ubuntu-drivers devices== /sys/devices/pci0000:03/0000:03:00.0 ==modalias : pci:v000010DEd00001BB3sv000010DEsd000011D8bc03sc02i00vendor : NVIDIA Corporationmodel : GP104GL [Tesla P4]driver : nvidia-driver-470-server - distro non-freedriver : nvidia-driver-470 - distro non-free recommendeddriver : nvidia-driver-418-server - distro non-freedriver : nvidia-driver-390 - distro non-freedriver : nvidia-driver-450-server - distro non-freedriver : xserver-xorg-video-nouveau - distro free builtin 根据输出内容，安装推荐的版本： 12apt -y install nvidia-driver-470reboot 重启节点会进入 Perform MOK management 页面，选择 EnrollMOK 并输入密码，完成后会再次重启节点。 重启完成，可以用 nvidia-smi 命令查看 GPU 信息： RKE1 节点配置 Nvidia Container Runtime节点需要先安装 Docker： 1curl https://releases.rancher.com/install-docker/20.10.sh | sh 安装 Nvidia Container Toolkit： 官方安装文档：Installing the NVIDIA Container Toolkit 12345678910curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.listsed -i -e '/experimental/ s/^#//g' /etc/apt/sources.list.d/nvidia-container-toolkit.listapt-get updateapt-get install -y nvidia-container-toolkit 配置 Nvidia Container Runtime： 123456789101112cat &lt;&lt;EOF &gt; /etc/docker/daemon.json{ &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;insecure-registries&quot; : [ &quot;0.0.0.0/0&quot; ], &quot;runtimes&quot;: { &quot;nvidia&quot;: { &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] } }}EOF 1234root@gpu-0:~# systemctl restart dockerroot@gpu-0:~# docker info | grep Runtime Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux nvidia runc Default Runtime: nvidia 测试容器是否可用： 123456789101112131415161718192021root@gpu-0:~# docker run --rm --runtime=nvidia --gpus all harbor.warnerchen.com/library/ubuntu:latest nvidia-smiWed Feb 19 09:43:47 2025+-----------------------------------------------------------------------------+| NVIDIA-SMI 470.256.02 Driver Version: 470.256.02 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 Tesla P4 Off | 00000000:03:00.0 Off | 0 || N/A 49C P0 23W / 75W | 0MiB / 7611MiB | 2% Default || | | N/A |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: || GPU GI CI PID Type Process name GPU Memory || ID ID Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ RKE2 节点配置 Nvidia Container Runtime安装 Nvidia Container Toolkit： 12345678910curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.listsed -i -e '/experimental/ s/^#//g' /etc/apt/sources.list.d/nvidia-container-toolkit.listapt-get updateapt-get install -y nvidia-container-toolkit 新版本的 RKE2 在启动时会自动检测节点上是否已安装 Nvidia Container Toolkit，若检测到，会自动为 containerd 进行相关配置，无需手动干预；如果是在节点注册进 RKE2 集群之后才安装的 Nvidia Container Toolkit，只需重启该节点的 RKE2 服务，即可完成自动配置： 12Apr 18 11:45:35 gpu-0 rke2[2226]: time=&quot;2025-04-18T11:45:35+08:00&quot; level=debug msg=&quot;Searching for nvidia container runtime&quot;Apr 18 11:45:35 gpu-0 rke2[2226]: time=&quot;2025-04-18T11:45:35+08:00&quot; level=info msg=&quot;Found nvidia container runtime at /usr/bin/nvidia-container-runtime&quot; 1234root@gpu-0:~# crictl info | grep -i nvidia &quot;nvidia&quot;: { &quot;BinaryName&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;, &quot;name&quot;: &quot;nvidia&quot; 如果是旧版本 RKE2，则需要手动修改配置： 1234567891011121314cp /var/lib/rancher/rke2/agent/etc/containerd/config.toml .cp /var/lib/rancher/rke2/agent/etc/containerd/config.toml /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmplvim /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl# 在最后面添加下面的内容[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.&quot;nvidia&quot;] runtime_type = &quot;io.containerd.runc.v2&quot;[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.&quot;nvidia&quot;.options] BinaryName = &quot;/usr/bin/nvidia-container-runtime&quot;systemctl restart rke2-server or rke2-agentrm /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl 配置 defaultRuntimeName： 123456789101112cp /var/lib/rancher/rke2/agent/etc/containerd/config.toml .cp /var/lib/rancher/rke2/agent/etc/containerd/config.toml /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmplvim /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl# 新增下面的内容[plugins.&quot;io.containerd.cri.v1.runtime&quot;.containerd] default_runtime_name = &quot;nvidia&quot;systemctl restart rke2-server or rke2-agentrm /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl 添加 Nvidia Helm Chart 仓库： 仓库地址：https://helm.ngc.nvidia.com/nvidia 安装 GPU Operator，在 values.yaml 中配置 containerd socket 和 config.toml 文件的路径： 1234567toolkit: enabled: true env: - name: CONTAINERD_CONFIG value: /var/lib/rancher/rke2/agent/etc/containerd/config.toml - name: CONTAINERD_SOCKET value: /run/k3s/containerd/containerd.sock 安装成功： 创建 Pod 验证 GPU 资源： 12345678910111213141516171819202122cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: nbody-gpu-benchmark namespace: defaultspec: restartPolicy: OnFailure runtimeClassName: nvidia containers: - name: cuda-container image: harbor.warnerchen.com/nvidia/k8s/cuda-sample:nbody args: [&quot;nbody&quot;, &quot;-gpu&quot;, &quot;-benchmark&quot;] resources: limits: nvidia.com/gpu: 1 env: - name: NVIDIA_VISIBLE_DEVICES value: all - name: NVIDIA_DRIVER_CAPABILITIES value: allEOF 查看日志，运行成功： 12345678910111213141516171819202122232425root@rke2-cilium-01:~# kubectl logs nbody-gpu-benchmarkRun &quot;nbody -benchmark [-numbodies=&lt;numBodies&gt;]&quot; to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=&lt;N&gt; (number of bodies (&gt;= 1) to run in simulation) -device=&lt;d&gt; (where d=0,1,2.... for the CUDA device to use) -numdevices=&lt;i&gt; (where i=(number of CUDA devices &gt; 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.&gt; Windowed mode&gt; Simulation data stored in video memory&gt; Single precision floating point simulation&gt; 1 Devices used for simulationGPU Device 0: &quot;Pascal&quot; with compute capability 6.1&gt; Compute 6.1 CUDA device: [Tesla P4]20480 bodies, total time for 10 iterations: 27.727 ms= 151.272 billion interactions per second= 3025.446 single-precision GFLOP/s at 20 flops per interaction","link":"/2024/12/17/RKE1-2-%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE-Nvidia-Container-Toolkit/"},{"title":"Rancher Elemental 使用随记","text":"简介Rancher Elemental 用于快速部署和管理基于容器的操作系统，如 SLE Micro 和 openSUSE MicroOS。它专为边缘计算和云原生环境设计，可以提供极简、易维护的操作系统。 组件： elemental：负责操作系统安装、更新和维护的命令行工具 elemental-operator：运行于 kubernetes 中，用于管理设备注册和生命周期 elemental-register：运行于设备中，用于将设备与 Rancher Elemental 集群注册 elemental-system-agent：负责设备的配置应用和生命周期管理 CRD： MachineRegistration：定义设备如何注册到 Rancher Elemental 集群，并提供初始化配置 MachineInventory：记录注册设备的详细信息，包括硬件属性和状态 MachineInventorySelector：用于选择一组符合特定标签或条件的设备 MachineInventorySelectorTemplate：用于生成 MachineInventorySelector，支持动态创建设备分组规则 ManagedOSImage：描述和管理设备可用的操作系统镜像信息 ManagedOSVersion：定义操作系统版本及其支持的功能和变更点 ManagedOSVersionChannel：管理操作系统版本更新的分发渠道 SeedImage：用于创建安装介质，将 Elemental 安装到节点上 使用随记在 Rancher Extension 安装 Elemental 添加一个 OS Channel 1234567891011apiVersion: elemental.cattle.io/v1beta1kind: ManagedOSVersionChannelmetadata: name: sl-micro-6.0-base-channel namespace: fleet-defaultspec: deleteNoLongerInSyncVersions: false options: image: registry.suse.com/rancher/elemental-channel/sl-micro:6.0-base syncInterval: 1h type: custom 创建一个 MachineRegistration，Cloud Configuration 可以根据需求自定义，例如设置主机名、网络配置等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: elemental.cattle.io/v1beta1kind: MachineRegistrationmetadata: name: elemental-cluster-1 namespace: fleet-defaultspec: config: cloud-config: users: - name: root passwd: password ssh-authorized-keys: - &gt;- ssh-rsa xxx write_files: - content: | [connection] id=eth0 type=ethernet interface-name=eth0 autoconnect=true [ipv4] method=manual address1=172.16.16.141/24,172.16.16.1 dns=172.16.16.12; [ipv6] method=ignore path: /etc/NetworkManager/system-connections/eth0.nmconnection permissions: '0600' elemental: install: debug: true device: /dev/sda reboot: true snapshotter: type: loopdevice reset: reboot: true reset-oem: true reset-persistent: true machineInventoryLabels: author: warner machineUUID: ${System Information/UUID} manufacturer: ${System Information/Manufacturer} productName: ${System Information/Product Name} serialNumber: ${System Information/Serial Number} 创建完后，选择对应的 OS Version 构建镜像，点击构建后在 fleet-default 命名空间下会生成一个 pod，用于 base image 拉取、镜像构建、生成镜像下载地址 在界面下载构建好的 ISO，也可以通过 SeedImage CRD 获取下载地址 1kubectl -n fleet-default get seedimages.elemental.cattle.io media-image-reg-xxx -ojsonpath={.status.downloadURL} 下载好后就可以通过这个 ISO 创建虚拟机，OS 安装过程中需要用到 TPM，所以需要在 vSphere 中开启本机类型的 TPM vSphere 开启本机类型的 TPM 有两个前提： vSphere 需要配置域名，否则创建好后会无法进行 TPM 备份，无法备份的话就无法给虚拟机添加 TPM 设备 创建虚拟机所在的主机需要在一个集群中，否则添加 TPM 设备后会无法创建 以上条件具备后，即可创建虚拟机，引导需要选择 EFI 模式 开机后就会自动进行 OS 安装，并注册到 Rancher Elemental 集群中，可以在节点上通过命令查看注册状态 1journalctl -f -u elemental-register-install.service 注册没问题的话，会生成一个 MachineInventory，记录设备的详细信息 接着就可以用这个节点创建集群","link":"/2024/11/20/Rancher-Elemental-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"Rancher Pod Metrics 部分 Panel No data 问题排查","text":"Rancher Pod Metrics 有部分 Panel 显示 No data，只有 Memory Utilization 显示正常： 最初检测了 Prometheus、Prometheus Targets、Grafana 等，都没有异常，直接查询也是有数据的。 后来检查了 cattle-dashboards namespace 下的 rancher-default-dashboards-pods ConfigMap，发现除了 Memory Utilization 之外，其他 Panel 的 PromQL 都使用了 __rate_interval，这个是 Grafana 7.2 引入的新变量，用于 Prometheus 的 rate 查询。 根据文档的解释，__rate_interval 依赖于 Prometheus scrape_interval 而进行计算，这个值在 Grafana 的数据源界面可以看到，Rancher Monitoring Grafana 的默认数据源配置是在 cattle-monitoring-system 的 rancher-monitoring-grafana-datasource ConfigMap，可以看到 timeInterval 为 30s： 与 Grafana 数据源界面中的 Scrape Interval 相同： 再去检查 Prometheus 的配置，发现 scrape_interval 为 5m，与默认的 30s 不一致： 所以针对这个问题有两个解法： 修改 cattle-monitoring-system 的 rancher-monitoring-grafana-datasource ConfigMap，将 timeInterval 改为 5m。 修改 Prometheus 的 scrape_interval 回 30s。 修改后问题解决：","link":"/2025/02/08/Rancher-Pod-Metrics-%E9%83%A8%E5%88%86-Panel-No-data-%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"},{"title":"RKE2 CoreDNS 配置随记","text":"RKE2 是通过 HelmChartConfig CRD 进行附加配置，所以要配置 CoreDNS 的话，建议也是通过这种方式进行配置。 配置 CoreDNS 打印解析日志123456789101112131415161718192021222324252627282930313233343536cat &lt;&lt;EOF | kubectl apply -f -apiVersion: helm.cattle.io/v1kind: HelmChartConfigmetadata: name: rke2-coredns namespace: kube-systemspec: valuesContent: |- servers: - zones: - zone: . port: 53 plugins: - name: errors - name: health configBlock: |- lameduck 5s - name: ready - name: kubernetes parameters: cluster.local in-addr.arpa ip6.arpa configBlock: |- pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 - name: prometheus parameters: 0.0.0.0:9153 - name: forward parameters: . /etc/resolv.conf - name: cache parameters: 30 - name: loop - name: reload - name: loadbalance # 此处添加 log 插件 - name: logEOF 效果如下： 1234567.:53[INFO] plugin/reload: Running configuration SHA512 = c6665a67f5213bb4bfff40d089abea74c2204a0de6a6081c3756bd4f5702dadc65adc46c9561ea09726560c76b14c7a17ee017d71c40337e40e1e7c3ee8d6580CoreDNS-1.11.1linux/amd64, go1.20.14 X:boringcrypto, ae2bbc29[INFO] 127.0.0.1:44801 - 7331 &quot;HINFO IN 394071549650387858.6622785674508966848. udp 56 false 512&quot; NOTIMP qr,rd,ra 56 0.015719481s[INFO] 172.16.16.140:49138 - 30061 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd 106 0.000305994s[INFO] 172.16.16.140:64634 - 7352 &quot;AAAA IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd 147 0.000231917s 配置 CoreDNS 对 IPv6 类型的 AAAA 记录查询返回域名不存在当业务不需要做 IPv6 的域名解析时，可以通过该配置降低通信成本： 12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOF | kubectl apply -f -apiVersion: helm.cattle.io/v1kind: HelmChartConfigmetadata: name: rke2-coredns namespace: kube-systemspec: valuesContent: |- servers: - zones: - zone: . port: 53 plugins: - name: errors - name: health configBlock: |- lameduck 5s - name: ready - name: kubernetes parameters: cluster.local in-addr.arpa ip6.arpa configBlock: |- pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 - name: prometheus parameters: 0.0.0.0:9153 - name: forward parameters: . /etc/resolv.conf - name: cache parameters: 30 - name: loop - name: reload - name: loadbalance - name: log # 此处添加 template 插件 - name: template parameters: ANY AAAA configBlock: |- rcode NXDOMAINEOF 效果如下： 1234567.:53[INFO] plugin/reload: Running configuration SHA512 = adff475b354490010d53800263b2eaf511bb7e61ee5f84f57447a0302a1d37032e245038e0026c71c91b32c77781b687ed1dc91ce7b82ce9f36ecdcbe5a8589dCoreDNS-1.11.1linux/amd64, go1.20.14 X:boringcrypto, ae2bbc29[INFO] 127.0.0.1:50998 - 41251 &quot;HINFO IN 3159548608793632308.3001229905203107415. udp 57 false 512&quot; NOTIMP qr,rd,ra 57 0.016489023s[INFO] 172.16.16.140:36027 - 14184 &quot;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NOERROR qr,aa,rd 106 0.00025433s[INFO] 172.16.16.140:61510 - 9818 &quot;AAAA IN kubernetes.default.svc.cluster.local. udp 54 false 512&quot; NXDOMAIN qr,aa,rd 54 0.000246489s","link":"/2025/03/04/RKE2-CoreDNS-%E9%85%8D%E7%BD%AE%E9%9A%8F%E8%AE%B0/"},{"title":"Rancher 对接 Shibboleth + OpenLDAP","text":"前提前提是需要部署好 Shibboleth 和 OpenLDAP。 OpenLDAP 部署可以参考：OpenLDAP Install Shibboleth 部署可以参考：Shibboleth Install 这里 Shibboleth 通过容器部署。 Shibboleth 部署通过容器启动 Shibboleth，可以先将所需要的配置文件从容器中拷贝出来，修改后挂载并重新启动容器，主要的文件有： 123456789root@docker-test-0:~/shibboleth-idp# tree.├── access-control.xml├── attribute-filter.xml├── attribute-resolver.xml├── ldap.properties├── metadata-providers.xml├── rancher-metadata.xml└── secrets.properties 准备 access-control.xml，准入哪些 IP 访问 Shibboleth： 1234&lt;entry key=&quot;AccessByIPAddress&quot;&gt; &lt;bean id=&quot;AccessByIPAddress&quot; parent=&quot;shibboleth.IPRangeAccessControl&quot; p:allowedRanges=&quot;#{ {'127.0.0.1/32', '172.16.16.11/32', '172.16.16.142/32', '172.16.16.140/32', '::1/128'} }&quot; /&gt;&lt;/entry&gt; 准备 attribute-filter.xml，配置返回属性的权限： 12345678910111213&lt;!-- 增加过滤条件, attributeID 跟 attribute-resolver 中的 friendlyName 值对应 --&gt;&lt;AttributeFilterPolicy id=&quot;anyone&quot;&gt; &lt;PolicyRequirementRule xsi:type=&quot;ANY&quot;/&gt; &lt;!--&lt;PolicyRequirementRule xsi:type=&quot;Requester&quot; value=&quot;http://test-shibboleth.jacie.work/idp&quot; /&gt;--&gt; &lt;AttributeRule attributeID=&quot;uid&quot;&gt; &lt;PermitValueRule xsi:type=&quot;ANY&quot;/&gt; &lt;/AttributeRule&gt; &lt;AttributeRule attributeID=&quot;displayName&quot; permitAny=&quot;true&quot; /&gt; &lt;AttributeRule attributeID=&quot;givenName&quot; permitAny=&quot;true&quot;/&gt; &lt;AttributeRule attributeID=&quot;entryDN&quot; permitAny=&quot;true&quot;/&gt; &lt;AttributeRule attributeID=&quot;entryUUID&quot; permitAny=&quot;true&quot; /&gt; &lt;AttributeRule attributeID=&quot;memberOf&quot; permitAny=&quot;true&quot; /&gt;&lt;/AttributeFilterPolicy&gt; 准备 attribute-resolver.xml，定义 Shibboleth 认证返回的属性信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;!-- 增加 attribute 配置，这里的 AttributeEncoder 中的 name 属性在多个 atribute 中不能重复 --&gt;&lt;AttributeDefinition xsi:type=&quot;Simple&quot; id=&quot;uid&quot;&gt; &lt;InputDataConnector ref=&quot;myLDAP&quot; attributeNames=&quot;uid&quot;/&gt; &lt;AttributeEncoder xsi:type=&quot;SAML1String&quot; name=&quot;urn:mace:dir:attribute-def:uid&quot; encodeType=&quot;false&quot; /&gt; &lt;AttributeEncoder xsi:type=&quot;SAML2String&quot; name=&quot;urn:oid:0.9.2342.19200300.100.1.1&quot; friendlyName=&quot;uid&quot; encodeType=&quot;false&quot; /&gt;&lt;/AttributeDefinition&gt;&lt;AttributeDefinition xsi:type=&quot;Simple&quot; id=&quot;surname&quot;&gt; &lt;InputDataConnector ref=&quot;myLDAP&quot; attributeNames=&quot;sn&quot;/&gt; &lt;AttributeEncoder xsi:type=&quot;SAML1String&quot; name=&quot;urn:mace:dir:attribute-def:sn&quot; encodeType=&quot;false&quot; /&gt; &lt;AttributeEncoder xsi:type=&quot;SAML2String&quot; name=&quot;urn:oid:2.5.4.4&quot; friendlyName=&quot;sn&quot; encodeType=&quot;false&quot; /&gt;&lt;/AttributeDefinition&gt;&lt;AttributeDefinition xsi:type=&quot;Simple&quot; id=&quot;givenName&quot;&gt; &lt;InputDataConnector ref=&quot;myLDAP&quot; attributeNames=&quot;givenName&quot;/&gt; &lt;AttributeEncoder xsi:type=&quot;SAML1String&quot; name=&quot;urn:mace:dir:attribute-def:givenName&quot; encodeType=&quot;false&quot; /&gt; &lt;AttributeEncoder xsi:type=&quot;SAML2String&quot; name=&quot;urn:oid:2.5.4.42&quot; friendlyName=&quot;givenName&quot; encodeType=&quot;false&quot; /&gt;&lt;/AttributeDefinition&gt;&lt;AttributeDefinition xsi:type=&quot;Simple&quot; id=&quot;displayName&quot;&gt; &lt;InputDataConnector ref=&quot;myLDAP&quot; attributeNames=&quot;displayName&quot;/&gt; &lt;AttributeEncoder xsi:type=&quot;SAML1String&quot; name=&quot;urn:mace:dir:attribute-def:displayName&quot; encodeType=&quot;false&quot; /&gt; &lt;AttributeEncoder xsi:type=&quot;SAML2String&quot; name=&quot;urn:oid:2.16.840.1.113730.3.1.241&quot; friendlyName=&quot;displayName&quot; encodeType=&quot;false&quot; /&gt;&lt;/AttributeDefinition&gt;&lt;!-- 获取用户所属角色信息，这里必须配置，否则对接 Rancher 后无法获取用户组信息 --&gt;&lt;AttributeDefinition xsi:type=&quot;Simple&quot; id=&quot;memberOf&quot;&gt; &lt;InputDataConnector ref=&quot;myLDAP&quot; attributeNames=&quot;memberOf&quot;/&gt; &lt;AttributeEncoder xsi:type=&quot;SAML1String&quot; name=&quot;urn:mace:dir:attribute-def:memberOf&quot; encodeType=&quot;false&quot; /&gt; &lt;AttributeEncoder xsi:type=&quot;SAML2String&quot; name=&quot;urn:oid:2.16.840.1.113730.3.1.244&quot; friendlyName=&quot;memberOf&quot; encodeType=&quot;false&quot; /&gt;&lt;/AttributeDefinition&gt;&lt;AttributeDefinition xsi:type=&quot;Simple&quot; id=&quot;entryDN&quot;&gt; &lt;InputDataConnector ref=&quot;myLDAP&quot; attributeNames=&quot;entryDN&quot;/&gt; &lt;AttributeEncoder xsi:type=&quot;SAML1String&quot; name=&quot;urn:mace:dir:attribute-def:entryDN&quot; encodeType=&quot;false&quot; /&gt; &lt;AttributeEncoder xsi:type=&quot;SAML2String&quot; name=&quot;urn:oid:2.5.4.5&quot; friendlyName=&quot;entryDN&quot; encodeType=&quot;false&quot; /&gt;&lt;/AttributeDefinition&gt;&lt;AttributeDefinition xsi:type=&quot;Simple&quot; id=&quot;entryUUID&quot;&gt; &lt;InputDataConnector ref=&quot;myLDAP&quot; attributeNames=&quot;entryUUID&quot;/&gt; &lt;AttributeEncoder xsi:type=&quot;SAML1String&quot; name=&quot;urn:mace:dir:attribute-def:entryUUID&quot; encodeType=&quot;false&quot; /&gt; &lt;AttributeEncoder xsi:type=&quot;SAML2String&quot; name=&quot;urn:oid:2.5.4.6&quot; friendlyName=&quot;entryUUID&quot; encodeType=&quot;false&quot; /&gt;&lt;/AttributeDefinition&gt;&lt;!-- 示例中的 myLDAP 配置中增加 ReturnAttributes 属性 --&gt;&lt;DataConnector id=&quot;myLDAP&quot; xsi:type=&quot;LDAPDirectory&quot; ldapURL=&quot;%{idp.attribute.resolver.LDAP.ldapURL}&quot; baseDN=&quot;%{idp.attribute.resolver.LDAP.baseDN}&quot; principal=&quot;%{idp.attribute.resolver.LDAP.bindDN}&quot; principalCredential=&quot;%{idp.attribute.resolver.LDAP.bindDNCredential}&quot; useStartTLS=&quot;%{idp.attribute.resolver.LDAP.useStartTLS:true}&quot; connectTimeout=&quot;%{idp.attribute.resolver.LDAP.connectTimeout}&quot; responseTimeout=&quot;%{idp.attribute.resolver.LDAP.responseTimeout}&quot;&gt; &lt;FilterTemplate&gt; &lt;![CDATA[ %{idp.attribute.resolver.LDAP.searchFilter} ]]&gt; &lt;/FilterTemplate&gt; &lt;ConnectionPool minPoolSize=&quot;%{idp.pool.LDAP.minSize:3}&quot; maxPoolSize=&quot;%{idp.pool.LDAP.maxSize:10}&quot; blockWaitTime=&quot;%{idp.pool.LDAP.blockWaitTime:PT3S}&quot; validatePeriodically=&quot;%{idp.pool.LDAP.validatePeriodically:true}&quot; validateTimerPeriod=&quot;%{idp.pool.LDAP.validatePeriod:PT5M}&quot; expirationTime=&quot;%{idp.pool.LDAP.idleTime:PT10M}&quot; /&gt; &lt;ReturnAttributes&gt;%{idp.attribute.resolver.LDAP.returnAttributes}&lt;/ReturnAttributes&gt;&lt;/DataConnector&gt; 准备 ldap.properties，配置 OpenLDAP 相关信息进行对接： 1234567891011121314151617181920idp.authn.LDAP.authenticator = bindSearchAuthenticatoridp.authn.LDAP.ldapURL = ldap://172.16.16.142:389idp.authn.LDAP.useStartTLS = falseidp.authn.LDAP.returnAttributes = cn,uid,sn,displayName,givenName,memberOf,entryDN,entryUUIDidp.authn.LDAP.baseDN = ou=shenzhen,dc=rancher,dc=workidp.authn.LDAP.subtreeSearch = trueidp.authn.LDAP.userFilter = (&amp;(objectclass=inetOrgPerson)(uid={user}))idp.authn.LDAP.bindDN = cn=admin,dc=rancher,dc=workidp.authn.LDAP.dnFormat = uid=warner chen,ou=shenzhen,dc=rancher,dc=workidp.attribute.resolver.LDAP.ldapURL = %{idp.authn.LDAP.ldapURL}idp.attribute.resolver.LDAP.connectTimeout = %{idp.authn.LDAP.connectTimeout:PT3S}idp.attribute.resolver.LDAP.responseTimeout = %{idp.authn.LDAP.responseTimeout:PT3S}idp.attribute.resolver.LDAP.connectionStrategy = %{idp.authn.LDAP.connectionStrategy:ACTIVE_PASSIVE}idp.attribute.resolver.LDAP.baseDN = %{idp.authn.LDAP.baseDN:undefined}idp.attribute.resolver.LDAP.bindDN = %{idp.authn.LDAP.bindDN:undefined}idp.attribute.resolver.LDAP.useStartTLS = %{idp.authn.LDAP.useStartTLS:true}idp.attribute.resolver.LDAP.startTLSTimeout = %{idp.authn.LDAP.startTLSTimeout:PT3S}idp.attribute.resolver.LDAP.trustCertificates = %{idp.authn.LDAP.trustCertificates:undefined}idp.attribute.resolver.LDAP.searchFilter = (uid=$resolutionContext.principal)idp.attribute.resolver.LDAP.returnAttributes = %{idp.authn.LDAP.returnAttributes} 准备 secrets.properties，配置密码认证信息： 1234idp.sealer.storePassword = idpsealerpwdidp.sealer.keyPassword = idpsealerpwdidp.authn.LDAP.bindDNCredential = passwordidp.attribute.resolver.LDAP.bindDNCredential = %{idp.authn.LDAP.bindDNCredential:undefined} 准备 metadata-providers.xml，配置对接 metadata，在原有的配置增加下面的内容： 12&lt;MetadataProvider id=&quot;LocalEntityMetadata&quot; xsi:type=&quot;FilesystemMetadataProvider&quot; metadataFile=&quot;/opt/shibboleth-idp/metadata/rancher-metadata.xml&quot;/&gt; 准备 rancher-metadata.xml，需要从 Rancher 中下载： 1curl --insecure https://&lt;server-url&gt;/v1-saml/shibboleth/saml/metadata -o rancher-metadata.xml 启动 Shibboleth 容器： 123456789docker run -d --name shibboleth-idp -p 8080:8080 \\ -v /root/shibboleth-idp/access-control.xml:/opt/shibboleth-idp/conf/access-control.xml \\ -v /root/shibboleth-idp/attribute-filter.xml:/opt/shibboleth-idp/conf/attribute-filter.xml \\ -v /root/shibboleth-idp/attribute-resolver.xml:/opt/shibboleth-idp/conf/attribute-resolver.xml \\ -v /root/shibboleth-idp/ldap.properties:/opt/shibboleth-idp/conf/ldap.properties \\ -v /root/shibboleth-idp/secrets.properties:/opt/shibboleth-idp/credentials/secrets.properties \\ -v /root/shibboleth-idp/rancher-metadata.xml:/opt/shibboleth-idp/metadata/rancher-metadata.xml \\ -v /root/shibboleth-idp/metadata-providers.xml:/opt/shibboleth-idp/conf/metadata-providers.xml \\ harbor.warnerchen.com/klaalo/shibboleth-idp:4.3.1 这时候需要部署一个 Nginx 作为反向代理，准备配置文件： 1234567891011121314151617181920212223242526cat &lt;&lt;EOF &gt; /root/nginx/default.confserver { listen 80; server_name idp.example.org; location / { return 301 https://$host$request_uri; }}server { listen 443 ssl; server_name idp.example.org; ssl_certificate /etc/nginx/ssl/myservice.cert; ssl_certificate_key /etc/nginx/ssl/myservice.key; location / { proxy_pass http://172.16.16.142:8080; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; }}EOF 准备证书： 1openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj &quot;/CN=idp.example.org&quot; 启动 Nginx： 1234567docker run -d \\ --name nginx \\ -v /root/nginx/default.conf:/etc/nginx/conf.d/default.conf \\ -v /root/nginx/ssl:/etc/nginx/ssl \\ -p 80:80 \\ -p 443:443 \\ harbor.warnerchen.com/library/nginx:mainline Rancher 对接 Shibboleth如图，证书和私钥可以用前面生成的，XML 元数据需要从 Shibboleth 的 /opt/shibboleth-idp/metadata/idp-metadata.xml 获取： 保存后会弹窗，需要进行登陆认证，直接使用 OpenLDAP 的账号密码即可，用户名只需要填 uid。 于此步骤中输入的凭据相关的 IDP 用户将映射到本地主体(admin)帐户并在 Rancher Prime Manager 中分配系统管理员权限，所以查看 local admin 账户可以看到： 12345678910apiVersion: management.cattle.io/v3description: ''displayName: Default Adminkind: User...principalIds: - local://user-xdvhb - shibboleth_user://warner chen...username: xxx Rancher 对接 Shibboleth + OpenLDAP 实现用户组授权在配置之前 OpenLDAP 需要开启 memberOf 属性，创建用户组并将用户添加到组中，配置方法可以参考：配置用户和用户组 配置后，用户的隐藏信息能够看到所属组： 然后 Rancher 需要配置用户组搜索库，这里的 DN 根据实际情况填写： OpenLDAP 目录结构为： 对接完成后，即可通过组进行授权： 授权后，组内用户登录 Rancher，即可以到对应的资源：","link":"/2024/12/25/Rancher-%E5%AF%B9%E6%8E%A5-Shibboleth-OpenLDAP/"},{"title":"Rancher Monitoring V2 Prometheus PVC 扩容","text":"根据 Prometheus Operator 官方文档 描述，即使 StorageClass 支持调整大小，Kubernetes（目前）也不支持通过 StatefulSets 扩展卷。这意味着在更新 Prometheus 等自定义资源的 spec.storage 字段中的存储请求时，Operator 必须删除/创建底层的 StatefulSet，而相关的 PVC 不会扩展。 可以通过下面的步骤进行手动扩容。 首先需要检查 StorageClass 支不支持 volume 的扩容： 1kubectl get storageclass &lt;storageclass_name&gt; -o custom-columns=NAME:.metadata.name,ALLOWVOLUMEEXPANSION:.allowVolumeExpansion 在 Rancher 更新 Helm Chart 的 values，将存储大小改成需要的值，再通过 Yaml 编辑将 Prometheus 的 paused 改成 true（需要注意 Yaml 中多处存在 paused 的配置，需要改的是 Prometheus 的）： 等待更新完成后，通过下面的命令检查是否更新成功： 1kubectl -n cattle-monitoring-system get prometheus rancher-monitoring-prometheus -oyaml | grep -E &quot;paused|storage&quot; 手动更新 PVC 的存储值： 1kubectl -n cattle-monitoring-system patch pvc/prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 --patch '{&quot;spec&quot;: {&quot;resources&quot;: {&quot;requests&quot;: {&quot;storage&quot;:&quot;10Gi&quot;}}}}' 删除 StatefulSet： 1kubectl -n cattle-monitoring-system delete statefulset -l operator.prometheus.io/name=rancher-monitoring-prometheus --cascade=orphan 在 Rancher 更新 Helm Chart 的 values，通过 Yaml 编辑将 Prometheus 的 paused 改回 false，更新完毕之后 Operator 就会重新生成 StatefulSet： 检查是否扩容成功，检查 Prometheus 和 StatefulSet 以及 PVC 数据是否一致： 1234kubectl -n cattle-monitoring-system get prometheus rancher-monitoring-prometheus -oyaml | grep -E &quot;paused|storage&quot;kubectl -n cattle-monitoring-system get sts prometheus-rancher-monitoring-prometheus -oyaml | grep &quot;storage:&quot;kubectl -n cattle-monitoring-system get pvc prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0kubectl -n cattle-monitoring-system exec -it prometheus-rancher-monitoring-prometheus-0 -- df -h /prometheus","link":"/2025/02/10/Rancher-Monitoring-V2-Prometheus-PVC-%E6%89%A9%E5%AE%B9/"},{"title":"Redis Operator 使用随记","text":"基于 OT-CONTAINER-KIT/redis-operator 项目，使用 Operator 在 Kubernetes 上部署 Redis。 安装 Operator123helm repo add ot-helm https://ot-container-kit.github.io/helm-charts/helm upgrade redis-operator ot-helm/redis-operator \\ --install --create-namespace --namespace ot-operators 部署 Redis Standalone 12345678910111213141516171819202122232425262728cat &lt;&lt;EOF | kubectl apply -f -apiVersion: redis.redis.opstreelabs.in/v1beta1kind: Redismetadata: name: redis-standalonespec: kubernetesConfig: image: quay.io/opstree/redis:v7.0.15 imagePullPolicy: IfNotPresent resources: requests: cpu: 101m memory: 128Mi limits: cpu: 101m memory: 128Mi storage: volumeClaimTemplate: spec: storageClassName: longhorn accessModes: [&quot;ReadWriteOnce&quot;] resources: requests: storage: 1Gi securityContext: runAsUser: 1000 fsGroup: 1000EOF 如果出现报错： 1Can't open or create append-only dir appendonlydir: Permission denied 需要修改数据目录权限解决： 1chown -R ubuntu:ubuntu /&lt;path-to-redis-data-dir&gt; 部署后就可以通过 redis-cli 工具连接： 123456root@rke2-cilium-01:~# kubectl exec -it redis-standalone-0 -- bashredis-standalone-0:/data$ redis-cli127.0.0.1:6379&gt; info# Serverredis_version:7.0.15... 部署 Redis Cluster 12345678910111213141516171819202122232425262728293031cat &lt;&lt;EOF | kubectl apply -f -apiVersion: redis.redis.opstreelabs.in/v1beta1kind: RedisClustermetadata: name: redis-clusterspec: clusterSize: 1 clusterVersion: v7 securityContext: runAsUser: 1000 fsGroup: 1000 persistenceEnabled: true kubernetesConfig: image: quay.io/opstree/redis:v7.0.15 imagePullPolicy: Always resources: requests: cpu: 101m memory: 128Mi limits: cpu: 101m memory: 128Mi storage: volumeClaimTemplate: spec: storageClassName: longhorn accessModes: [&quot;ReadWriteOnce&quot;] resources: requests: storage: 1GiEOF 部署 Redis Sentinel 1234567891011121314151617181920212223cat &lt;&lt;EOF | kubectl apply -f -apiVersion: redis.redis.opstreelabs.in/v1beta1kind: RedisSentinelmetadata: name: redis-sentinelspec: clusterSize: 1 securityContext: runAsUser: 1000 fsGroup: 1000 redisSentinelConfig: redisReplicationName : redis-replication kubernetesConfig: image: quay.io/opstree/redis-sentinel:v7.0.15 imagePullPolicy: IfNotPresent resources: requests: cpu: 101m memory: 128Mi limits: cpu: 101m memory: 128MiEOF 配置 Auth1kubectl create secret generic redis-secret --from-literal=password=changeme 12345678910111213141516171819202122232425262728293031cat &lt;&lt;EOF | kubectl apply -f -apiVersion: redis.redis.opstreelabs.in/v1beta1kind: Redismetadata: name: redis-standalonespec: kubernetesConfig: image: quay.io/opstree/redis:v7.0.15 imagePullPolicy: IfNotPresent resources: requests: cpu: 101m memory: 128Mi limits: cpu: 101m memory: 128Mi redisSecret: name: redis-secret key: password storage: volumeClaimTemplate: spec: storageClassName: longhorn accessModes: [&quot;ReadWriteOnce&quot;] resources: requests: storage: 1Gi securityContext: runAsUser: 1000 fsGroup: 1000EOF 可以看到需要密码认证后才能使用： 12345root@rke2-cilium-01:~# kubectl exec -it redis-standalone-0 -- redis-cli127.0.0.1:6379&gt; infoNOAUTH Authentication required.127.0.0.1:6379&gt; auth changemeOK 添加额外配置1234567891011cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: ConfigMapmetadata: name: redis-external-configdata: redis-additional.conf: | tcp-keepalive 400 slowlog-max-len 158 stream-node-max-bytes 2048EOF 123456789101112131415161718192021222324252627282930313233cat &lt;&lt;EOF | kubectl apply -f -apiVersion: redis.redis.opstreelabs.in/v1beta1kind: Redismetadata: name: redis-standalonespec: redisConfig: additionalRedisConfig: redis-external-config kubernetesConfig: image: quay.io/opstree/redis:v7.0.15 imagePullPolicy: IfNotPresent resources: requests: cpu: 101m memory: 128Mi limits: cpu: 101m memory: 128Mi redisSecret: name: redis-secret key: password storage: volumeClaimTemplate: spec: storageClassName: longhorn accessModes: [&quot;ReadWriteOnce&quot;] resources: requests: storage: 1Gi securityContext: runAsUser: 1000 fsGroup: 1000EOF","link":"/2025/01/13/Redis-Operator-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"SUSE AI 使用随记","text":"SUSE AI 是一个开放的生成式人工智能解决方案，主要包含以下组件： Ollama：简化本地设备上大型语言模型 (LLM) 安装和管理的平台。 Open WebUI：Ollama LLM 运行程序的可扩展网络用户界面。 Milvus：为生成式人工智能应用构建的向量数据库，性能损失最小。 安装前准备安装 SUSE AI 前需要安装 cert-manager： 12345678helm repo add jetstack https://charts.jetstack.iohelm repo updatehelm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.15.3 \\ --set crds.enabled=true 还需要安装中间件使用的存储，这里使用 local-path： 1$ kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.31/deploy/local-path-storage.yaml 其它安装前准备： 1234567891011kubectl create namespace suse-private-aikubectl create secret docker-registry application-collection \\ --docker-server=dp.apps.rancher.io \\ --docker-username=APPCO_USERNAME \\ --docker-password=APPCO_USER_TOKEN \\ -n suse-private-aihelm registry login dp.apps.rancher.io/charts \\ -u APPCO_USERNAME \\ -p APPCO_USER_TOKEN 安装 Milvus准备 Milvus 配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455cat &lt;&lt;EOF &gt; milvus_custom_overrides.yamlglobal: imagePullSecrets: - application-collectioncluster: enabled: Truestandalone: persistence: persistentVolumeClaim: storageClass: local-pathetcd: replicaCount: 1 persistence: storageClassName: local-pathminio: mode: standalone replicas: 1 rootUser: &quot;admin&quot; rootPassword: &quot;adminminio&quot; persistence: storageClass: local-path resources: requests: memory: 1024Mi persistence: enabled: true storageClass: local-path accessMode: ReadWriteOnce size: 10Gikafka: enabled: true name: kafka replicaCount: 3 controller: statefulset: replicas: 1 broker: enabled: true statefulset: replicas: 1 cluster: listeners: client: protocol: 'PLAINTEXT' controller: protocol: 'PLAINTEXT' persistence: enabled: true accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: &quot;local-path&quot;EOF 如需自定义镜像仓库： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273cat &lt;&lt;EOF &gt; milvus_custom_overrides.yamlimage: all: registry: harbor.warnerchen.com repository: appco/containers/milvuscluster: enabled: Truestandalone: persistence: persistentVolumeClaim: storageClass: local-pathetcd: images: etcd: registry: harbor.warnerchen.com repository: appco/containers/etcd replicaCount: 1 persistence: storageClassName: local-pathminio: image: registry: harbor.warnerchen.com repository: appco/containers/minio mcImage: registry: harbor.warnerchen.com repository: appco/containers/mc mode: standalone replicas: 1 rootUser: &quot;admin&quot; rootPassword: &quot;adminminio&quot; persistence: storageClass: local-path resources: requests: memory: 1024Mi persistence: enabled: true storageClass: local-path accessMode: ReadWriteOnce size: 10Gikafka: images: controller: registry: harbor.warnerchen.com repository: appco/containers/apache-kafka broker: registry: harbor.warnerchen.com repository: appco/containers/apache-kafka enabled: true name: kafka replicaCount: 3 controller: statefulset: replicas: 1 broker: enabled: true statefulset: replicas: 1 cluster: listeners: client: protocol: 'PLAINTEXT' controller: protocol: 'PLAINTEXT' persistence: enabled: true accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: &quot;local-path&quot;EOF 安装 Milvus： 123helm upgrade --install milvus oci://dp.apps.rancher.io/charts/milvus \\ -n suse-private-ai \\ --version 4.2.2 -f milvus_custom_overrides.yaml 安装 Ollama准备 Ollama 配置： 1234567891011121314151617181920cat &lt;&lt;EOF &gt; ollama_custom_overrides.yamlglobal: imagePullSecrets: - application-collectionreplicaCount: 1runtimeClassName: &quot;nvidia&quot;ingress: enabled: falsedefaultModel: &quot;llama2&quot;ollama: models: - &quot;llama2&quot; gpu: enabled: true type: 'nvidia' number: 1persistentVolume: enabled: true storageClass: local-pathEOF 如需自定义镜像仓库： 123456789101112131415161718192021cat &lt;&lt;EOF &gt; ollama_custom_overrides.yamlimage: repository: appco/containers/ollama registry: harbor.warnerchen.com pullPolicy: IfNotPresentreplicaCount: 1runtimeClassName: &quot;nvidia&quot;ingress: enabled: falsedefaultModel: &quot;llama2&quot;ollama: models: - &quot;llama2&quot; gpu: enabled: true type: 'nvidia' number: 1persistentVolume: enabled: true storageClass: local-pathEOF 安装 Ollama： 123helm upgrade --install ollama oci://dp.apps.rancher.io/charts/ollama \\ -n suse-private-ai \\ --version 0.63.0 -f ollama_custom_overrides.yaml 安装 Open WebUI准备 Open WebUI 配置： 12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOF &gt; owui_custom_overrides.yamlglobal: imagePullSecrets: - application-collectionollamaUrls:- http://ollama.suse-private-ai.svc.cluster.local:11434persistence: enabled: true storageClass: local-pathollama: enabled: falsepipelines: enabled: Falseingress: enabled: true class: &quot;nginx&quot; annotations: nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot; host: suse-ollama-webui.warnerchen.com tls: trueextraEnvVars:- name: DEFAULT_MODELS value: &quot;llama2&quot;- name: DEFAULT_USER_ROLE value: &quot;user&quot;- name: WEBUI_NAME value: &quot;SUSE AI&quot;- name: GLOBAL_LOG_LEVEL value: INFO- name: RAG_EMBEDDING_MODEL value: &quot;sentence-transformers/all-MiniLM-L6-v2&quot;- name: VECTOR_DB value: &quot;milvus&quot;- name: MILVUS_URI value: http://milvus.suse-private-ai.svc.cluster.local:19530- name: INSTALL_NLTK_DATASETS value: &quot;true&quot;cert-manager: enabled: falseEOF 如需自定义镜像仓库： 1234567891011121314151617181920212223242526272829303132333435363738394041cat &lt;&lt;EOF &gt; owui_custom_overrides.yamlimage: registry: harbor.warnerchen.com repository: appco/containers/open-webui pullPolicy: IfNotPresentollamaUrls:- http://ollama.suse-private-ai.svc.cluster.local:11434persistence: enabled: true storageClass: local-pathollama: enabled: falsepipelines: enabled: Falseingress: enabled: true class: &quot;nginx&quot; annotations: nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot; host: suse-ollama-webui.warnerchen.com tls: trueextraEnvVars:- name: DEFAULT_MODELS value: &quot;llama2&quot;- name: DEFAULT_USER_ROLE value: &quot;user&quot;- name: WEBUI_NAME value: &quot;SUSE AI&quot;- name: GLOBAL_LOG_LEVEL value: INFO- name: RAG_EMBEDDING_MODEL value: &quot;sentence-transformers/all-MiniLM-L6-v2&quot;- name: VECTOR_DB value: &quot;milvus&quot;- name: MILVUS_URI value: http://milvus.suse-private-ai.svc.cluster.local:19530- name: INSTALL_NLTK_DATASETS value: &quot;true&quot;cert-manager: enabled: falseEOF 等待所有 Pod 正常运行： 访问 Open WebUI： 选择模型进行对话： 可以看到 Ollama 调用 GPU： 参考链接： https://hackmd.io/@7vxmAdNPTmmlYGSRMuvbmw/HkibY6m8ke https://documentation.suse.com/suse-ai/1.0/html/AI-deployment-intro/index.html","link":"/2025/02/19/SUSE-AI-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"Ansible","text":"一、Ansible简介ansbile是一个IT自动化的配置管理工具，自动化主要体现在Ansible集成了丰富的模块，可以通过一个命令完成一系列的操作，进而减少运维重复性的工作和维护成本，提高工作效率。 1.1 为什么需要ansible思考：假设我们要在10台服务器上安装并运行nginx，要如何操作？ ssh远程登录到服务器 执行yum -y install nginx 执行systemctl start nginx 执行systemctl enable nginx 重复十次。。。 可以看到简单的工作要做十次是很浪费时间的，这时候我们就需要ansible了。 1.2 ansible有哪些功能 批量执行远程命令，可以同时对N台主机执行命令 批量配置软件服务，可以用自动化的方式配置和管理服务 实现软件开发功能，jumpserver底层使用ansible来实现自动化管理 编排高级的IT任务，playbook是ansbile的一门编程语言，可以用来描绘一套IT架构 二、Ansible安装2.1 在控制端上安装ansible直接利用yum源安装即可，ansible配置文件一般不需要做变动 1yum -y install ansible 2.2 ansible配置文件的优先级 如果当前目录不存在ansible.cfg，会采用默认的配置文件 1234ansible --version...config file = /etc/ansible/ansible.cfg... 如果当前目录存在ansible.cfg，则会采用当前目录的配置文件 1234ansible --version...config file = /root/project1/ansible.cfg... 2.3 ansible的Inventory利用密钥来连接被控端 在项目目录下创建hosts文件 12345cd project1vim hosts[cqm]192.168.88.133192.168.88.134 创建密钥 123ssh-keygenls /root/.ssh/authorized_keys id_rsa id_rsa.pub known_hosts 将公钥传送至被控端主机 12ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.88.133ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.88.134 测试是否不需要密码就可以连接 12ssh root@192.168.88.133ssh root@192.168.88.134 三、Ansible的ad-hoc和常用模块3.1 ansible的ad-hocad-hoc简而言之就是“临时命令”，执行完就结束，并不会保存 主要格式为：ansible + -i + 指定主机清单 + 指定主机组名称 + -m + 指定模块 + -a + 具体命令 使用ad-hoc后返回结果的颜色： 绿色：被控端主机没有发生变化 黄色：被控端主机发生了变化 红色：出现故障 3.2 ansible基本命令 直接执行命令 1ansible -i hosts 主机组名 -m 模块 -a 命令 列出某个主机组的主机清单 1ansible -i hosts 主机组名 --list-hosts 查看某个模块使用教程 12ansible-doc 模块名称/EX 3.2 shell和command模块shell和command本质上都是用来执行linux的基础命令，如cat、ls等等，但command不支持用|这种管道符命令 例子，同样的命令在command上会报错 12ansible -i hosts cqm -m shell -a 'ps -ef | grep nginx'ansible -i hosts cqm -m command -a 'ps -ef | grep nginx' 3.3 yum模块1234567ansible -i hosts cqm -m yum -a 'name=httpd state=latest enablerepo=epel'name:指定安装软件名称state: 1.latest为安装最新版本 2.absent为卸载 3.present为安装enablerepo:指定在哪个yum源下安装 3.4 copy模块1234567ansible -i hosts cqm -m copy -a 'src=./httpd.conf dest=/etc/httpd/conf/httpd.conf owner=www group=www mode=0755'src:表示要复制的文件路径dest:要复制到被控端的哪个路径owner:复制文件的属主group:复制文件的属组mode:文件的权限，0755为rwxr-xr-x(r:4,w:2,x:1)，也可以写成(u+rwx,g+x,o-rwx)的形式backup:如果被控端有同名文件，是否保留 3.5 file模块123#更改文件权限ansible -i hosts cqm -m file -a 'path=/etc/nginx/nginx.conf owner=www group=www mode=0644'path:代表要改变文件的路径(被控端) 12345#创建符号连接(软连接)ansible -i hosts cqm -m file -a 'src=/root/test1 dest=/root/test2 mode=0755 state=link'src:源文件路径(被控端)dest:软连接路径(被控端)state:link创建软连接 12345678#创建文件和文件夹ansible -i hosts cqm -m file -a 'path=/root/text state=touch/directory mode=0644 recurse=yes'path:要创建在哪state: 1.touch表创建文件 2.directory表创建文件夹 3.absent表删除recurse: 递归授权 3.6 service模块12345678ansible -i hosts cqm -m service -a 'name=httpd state=started enabled=yes'name:服务名称state: 1.started启动 2.stopped关闭 3.restarted重启 4.reloaded重载enabled:是否开机自启 3.7 group模块1234567ansible -i hosts cqm -m group -a 'name=cqm state=present gid=1000 system=yes/no'name:创建组名称state: 1.present创建组 2.absent删除组gid:组idsystem:是否设置为系统组 3.8 user模块123456789ansible -i hosts cqm -m user -a 'name=cqm uid=1000 group=cqm shell=/bin/bash state=present create_home=no'name:创建用户名称uid:用户idgroup:添加到哪个组shell:使用/bin/bash创建用户，或/sbin/nologinstate: 1.present创建用户 2.absent删除用户create_home:是否创建家目录，默认为yes 1234ansible -i hosts cqm -m user -a 'name=cqm uid=1000 groups=cqm1,cqm2 append=yes state=present system:yes'groups:添加到哪些组append:添加到多个组时添加该项system:是否设置为系统用户 12ansible -i hosts cqm -m user -a 'name=cqm state=absent remove=yes'remove:是否删除家目录 3.9 cron模块1234567891011ansible -i hosts cqm -m cron -a 'name=&quot;check dirs&quot; minute=0 hour=5,2 job=&quot;ls -al &gt; /dev/null&quot;'name:创建定时任务名称minute:分钟hour:小时job:任务state:删除定时任务用absent#在被控端可查看crontab -l#Ansible: check dirs0 5,2 * * * ls -al &gt; /dev/null 3.10 mount模块1234567891011121314151617ansible -i hosts cqm -m mount -a 'path=/root/data src=/dev/sr0 fstype=iso9660 opts=&quot;ro&quot; state=present'path:挂载到哪src:被挂载的目录，可以是以下形式 1.UUID形式:src=&quot;UUID=......&quot; 2.ip形式:src=&quot;192.168.88.128:/data&quot;，一般用于挂载nfs服务器的目录fstype:挂载类型 1.iso9660:文件系统是一个标准CD-ROM文件系统 2.ext4:Linux系统下的日志文件系统 3.xfs:高性能的日志文件系统 4.nonestate:挂载类型 1.present:开机挂载，仅将挂载配置写入/etc/fstab 2.mounted:挂载设备，并将配置写入/etc/fstab 3.unmounted:卸载设备，不会清除/etc/fstab写入的配置 4.absent:卸载设备，会清理/etc/fstab写入的配置opts:权限等，默认填defaults/etc/fstab:磁盘被手动挂载之后都必须把挂载信息写入/etc/fstab这个文件中，否则下次开机启动时仍然需要重新挂载。 3.11 firewalld模块12345678910111213141516ansible -i hosts cqm -m firewalld -a 'service=https permanent=yes enable=yes immediate=yes state=enabled'service:放行服务port:放行端口，如80/tcpsource:放行指定ip地址范围，如192.168.88.0/24state:表防火墙策略状态 1.enabled策略生效 2.disabled禁用策略 3.present添加策略 4.absent删除策略zone:指定防火墙信任级别 1.drop:丢弃所有进入的包，而不给出任何响应 2.block:拒绝所有外部发起的连接，允许内部发起的连接 3.public:允许指定的进入连接 4.internal:范围针对所有互联网用户permanent:yes为永久生效immediate:yes为立即生效 3.12 unarchive模块unarchive模块能够实现解压再拷贝的功能 1234ansible -i hosts cqm -m unarchive -a 'src=./apache.tar.gz dest=/etc/httpd mode=0755 owner=www group=www'src:压缩文件路径dest:解压到哪remote_src:设置为yes时表示压缩包已经在被控端主机上，而不是ansible控制端本地 3.13 get_url模块1234...url:下载链接dest:保存文件地址mode:设置权限 3.14 template模块template与copy用法相同，但template可以实别变量，而copy不行。 3.15 yum_repository模块yum_repository可以用来配置yum源 123456789101112131415- name: Configure Nginx YUM Repo yum_repository: name: nginx description: Nginx YUM Repo file: nginx baseurl: http://nginx.org/packages/centos/7/$basearch/ gpgcheck: no enabled: yes state: presentname:相当于.repo文件中括号的[仓库名]description:相当于.repo文件中的namefile:相当于.repo文件的名称baseurl:相当于.repo文件中baseurlgpgcheck:相当于.repo文件中gpgcheckenabled:相当于.repo文件中enabled 四、Ansible的Playbookplaybook是ansible的另一种使用方式，被称为“剧本”，与ad-hoc不同的是，playbook可以实现持久使用。 playbook是由一个或多个play组成的列表，play的主要功能在于将事先归并为一组的主机装扮成事先通过ansible中的task定义好的角色，从根本上来讲，所谓的task无非就是调用ansible的一个module，将多个play组织在一个playbook中，即可实现同时完成多项任务。 playbook的核心元素 hosts：被控主机清单 tasks：任务集 vars：变量 templates：模板 handlers和notify：触发器 tags：标签 4.1 利用playbook安装apacheplaybook采用的是yml语法，举个栗子 12cd /etc/project1vim httpd.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647- hosts: cqm tasks: #安装apache服务 - name: Install Httpd Service yum: name: httpd state: latest #创建www组 - name: Create www Group group: name: www state: present #创建www用户 - name: Create www User user: name: www group: www shell: /sbin/nologin state: present #修改apache配置文件 - name: Configure Httpd Conf copy: src: ./httpd.conf dest: /etc/httpd/conf/httpd.conf owner: www group: www mode: 0644 #启动服务并设为开机自启 - name: Start Httpd Service service: name: httpd state: started enabled: yes #放行端口 - name: Configure Firewall firewalld: zone: public ports: 8080/tcp permanent: yes immediate: yes state: enabled 1234#检查语法ansible-playbook --syntax -i hosts httpd.yml#执行playbookansible-playbook -i hosts httpd.yml 4.2 利用playbook部署nfs 准备好nfs配置文件 123cd /root/project1cat exports/root/nfs_data 192.168.88.0/24(rw,no_root_squash) 编写nfs.yml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#nfs主机- hosts: 192.168.88.133 tasks: - name: Create Share Directory file: path: /root/nfs_data mode: 0744 owner: root group: root recurse: yes state: directory - name: Install NFS Service yum: name: nfs-utils state: latest - name: Configure NFS copy: src: ./exports dest: /etc/exports backup: yes - name: Start NFS Service service: name: nfs state: started enabled: yes - name: Stop Firewall Service service: name: firewalld state: stopped#挂载nfs目录的主机- hosts: 192.168.88.134 tasks: - name: Create Share Directory file: path: /root/nfs_data state: directory - name: Install NFS Service yum: name: nfs-utils state: latest - name: Start NFS Service service: name: nfs state: started enabled: yes - name: Mount NFS Share Directory mount: path: /root/nfs_data src: 192.168.88.133:/root/nfs_data fstype: nfs opts: defaults state: mounted 执行playbook 1ansible-playbook -i hosts nfs.yml 五、Ansible的变量5.1 在play中设置变量123456789101112- hosts: cqm vars: - web_package: httpd - ftp_package: vsftpd tasks: - name: Install {{ web_package }} and {{ ftp_package }} Service yum: name: - &quot;{{ web_package }}&quot; - &quot;{{ ftp_package }}&quot; state: latest 5.2 在vars_file中设置变量123cat vars.ymlweb_package: httpdftp_package: vsftpd 1234567891011- hosts: cqm vars_files: - ./vars.yml tasks: - name: Install {{ web_package }} and {{ ftp_package }} Service yum: name: - &quot;{{ web_package }}&quot; - &quot;{{ ftp_package }}&quot; state: latest 5.3 通过inventory主机清单设置变量 创建两个变量目录 12mkdir hosts_varsmkdir groups_vars 在groups_vars目录中针对某个组创建变量 12345cat group_vars/cqmweb_package: httpdftp_package: vsftpd#groups_vars目录中也可以新建all来设置变量，这样所有的主机组都可以调用 在hosts_vars目录中针对某个主机创建变量 123cat hosts_vars/192.168.88.133web_package: httpdftp_package: vsftpd 5.4 在执行playbook时通过 -e 参数设置变量1234567- hosts: cqm tasks: - name: Install {{ web_package }} Service yum: name: - &quot;{{ web_package }}&quot; state: latest 1ansible-playbook -i hosts -e 'web_package=httpd' test.yml 5.5 ansible变量的优先级优先级由上至下递减 -e（外置传参） vars_files play中的vars hosts_vars groups_vars中的某个主机组 groups_vars中的all 5.6 register注册变量1234567891011121314151617181920- hosts: cqm tasks: - name: Install Apache Service yum: name: httpd state: latest - name: Start Apache Service service: name: httpd state: started - name: Check Apache Process shell: ps -ef | grep httpd #将结果储存到变量里 register: check_apache #利用debug模块调用 - name: Output check_apache debug: #输出msg中的stdout_lines msg: &quot;{{ check_apache.stdout_lines }}&quot; 5.7 facts变量facts变量是ansible控制端采集被控端的变量，可以直接调用 为了方便可以将facts变量写到文本里 1ansible -i hosts cqm -m setup &gt; fasts.txt 六、Ansible语句6.1 条件判断whencentos安装apache是httpd，而ubuntu安装apache则是httpd2，所以这时候就需要条件判断了 例一：不同系统安装apache 12345678910111213141516- hosts: cqm tasks - name: Centos Install Apache Service yum: name: httpd state: latest when: - ( ansible_distribution == &quot;CentOS&quot; ) - name: Utunbu Install Apache2 Service yum: name: httpd2 state: latest when: - ( ansible_distribution == &quot;Utunbu&quot; ) 例二：给主机名带有web的主机配置yum源 1234567891011- hosts: cqm tasks: - name: Configure Cqm Yum Repo yum_repository: name: cqm description: cqm yum repo baseurl: http://www.cqmmmmm.com gpgcheck: no enabled: yes when: - ( ansible_fqdn is match (&quot;web*&quot;) ) 例三：判断nfs服务是否启动，没有则启动，否则重启 利用echo $？的返回值来查看是否启动 12345678910111213- hosts: cqm tasks: - name: Check NFS Service Status shell: systemctl is-active nfs ignore_errors: yes register: check_nfs - name: Restart NFS Service service: name: nfs state: restarted when: - ( check_nfs.rc == 0 ) 6.2 循环语句with_itemswith_items可以实现循环，可以减少playbook中同样模块的使用次数 例一：利用with_items重启nginx和php服务 123456789- hosts: cqm tasks: - name: Restart Nginx and PHP Service service: name: {{ item }} state: restarted with_items: - nginx - php-fpm 例二：利用变量循环安装多个服务 12345678910- hosts: cqm tasks: - name: Install Nginx and Mysql Service yum: name: {{ pack }} state: latest vars: pack: - nginx - mysql-server 例三：利用循环创建多个用户 12345678910- hosts: cqm tasks: - name: Create Multiple Users user: name: {{ item.name }} group: {{ item.group }} state: present with_items: - { name: 'aaa', group: 'aaa' } - { name: 'bbb', group: 'bbb' } 6.3 触发器handlershandlers是ansible的触发器，配合notify使用。 handlers只有在playbook执行到最后才会执行，简单来说可以将handlers看作一个特殊的tasks，只有在notify指定的某个模块运行了才会触发handlers中的模块。 例一：修改nginx.conf的话就重启nginx服务 1234567891011121314151617- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes notify: Restart Nginx Service handlers: - name: Restart Nginx Service service: name: nginx state: restarted 6.4 tags标签对tasks指定标签，可以在执行playbook的时候指定执行哪个tags的任务。 例一：利用tags来执行开启nginx和php服务 123456789101112- hosts: cqm tasks: - name: Start Nginx and PHP Service service: name: {{ item }} state: started enabled: yes with_items: - nginx - php-fpm tags: start_services...... 1ansible-playbook -i hosts cqm -t 'start_services' test.yml 如果要指定不执行哪个标签的任务，添加参数–skip-tags 1ansible-playbook -i hosts cqm --skip-tags 'start_services' test.yml 6.5 包含include如果每个playbook都会用到重启某个服务的任务，那么每个playbook都要写一次，利用include就只用写一次，让每个playbook调用即可。 例一：准备一个重启nginx的yml文件来给各个playbook调用 1vim nginx_restart.yml 1234- name: Restart Nginx Service service: name: nginx state: restarted 1234567891011- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes include: ./restart_nginx.yml 6.6 错误忽略ignore_errors就是字面意思- -，尽管某个任务出错了，也会继续执行下面的任务。 例一：忽略某个任务 123456- hosts: cqm tasks: - name: ignore errors test command: /bin/false ignore_errors: yes...... 6.7 错误模块failfail模块是一个专门用来“执行失败”的模块，我们都知道在shell中只要添加exit就可以停止执行，而playbook只有在某个任务出错了才会停止执行，这时候就可以利用fail来停止playbook的执行。 例一：利用fail来实现exit的功能 1234567891011121314151617181920- hosts: cqm tasks: - name: debug1 debug: msg: &quot;1&quot; - name: debug2 debug: msg: &quot;2&quot; #执行该模块后后面的debug都会执行 - name: fail fail: msg: &quot; this is fail module test &quot; - name: debug3 debug: msg: &quot;3&quot; - name: debug4 debug: msg: &quot;4&quot; 6.8 错误改变failed_whenfailed_when的作用就是将条件成立的任务状态设置为失败。 例一：判断是否输出了error，是则设置为任务失败 12345678910111213141516- hosts: cqm tasks: - name: debug1 debug: msg: &quot;i am debug1&quot; #因为输出里包含了error，所以条件成立将该任务设置为了fail，playbook中止 - name: Output Error shell: echo 'this is error' register: output_error failed_when: - ( 'error' in output_error.stdout ) - name: debug2 debug: msg: &quot;i am debug2&quot; 6.9 错误处理changed_whenchanged_when的作用就是将条件成立的任务状态设置为changed。 我们在调用handlers的时候，只有任务状态为changed才会调用，这时候就可以用changed_when改变任务状态为changed，就可以实现调用handlers了。 例一：改变任务状态为changed而调用触发器 12345678910111213141516171819- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes notify: Restart Nginx Service #尽管文件没有改变，任务状态为ok也会被改为changed而调用handlers changed_when: yes handlers: - name: Restart Nginx Service service: name: nginx state: restarted changed_when也可以使任务永远不会是changed。 例二：使任务永远为ok 12345678910111213141516171819- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes notify: Restart Nginx Service #尽管文件发生了改变，任务状态为changed也会被改为ok,永远不会调用handlers changed_when: false handlers: - name: Restart Nginx Service service: name: nginx state: restarted 七、Ansible的rolesroles就是通过分别将变量、文件、任务、模块及处理器放置于单独的目录中、并可以便捷地include它们的一种机制。 roles的目录结构： files：存放普通文件，比如copy调用的文件 handlers：触发器 meta：依赖关系 tasks：任务 templates：存放含有变量的文件 vars：变量 在每个目录里的yml文件都必须命名为main.yml 一键生成roles目录 1ansible-galaxy role init test 八、利用Ansible搭建Kodcloud项目架构图 8.1 准备项目环境 基本配置 123mkdir -p /root/project/{host_vars,group_vars}cd /root/projectcp /etc/ansible/ansible.cfg . 配置inventory 12345vim hosts[web]192.168.88.133[db]192.168.88.134 配置通用变量 123456vim group_vars/allnfs_server_ip: 192.168.88.134redis_server_ip: 192.168.88.134ip_address_range: 192.168.88.0/24web_user: wwwweb_group: www 创建各个role文件目录 1mkdir -p {db_base,redis,mariadb,nfs_server,web_base,nginx,php,nfs_client,kodcloud}/{files,handlers,tasks,templates,vars} 8.2 配置roles文件1vim kod.yml 1234567891011121314151617181920212223242526272829303132- hosts: db roles: - role: db_base tags: db_base - role: redis tags: redis - role: mariadb tags: mariadb - role: nfs_server tags: nfs_server- hosts: web roles: - role: web_base tags: web_base - role: nginx tags: nginx - role: php tags: php - role: nfs_client tags: nfs_client - role: kodcloud tags: kodcloud 8.3 配置db端基本环境 编写tasks 1vim db_base/tasks/main.yml 1234- name: Stop Firewall Service service: name: firewalld state: stopped 8.4 配置db端redis服务 编写tasks 1vim redis/tasks/main.yml 1234567891011121314151617- name: Install Redis Service yum: name: redis state: latest- name: Configure Redis Service template: src: redis.conf.j2 dest: /etc/redis.conf backup: yes notify: Restart Redis Service- name: Start Redis Service service: name: redis state: started enabled: yes 编写templates 123456vim redis/temlpates/redis.conf.j2egrep -v '^#|^$' redis/templates/redis.conf.j2bind {{ ansible_default_ipv4.address }}protected-mode yesport 6379...... 编写handlers 1cat redis/handlers/main.yml 1234- name: Restart Redis Service service: name: redis state: restarted 8.5 配置db端mariadb服务 编写tasks 1vim mariadb/tasks/main.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243- name: Install Mariadb Service yum: name: '{{ item }}' state: latest with_items: - mariadb - mariadb-server - MySQL-python- name: Configure Mariadb Service copy: src: my.cnf.j2 dest: /etc/my.cnf backup: yes- name: Start Mariadb Service service: name: mariadb state: started- name: Configure Mariadb Root User mysql_user: user: root password: toortoor- name: Create {{ website }} Databases mysql_db: login_user: root login_password: toortoor name: '{{ website }}' state: present collation: utf8_bin encoding: utf8- name: Create {{ website }} DB User mysql_user: login_user: root login_password: toortoor name: '{{ web_db_user }}' password: '{{ web_db_pass }}' host: '192.168.88.%' priv: '*.*:ALL,GRANT' state: present 编写files 1vim mariadb/files/my.cnf.j2 123456789101112131415161718[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# Settings user and group are ignored when systemd is used.# If you need to run mysqld under a different user or group,# customize your systemd unit file for mariadb according to the# instructions in http://fedoraproject.org/wiki/Systemd[mysqld_safe]log-error=/var/log/mariadb/mariadb.logpid-file=/var/run/mariadb/mariadb.pid## include all files from the config directory#!includedir /etc/my.cnf.d 编写vars 1vim mariadb/vars/main.yml 123website: kodcloudweb_db_user: kodcloudweb_db_pass: toortoor 8.6 配置db端nfs服务 编写tasks 1vim nfs_server/tasks/main.yml 123456789101112131415161718192021222324- name: Create NFS Share Directory file: path: '{{ nfs_share_directory }}' mode: 0757 recurse: yes state: directory- name: Install NFS Service yum: name: nfs-utils state: latest- name: Configure NFS template: src: exports.j2 dest: /etc/exports backup: yes notify: Restarted NFS Service- name: Start NFS Service service: name: nfs state: started enabled: yes 编写handlers 1vim nfs_server/handlers/main.yml 1234- name: Restarted NFS Service service: name: nfs state: restarted 编写vars 1vim nfs_server/vars/main.yml 1nfs_share_directory: /root/nfs_share 编写temlpates 12vim nfs_server/templates/exports.j2{{ nfs_share_directory }} {{ ip_address_range }}(rw) 8.7 配置web端基本环境 编写tasks 1vim web_base/tasks/main.yml 123456789101112- name: Create {{ web_group }} Group group: name: '{{ web_group }}' state: present- name: Create {{ web_user }} User user: name: '{{ web_user }}' group: '{{ web_group }}' state: present create_home: no shell: /sbin/nologin 8.8 配置web端nginx服务 编写tasks 1vim nginx/tasks/main.yml 1234567891011121314151617181920212223242526272829303132333435363738- name: Configure Firewall firewalld: zone: public port: 80/tcp permanent: yes immediate: yes state: enabled- name: Configure Nginx YUM Repo yum_repository: name: nginx description: Nginx YUM Repo file: nginx baseurl: http://nginx.org/packages/centos/7/$basearch/ gpgcheck: no enabled: yes state: present- name: Install Nginx Service yum: name: nginx state: latest- name: Configure Nginx template: src: '{{ item.src }}' dest: '{{ item.dest }}' backup: yes notify: Restart Nginx Service with_items: - { src: 'nginx.conf.j2', dest: '/etc/nginx/nginx.conf' } - { src: 'default.conf.j2', dest: '/etc/nginx/conf.d/default.conf' }- name: Started Nginx Service service: name: nginx state: started enabled: yes 编写handlers 1vim nginx/handlers/main.yml 1234- name: Restart Nginx Service service: name: nginx state: restarted 编写templates 1234567891011121314151617181920212223242526272829303132vim nginx/templates/nginx.conf.j2user {{ web_user }};worker_processes {{ ansible_processor_count * 1 }};error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events { worker_connections {{ ansible_processor_count * 1024 }};}http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546vim nginx/templates/default.conf.j2server { listen {{ nginx_port }}; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.php index.html index.htm; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { root /usr/share/nginx/html; # fastcgi_pass 127.0.0.1:9000; fastcgi_pass unix:/etc/nginx/php-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #}} 编写vars 1vim nginx/vars/main.yml 1nginx_port: 80 8.9 配置web端php服务 编写tasks 1vim php/tasks/main.yml 123456789101112131415161718192021222324252627282930313233343536373839- name: Configure PHP YUM Repo yum: name: http://rpms.remirepo.net/enterprise/remi-release-7.rpm state: present- name: Install PHP Service yum: name: '{{ item }}' state: present with_items: - '{{ php }}' - &quot;{{ php_version }}-cli&quot; - &quot;{{ php_version }}-common&quot; - &quot;{{ php_version }}-devel&quot; - &quot;{{ php_version }}-embedded&quot; - &quot;{{ php_version }}-gd&quot; - &quot;{{ php_version }}-mcrypt&quot; - &quot;{{ php_version }}-mbstring&quot; - &quot;{{ php_version }}-pdo&quot; - &quot;{{ php_version }}-xml&quot; - &quot;{{ php_version }}-fpm&quot; - &quot;{{ php_version }}-mysqlnd&quot; - &quot;{{ php_version }}-opcache&quot; - &quot;{{ php_version }}-pecl-memcached&quot; - &quot;{{ php_version }}-pecl-redis&quot; - &quot;{{ php_version }}-pecl-mongodb&quot;- name: Configure PHP Service copy: src: www.conf.j2 dest: &quot;{{ php_route }}/php-fpm.d/www.conf&quot; backup: yes notify: Restart Nginx and PHP Service- name: Start PHP Service service: name: &quot;{{ php_version }}-fpm&quot; state: started enabled: yes 编写handlers 1vim php/handlers/main.yml 1234567- name: Restart Nginx and PHP Service service: name: '{{ item }}' state: restarted with_items: - &quot;{{ php_version }}-fpm&quot; - nginx 编写files 123456789vim php/files/www.conf.j2[www]user = wwwgroup = wwwlisten = /etc/nginx/php-fpm.socklisten.owner = wwwlisten.group = wwwlisten.mode = 0660...... 编写vars 1vim php/vars/main.yml 123php: php74php_version: php74-phpphp_route: /etc/opt/remi/php74 8.10 配置web端nfs服务 编写tasks 1vim nfs_client/tasks/main.yml 1234567891011121314151617181920212223- name: Create NFS Share Directory file: path: '{{ nfs_share_directory }}' state: directory- name: Install NFS Service yum: name: nfs-utils state: latest- name: Start NFS Service service: name: nfs state: started enabled: yes- name: Mount NFS Share Directory mount: path: '{{ nfs_share_directory }}' src: &quot;{{ nfs_server_ip }}:{{ nfs_share_directory }}&quot; fstype: nfs opts: defaults state: mounted 编写vars 1vim nfs_client/tasks/main.yml 1nfs_share_directory: /root/nfs_share 8.11 配置web端kodcloud 编写tasks 1vim kodcloud/tasks/main.yml 123456789101112131415161718192021- name: Create Kodcloud Directory file: path: /usr/share/nginx/html/kodcloud owner: www group: www mode: 0777 state: directory- name: Input Kodcloud File unarchive: src: '{{ kod_version }}' dest: /usr/share/nginx/html/kodcloud owner: www group: www mode: 0777- name: Configure Nginx Virtual Host copy: src: kodcloud.conf.j2 dest: /etc/nginx/conf.d/kodcloud.conf notify: Restart Nginx and PHP Service 编写handlers 1vim kodcloud/handlers/main.yml 1234567- name: Restart Nginx and PHP Service service: name: '{{ item }}' state: restarted with_items: - nginx - &quot;{{ php_version }}-fpm&quot; 编写vars 1vim kodcloud/vars/main.yml 12kod_version: kodbox.1.15.zipphp_version: php74-php 编写files 1wget http://static.kodcloud.com/update/download/kodbox.1.15.zip kodcloud/files/kodbox.1.15.zip 12345678910vim kodcloud/files/kodcloud.conf.j2server { listen 80; server_name localhost; location / { root /usr/share/nginx/html/kodcloud; index index.php index.html index.htm; }} 8.12 执行playbook1ansible-playbook -i hosts kod.yml 8.13 测试 登录到web安装kodcloud 登录kodcloud","link":"/2024/02/18/ansible/"},{"title":"RKE2 使用 Custom CA 部署","text":"参考文档：https://docs.rke2.io/zh/security/certificates#using-custom-ca-certificates 在注册首台 RKE2 节点时，rke2 进程会检查 /var/lib/rancher/rke2/server/tls 目录中是否已存在相关证书文件；若不存在，就会生成相关证书供 Kubernetes 使用。 如果有自定义证书的需求，也可以提前将生成好的证书放置到该目录中。rke2 会检测到证书已存在，从而跳过证书生成流程，直接使用这些自定义证书。 生成自定义证书： 1234567891011121314151617181920cat &lt;&lt;EOF &gt; config[v3_ca]subjectKeyIdentifier = hashauthorityKeyIdentifier = keyid:alwaysbasicConstraints = critical, CA:truekeyUsage = critical, digitalSignature, keyEncipherment, keyCertSignEOFopenssl genrsa -out root-ca.key 4096openssl req -x509 -new -nodes -sha256 -days 7300 \\ -subj &quot;/CN=rke2-root-ca&quot; \\ -key root-ca.key \\ -out root-ca.pem \\ -config config \\ -extensions v3_camkdir -pv /var/lib/rancher/rke2/server/tlscp root-ca.pem root-ca.key /var/lib/rancher/rke2/server/tls 通过脚本，使用根 CA 证书生成其他证书： 12# 该脚本会检测是否存在 root-ca.pem root-ca.key，不存在则会自动生成curl -sL https://github.com/k3s-io/k3s/raw/master/contrib/util/generate-custom-ca-certs.sh | PRODUCT=rke2 bash - 生成后的证书文件如下，即可进行节点注册： 12345678910111213141516171819root@test-0:/var/lib/rancher/rke2/server/tls# ls -lhtotal 76K-rw-r----- 1 root root 4.9K Apr 15 09:51 client-ca.crt-rw------- 1 root root 227 Apr 15 09:51 client-ca.key-rw-r----- 1 root root 1.3K Apr 15 09:51 client-ca.pemdrwxr-x--- 2 root root 126 Apr 15 09:51 etcd-rw-r----- 1 root root 3.6K Apr 15 09:51 intermediate-ca.crt-rw------- 1 root root 3.2K Apr 15 09:51 intermediate-ca.key-rw-r----- 1 root root 1.9K Apr 15 09:51 intermediate-ca.pem-rw-r----- 1 root root 4.9K Apr 15 09:51 request-header-ca.crt-rw------- 1 root root 227 Apr 15 09:51 request-header-ca.key-rw-r----- 1 root root 1.3K Apr 15 09:51 request-header-ca.pem-rw-r----- 1 root root 1.8K Apr 15 09:51 root-ca.crt-rw------- 1 root root 3.2K Apr 15 09:50 root-ca.key-rw-r--r-- 1 root root 1.8K Apr 15 09:50 root-ca.pem-rw-r----- 1 root root 4.9K Apr 15 09:51 server-ca.crt-rw------- 1 root root 227 Apr 15 09:51 server-ca.key-rw-r----- 1 root root 1.3K Apr 15 09:51 server-ca.pem-rw------- 1 root root 1.7K Apr 15 09:51 service.key 如果是通过 Rancher 创建的 RKE2 集群，注册完成后，cattle-cluster-agent 可能会存在报错： 123...400 Bad Request: Request Header Or Cookie Too Large... 这是因为自签名证书导致的请求头过大而请求失败，调整 Local 集群 Ingress 相关参数即可，RKE2 可以通过下面的命令调整： 1234567891011121314cat &lt;&lt;EOF | kubectl apply -f -apiVersion: helm.cattle.io/v1kind: HelmChartConfigmetadata: name: rke2-ingress-nginx namespace: kube-systemspec: valuesContent: |- controller: config: large-client-header-buffers: &quot;4 64k&quot; http2-max-field-size: &quot;32k&quot; http2-max-header-size: &quot;64k&quot;EOF","link":"/2025/04/15/RKE2-%E4%BD%BF%E7%94%A8-Custom-CA-%E9%83%A8%E7%BD%B2/"},{"title":"cgroup netns mountns随记","text":"CgroupCgroup 即 Control Group，是一种 Linux 内核机制，它允许对进程进行资源控制和管理。Cgroup 可以限制进程的 CPU 使用率、内存使用量、磁盘 I/O 等资源。 Cgroup 主要作用: 资源控制: 可以限制进程的资源使用量，防止单个进程占用过多资源导致系统资源枯竭 资源隔离: 可以将不同类型的进程隔离到不同的 cgroup 中，以便更好地管理资源 资源统计: 可以收集 cgroup 中进程的资源使用情况统计数据，用于分析和优化资源分配 NetnsNetns 即 Network Namespace，是一种 Linux 内核机制，它允许在不同的进程之间隔离网络资源。这意味着每个 Network Namespace 都有自己的独立网络配置，例如 IP 地址、端口号和路由表。 Netns 主要作用: 隔离不同用户的网络连接 隔离不同应用程序的网络连接 创建虚拟网络 MountnsMountns 即 Mount namespace，是一种 Linux 内核机制，它允许在不同的进程之间隔离挂载点。这意味着每个 mount namespace 都有自己的独立挂载视图，即使它们位于同一个物理主机上。 Mountns 主要作用: 隔离不同用户的挂载点 隔离不同应用程序的挂载点 创建沙箱环境 总结Cgroup、Netns、Mountns 都是 Linux 内核机制，用于处理不同类型资源的隔离。 Cgroup 可以限制进程的 CPU/内存/磁盘IO Netns 可以隔离进程的网络连接 Mountns 可以隔离进程的挂载点","link":"/2024/03/31/cgroup-netns-mountns%E9%9A%8F%E8%AE%B0/"},{"title":"Cisco","text":"在 Cisco 中，交换机设备主要分为以下几种模式： 用户模式，刚进入设备都是用户模式，只能用来看一些统计信息 1Switch&gt; 特权模式，可以查看并修改配置 12Switch&gt;enableSwitch# 全局配置模式，可以修改主机名等 12Switch#config terminalSwitch(config)# 接口模式，可以对指定接口进行配置 12Switch(config)#int f0/1Switch(config-if)# 1. Vlan划分Vlan 即虚拟局域网，能够有效防止广播风暴的发生。 Vlan 的划分是在交换机上进行，实际上就是给指定端口进行划分。 在划分 vlan 前，可以看到左边的主机与右边的主机都是能够通信的 接下来将左右边的主机分别划分为 vlan10、vlan20，进入全局配置模式 1234567891011121314151617# 创建vlanSwitch(config)#vlan 10Switch(config-vlan)#exitSwitch(config)#vlan 20Switch(config-vlan)#exit# 划分vlanSwitch(config)#int range f0/1-3Switch(config-if-range)#switchport mode accessSwitch(config-if-range)#switchport access vlan 10Switch(config-if-range)#exitSwitch(config)#int range f0/4-6Switch(config-if-range)#switchport mode accessSwitch(config-if-range)#switchport access vlan 20Switch(config-if-range)#exit# 查看配置Switch#show running-config... 再去进行 ping 测试连通性，会发现只能 ping 通同一 vlan 下的主机 2. 分类地址的通信IP 地址分为五类，如果不是同一网络号下的地址则不能实现通信，如下图 这时候可以在中间添加一台路由器，来作为各主机之间的默认网关（默认路由），即可实现通信。 路由配置 123456789Router&gt;enRouter#conf t# 进入端口进行配置Router(config)#int g0/0Router(config-if)#ip add 192.168.88.1 255.255.255.0Router(config-if)#no shutdownRouter(config)#int g0/1Router(config-if)#ip add 172.16.0.1 255.255.0.0Router(config-if)#no shutdown 主机配置默认网关 连通性测试，第一个 ICMP 包会丢失是因为路由器会发送 ARP 包去寻找目标主机的 Mac 地址，所以在规定时间内没有收到回送报文就会超时 3. 划分子网子网的划分实际上就是在原有的基础上进行更小的划分，划分子网后，通过使用掩码，把子网隐藏起来，使得从外部看网络没有变化，这就是子网掩码。 如下图，左右边主机看似好像是在同一网络号下，但由于子网掩码是 255.255.255.192，所以之间是不能够通信的，只有 IP 地址与子网掩码进行位与的操作之后网络号相同的才能够进行通信，例如右边两台主机与子网掩码进行位与后网络号都是 192.168.88.64，即处于同一网络号下能够进行通信。 每个子网内可以使用的 IP 个数由子网号来判断，例如 255.255.255.192，192 转化为二进制为 11000000，有 6 个 0，即 IP 个数为 2 ** 6 = 64 个，常见子网掩码有以下几个（子网掩码：可用主机 IP 个数）： 255.255.255.128：126 255.255.255.192：62 255.255.255.224：30 255.255.255.240：14 255.255.255.248：6 255.255.255.252：2 要解决上图网络通信问题，只需要在中间添加个路由器即可。 路由器配置 12345678Router&gt;enRouter#conf tRouter(config)#int g0/0Router(config-if)#ip add 192.168.88.12 255.255.255.192Router(config-if)#no shutdownRouter(config)#int g0/1Router(config-if)#ip add 192.168.88.102 255.255.255.192Router(config-if)#no shutdown 主机添加默认网关 测试连通性 4. 构造超网（路由聚合）CIDR 地址格式为 xxx.xxx.xxx.xxx/xx，如 192.168.88.10/24，24 用来判断地址块信息，二进制后的 IP 前 24 位不变，后 8 位为 0，即可得出最小地址，后 8 位为 1 时，即可得出最大地址，即 192.168.88.0、192.168.88.255。 如下图，再没有配置静态路由时，上下网络的主机是不能够与右边的主机通信的，这时候添加静态路由，设置下一跳地址即可实现通信。 左边路由配置 1Router(config)#ip route 192.168.88.196 255.255.255.252 192.168.88.194 右边路由配置 12Router(config)#ip route 192.168.88.0 255.255.255.128 192.168.88.193Router(config)#ip route 192.168.88.128 255.255.255.192 192.168.88.193 测试连通性 从右边路由配置可以看出，两条静态路由可以进行路由聚合，即 192.168.88 前三个字节是相同的，那么聚合后的地址块为 192.168.88.0/24，路由配置为 1Router(config)#ip route 192.168.88.0 255.255.255.0 192.168.88.193 5. 特定路由和默认路由特定路由即将数据报转发给特定的目标地址，而默认路由即除了特定的目标地址外都转发给默认的路由，流程如下图。 在以下网络中，目的地址只要是 192.168.4 网段，都会下一跳给 R1 路由器，再由 R2 进行路由转发。 R1 路由配置 12Router(config)#ip route 192.168.4.1 255.255.255.255 10.0.1.1Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.0.1 R2 路由配置 1Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.1.2 R3 路由配置 123Router(config)#ip route 192.168.0.0 255.255.255.0 10.0.0.2Router(config)#ip route 192.168.4.0 255.255.255.0 10.0.0.2Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.2.2 R4 路由配置 1Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.2.1 6. 黑洞路由如果静态路由配置了不存在的网段，那么就可能会导致路由环路的问题，这时候可以添加一个黑洞路由进行解决，如下，只要目的地址是 192.168.88.0/24 地址快的数据包都会被丢弃。 1Router(config)#ip route 192.168.88.0 255.255.255.0 null0 7. 路由信息协议RIPRIP 是一种分布式的基于距离向量的路由选择协议，属于内部网关协议，主要适用于小规模的网络环境。 在 RIP 协议中，每次数据的发送都会选择跳数最少的路由，即经过节点最少的路由线路。 如下网络中，在给所有主机和路由端口配好 IP 后，还是不能够通信的，给每个路由器设置宣告地址段，即配置 RIP，路由器就会发送 RIP 报文来算出每个网段之间的最短路径，并写入路由表中，最终路径为 PC1 -&gt; R1 -&gt; R3 -&gt; PC2。 在每个路由器设置 RIP 1234Router(config)#router ripRouter(config-router)#network 10.0.0.0Router(config-router)#network 192.168.2.0Router(config-router)#network 192.168.1.0 路由表中 R 代表的就是 RIP 协议写入的路由 1Router#show ip route 8. 开放最短路径优先OSFPOSPF 也属于内部网关协议，用于在单一自治系统 AS 内决策路由，是基于链路状态的动态路由选择协议。 在 OSPF 中，当链路状态发生变化就会采用洪泛法去更新信息，每个路由器都与相邻的路由器成邻居关系，邻居再相互发送链路状态信息形成邻接关系，之后各自根据最短路径算法算出路由，放在OSPF路由表，OSPF路由与其他路由比较后优的加入全局路由表。整个过程使用了五种报文、三个阶段、四张表。 在以下网络中，通过 OSPF 配置路由后的路径为 PC1 -&gt; R1 -&gt; R2 -&gt; R3 -&gt; PC2，因为 R1 与 R3 之间采用的是串行接口，是低速链路。 R1 路由器配置 123456# 100为指定进程，可在1-65535之间选择Router(config)#route ospf 100# 0.0.0.255为反子网掩码，area为地区号Router(config-router)#network 10.0.0.0 0.0.0.255 area 0Router(config-router)#network 192.168.1.0 0.0.0.255 area 0Router(config-router)#network 10.0.1.0 0.0.0.255 area 0 R2 路由器配置 1234Router(config)#route ospf 100Router(config-router)#network 10.0.0.0 0.0.0.255 area 0Router(config-router)#network 10.0.1.0 0.0.0.255 area 0Router(config-router)#network 10.0.2.0 0.0.0.255 area 0 R3 路由器配置 1234Router(config)#route ospf 100Router(config-router)#network 10.0.1.0 0.0.0.255 area 0Router(config-router)#network 10.0.2.0 0.0.0.255 area 0Router(config-router)#network 192.168.2.0 0.0.0.255 area 0 通过测试可以看到路由线路的选择 如果线路断了会自动更新路由表，这时候再看线路的选择就会不一样，但依旧可以联通 9. 边界网关协议BGPBGP 是一种外部网关协议，是基于自治系统 AS 之间的路由协议，BGP交换的网络可达性信息提供了足够的信息来检测路由回路并根据性能优先和策略约束对路由进行决策。 每个 AS 会设置一个 BGP 发言人，即一个路由器，用来与相邻 AS 的 BGP 发言人交换网络可达性的信息。 如下图网络，分别有三个 AS，要将不同 AS 之间进行联通，就可以通过 BGP 来实现。 AS 100 中路由配置 1234567# 100为自治系统编号Router(config)#route bgp 100# 指定邻居，IP即与相邻路由器的连接端口IP，200/300为相邻AS的编号Router(config-router)#neighbor 10.0.1.2 remote-as 200Router(config-router)#neighbor 10.0.0.2 remote-as 300# 将自身AS中的网络信息通报出去Router(config-router)#network 192.168.10.0 mask 255.255.255.0 AS 200 中路由配置 123Router(config)#route bgp 200Router(config-router)#neighbor 10.0.1.1 remote-as 100Router(config-router)#network 192.168.20.0 mask 255.255.255.0 AS 300 中路由配置 123Router(config)#route bgp 300Router(config-router)#neighbor 10.0.0.1 remote-as 100Router(config-router)#network 192.168.30.0 mask 255.255.255.0 查看 BGP 路由信息，可以看到 B 代表的 BGP 路由信息 12345Router(config-router)#do show ip route...B 192.168.10.0/24 [20/0] via 10.0.1.1, 00:00:00B 192.168.30.0/24 [20/0] via 10.0.1.1, 00:00:00... 10. 多臂路由实现vlan之间的通信vlan 之间如果网段不同，就无法进行通信，可以在中间添加一台路由器来做双方的网关，已多臂路由的方式实现不同 vlan 不同网段之间的通信。 多臂路由即一个端口对应一个 vlan，如下图。 11. 单臂路由实现vlan之间的通信单臂路由与多臂路由的原理是相同的，区别在于在多臂路由中，交换机到路由器的链路配置的是 access，在单臂路由中用一个主干 trunk 链路替代。 在连接交换机的路由器上配置 12345678910111213# 创建逻辑子接口Router(config)#int g0/0.1# 可接受id为10的vlan数据包，并封装成802.1q桢进行转发Router(config-subif)#encapsulation dot1Q 10Router(config-subif)#ip add 192.168.1.254 255.255.255.0Router(config-subif)#exitRouter(config)#int g0/0.2Router(config-subif)#encapsulation dot1Q 20Router(config-subif)#ip add 192.168.2.254 255.255.255.0Router(config-subif)#exit# 开启接口Router(config)#int g0/0Router(config-if)#no shutdown 在交换机上将与路由器相接的端口设置为 trunk 口 12Switch(config)#int f0/7Switch(config-if)#switchport mode trunk 连通性测试 12. 三层交换机实现vlan之间的通信三层交换机就是具有部分路由器功能的交换机。 三层交换机配置 1234567891011121314151617181920212223Switch(config)#vlan 10Switch(config-vlan)#exit# 进入vlan添加IPSwitch(config)#int vlan10Switch(config-if)#ip add 192.168.1.254 255.255.255.0Switch(config-if)#no shutdownSwitch(config-if)#exitSwitch(config)#vlan 20Switch(config-vlan)#exitSwitch(config)#int vlan20Switch(config-if)#ip add 192.168.2.254 255.255.255.0Switch(config-if)#no shutdownSwitch(config-if)#exit Switch(config)#int range f0/1-3Switch(config-if-range)#switchport mode access Switch(config-if-range)#switchport access vlan 10Switch(config-if-range)#Switch(config-if-range)#exitSwitch(config)#int range f0/4-6Switch(config-if-range)#switchport mode access Switch(config-if-range)#switchport access vlan 20# 开启路由功能Switch(config)#ip routing 测试连通性","link":"/2024/02/18/cisco/"},{"title":"calico的三种模式","text":"Calico 的运行支持三种模式: vxlan ipip bgp VXLAN封包解包: 在 vxlan 设备上将 Pod 发来的数据包源、目的 mac 地址替换为本主机 vxlan 网卡和对端 vxlan 网卡的 mac 地址 优缺点: vxlan 的数据包会封装在 udp 数据包中，所以要求节点之间三层互通，支持跨网段。但封包解包的过程会有一定网络性能损耗 IPIP封包解包: 在 tun0 设备上将 Pod 发来的数据包的 mac 层去除，留下 ip 层并使用宿主机的 ip 进行一次封包 优缺点: 要求节点之间三层互通，支持跨网段。但封包解包的过程会有一定网络性能损耗 BGP封包解包: 不进行封包解包 优缺点: 通过 bgp 协议就可以支持节点之间的三层互通。 CrossSubnetvxlan 和 ipip 都支持配置 CrossSubnet 模式，这种模式下，只有跨网段节点的 Pod 之间的通信才会进行封包解包，而同一网段节点的 Pod 之间则使用 bgp 模式进行通信，能够在一定程度上提高网络性能。","link":"/2024/03/31/calico%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F/"},{"title":"Docker","text":"一. Docker介绍1.1 Docker是什么 Docker 是一个开源的应用容器引擎，基于 Go 语言并遵从 Apache2.0 协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 二. Docker的基本操作2.1 Docker的安装121.安装Docker依赖环境yum -y install yum-utils device-mapper-persistent-data lvm2 122.下载Docker镜像源yum-config-manager --add-repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 1233.安装Dockeryum makecache fastyum -y install docker-ce 1234.启动Docker并设为开机自启systemctl start dockersystemctl enable docker 123455.测试运行hello worlddocker run hello-world...Hello from Docker!... 2.2 Docker的中央仓库1、Docker官方（hub.docker.com）：镜像最全，下载最慢 2、国内镜像网站：网易蜂巢（163yun.com） ​ daocloud（hub.daocloud.io） 3、公司内部私服拉取镜像： 123456781.配置/etc/docker/daemon.json{ &quot;registry-mirrors&quot;:[&quot;https://registry.docker-cn.com&quot;], &quot;insecure-registries&quot;:[&quot;ip:port&quot;]}2.重启两个服务systemctl daemon-reloadsystemctl restart docker 2.3 Docker镜像的操作12341.拉取镜像到本地docker pull 镜像名称[:tag]举个栗子docker pull daocloud.io/library/nginx:1.18.0 122.查看本地镜像docker images 123.删除本地镜像docker rmi 镜像id(image id) 1234.本地镜像的导入导出docker save -o 导出路径 镜像iddocker load -i 镜像文件 125.给镜像添加标签docker tag 镜像id repository:tag 2.4 Docker容器的操作1234561.运行容器docker run 镜像id/镜像名称[:tag]docker run -d -p 宿主机端口:容器端口 --name 容器名称 镜像id/镜像名称[:tag]-d:代表后台运行容器-p:为了映射linux端口和容器端口--name:指定容器名称 12342.查看容器docker ps -qa-a:查看所有容器，包括没有运行的-q:只看容器id 1233.查看容器日志docker logs -f 容器id-f:可以滚动查看容器日志的最后几行 124.进入容器内部docker exec -it 容器id bash 123455.停止/删除容器docker stop 容器iddocker stop $(docker ps -qa)docker rm 容器iddocker rm $(docker ps -qa) 126.启动容器docker start 容器id 三. Docker数据卷3.1 数据卷操作数据卷就是将宿主机的一个目录映射到容器的一个目录中 可以在宿主机的目录中直接操作，容器的目录和文件也会跟着改变 创建好的数据卷会存放在一个默认的目录下：/var/lib/docker/volumes/ 121.创建数据卷docker volume create 数据卷名称 12342.查看数据卷详细信息docker volume inspect 数据卷名称查看全部数据卷docker volume ls 123.删除数据卷docker volume rm 数据卷名称 12345678910114.应用数据卷#如果映射数据卷不存在，docker会自动创建，会将容器内部的文件存储在数据卷中docker run -v 数据卷名称:容器内部路径 镜像id#直接指定一个路径作为存放路径，建议使用这种docker run -v 数据卷路径:容器内部路径 镜像id-v:映射数据卷#可以在指定路径后面加上权限，一旦设置权限就不可以改了，举个栗子docker run -v /root/nginx:/etc/nginx:rw nginxro:只读rw:可读可写 3.2 实现容器之间数据同步–volumes-from可以实现容器之间的数据同步，即使有一台容器挂了也不会影响到数据，别的容器仍然会有数据在。 举个栗子 12#启动容器一，并创建测试目录docker run -d -v /root/docker/test1:/test1 -v /root/docker/test2:/test2 --name nginx01 镜像id 123#启动容器二，挂载容器一的目录docker run -d --name nginx02 --volumes-from nginx01 镜像iddocker exec -it 容器二id bash 进入容器二内部后可以看见容器一的test1和test2，这时候无论在容器一还是容器二生成或删除文件都会进行同步 用docker inspect 容器id可以看到两个容器映射的都是同一个目录 四. Dockerfile自定义镜像Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 4.1 创建自定义镜像123456789101112131.创建一个Dockerfile文件，并指定自定义镜像信息#Dockerfile中常用信息FROM: 这个镜像的妈妈是谁?(指定基础镜像)MAINTAINER: 告诉别人，谁负责养它?(指定维护者信息)CMD: 你想让它干啥(指定容器run前要做什么，只有最后一个会生效)ENTRYPOINT: 你想让它干啥(指定容器run前要做什么，可以追加命令)RUN: 你想让它干啥(指定容器run后要做什么)COPY: 给它点创业资金(拷贝文件到镜像内部)ADD: 给它点创业资金(拷贝文件到镜像内部，如果是压缩包会解压了再就行拷贝)WORKDIR: 我是cd(配置工作目录，栗子:如果WORKDIR配置了/home,那么COPY和ADD使用.作为拷贝路径的话都会拷贝到/home下)VOLUME: 给一个存放行李的地方(设置数据卷，挂载到主机目录)EXPOSE: 给一个门(指定对外开放的端口号)ENV: 设置环境变量(environment，也就是参数-e) 122.在linux上通过docker指定镜像docker build -t 镜像名称:[tag] . 12343.举个nginx栗子cat DockerfileFROM daocloud.io/library/nginx:1.18.0COPY test.html /usr/share/nginx/html/test.html 4.2 CMD和ENTRYPOINT的区别用CMD和ENTRYPOINT做同一个测试 CMD 121.创建Dockerfile文件vim /root/docker/Dockerfile 1232.编写FROM daocloud.io/library/centos:latestCMD [&quot;ls&quot;,&quot;-a&quot;] 123.生成镜像文件docker build -f /root/docker/Dockerfile -t cmd_centos:1.0 . 124.启动容器，会发现执行了ls -adocker run cmd_centos镜像id 如果启动镜像的时候再加参数会报错，但完整的命令就可以 ENTRYPOINT 123#除了Dockerfile中的CMD换成了ENTRYPOINT外其它都一样FROM daocloud.io/library/centos:latestENTRYPOINT [&quot;ls&quot;,&quot;-a&quot;] 这时候我们再追加参数发现可以了，这就是ENTRYPOINT比起CMD有可以追加参数的不同 4.3 制作Tomcat镜像 准备tomcat压缩包和jdk压缩包 编写Dockerfile文件 12345678910111213141516171819FROM daocloud.io/library/centos:7MAINTAINER cqm's diy tomcat&lt;chenqiming13@qq.com&gt;ADD jdk-8u271-linux-x64.tar.gz /usr/localADD apache-tomcat-9.0.41.tar.gz /usr/localRUN yum -y install vimENV MYPATH /usr/localWORKDIR $MYPATH #进入容器后就会进入MYPATH目录ENV JAVA_HOME /usr/local/jdk1.8.0_271ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.41ENV CATALINA_BASH /usr/local/apache-tomcat-9.0.41ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/binEXPOSE 8080CMD /usr/local/apache-tomcat-9.0.41/bin/startup.sh &amp;&amp; tailf /usr/local/apache-tomcat-9.0.41/bin/logs/catalina.out 生成tomcat镜像 1docker build -t cqm_tomcat:1.0 . 启动容器 1docker run -d -p 8080:8080 --name cqm_tomcat -v /root/tomcat/test:/usr/local/apache-tomcat-9.0.41/webapps/test -v /root/tomcat/logs:/usr/local/apache-tomcat-9.0.41/logs cqm_tomcat镜像id 访问测试 上传项目 123cd /root/tomcat/testmkdir WEB-INF &amp;&amp; cd WEB-INFvim web.xml 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; id=&quot;WebApp_ID&quot; version=&quot;2.5&quot;&gt;&lt;display-name&gt;db&lt;/display-name&gt;&lt;welcome-file-list&gt;&lt;welcome-file&gt;index.html&lt;/welcome-file&gt;&lt;welcome-file&gt;index.htm&lt;/welcome-file&gt;&lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;&lt;welcome-file&gt;default.html&lt;/welcome-file&gt;&lt;welcome-file&gt;default.htm&lt;/welcome-file&gt;&lt;welcome-file&gt;default.jsp&lt;/welcome-file&gt;&lt;/welcome-file-list&gt;&lt;/web-app&gt; 1vim ../index.hsp 123456789101112131415&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;cqm's tomcat&lt;/title&gt;&lt;/head&gt;&lt;body&gt;this is cqm's tomcat web!!!&lt;br/&gt;&lt;%System.out.println(&quot;-----this is cqm's tomcat web logs-----&quot;);%&gt;&lt;/body&gt;&lt;/html&gt; 查看项目是否部署成功 4.4 上传自定义镜像到Docker Hub12345671.登录docker账号docker login -u docker用户名...Login Succeeded#退出账号docker logout 12345671.登录docker账号docker login -u docker用户名...Login Succeeded#退出账号docker logout 122.给要上传的镜像添加标签docker tag 镜像id docker用户名/镜像名称:版本号 123.上传镜像到Docker Hubdocker push 镜像id docker用户名/镜像名称:版本号 五. Docker网络Docker使用桥接模式在Linux主机上添加一个docker0的网卡，而Docker每启动一个容器就会添加一个新的网卡，使用的是evth-pair技术。evth-pair就是一对虚拟设备接口，添加的网卡都是成对出现的，一段连接镜像，一段连接docker0，所以evth-pair就是一座桥梁，用来连接各种虚拟网络设备。 利用ip addr来查看Linux主机的网卡情况，可以看到docker0网卡 我们拉取一个镜像，并启动 我们在使用ip addr可以看到出现了对新的网卡 进入容器内部查看网卡可以看到是对应的 由此可知docker网络的架构 5.1创建自定义网络 创建自定义网络 1234docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 cqmnet--driver:网络类型，bridge为桥接--subnet:配置子网--gateway:配置网关地址 启动两个tomcat容器并指定自定义网络 12docker run -d -p 8081:8080 --net cqmnet --name tomcat01 镜像iddocker run -d -p 8082:8080 --net cqmnet --name tomcat02 镜像id 测试两个容器之间是否连通 5.2 实现docker0和自定义网络之间的连通 创建两个docker0网段的容器 12docker run -d -p 8083:8080 --name tomcat03 镜像iddocker run -d -p 8084:8080 --name tomcat04 镜像id 利用coonnect进行连通 1docker network connect cqmnet tomcat03 这时候再看自定义网络的信息，可以看到tomcat03被添加进来了 测试 结论，由此可知，connect是将一个网络中的容器添加到另一个网络中，通俗来讲就是该容器同时拥有两个网络的地址，就实现了两个网络之间的互通。 5.3 不同主机之间容器的互联方法一：静态路由 静态路由方法原理就是将容器的请求转发到指定的主机上，再由主机转发给容器。 修改 daemon.json 添加以下内容，使之每个主机的 docker 默认网段不同 1234# 主机一&quot;bip&quot;: &quot;172.17.1.10/24&quot;# 主机二&quot;bip&quot;: &quot;172.17.2.10/24&quot; 添加路由规则 1234# 主机一，网关为主机二的IP地址，即访问172.17.2.0网段的数据包都会被转发到主机二上route add -net 172.17.2.0 netmask 255.255.255.0 gw 192.168.88.130# 主机二route add -net 172.17.1.0 netmask 255.255.255.0 gw 192.168.88.135 方法二：Macvlan Macvlan 原理是将一张物理网卡虚拟成多块虚拟网卡的技术，使得虚拟网卡与物理网卡的参数等都相同。 开启网卡的混杂模式 1ip link set ens33 promisc [on/off] / ifconfig ens33 [-]promisc 创建 Macvlan 网络 12# 子网和网关均和主机一致docker network create --driver macvlan --subnet 192.168.88.0/24 --gateway 192.168.88.1 -o parent=ens33 [network_name] 创建容器需要指定网络 1docker run -d -it --name centos --net [network_name] --ip 192.168.88.10 centos:7 5.4 Overlayoverlay 是一种在网络架构上进行叠加的虚拟化技术，即在原有的网络框架上叠加一层虚拟网络，从而实现应用在虚拟网络上承载，以及与其它网络业务分离。 在这个overlay网络模式里面，有一个类似于服务网关的地址，然后把这个包转发到物理服务器这个地址，最终通过路由和交换，到达另一个服务器的ip地址。 实现 overlay 网络需要有注册发现中心的键值数据库支持，可以用 consul、etcd、zookeeper 等。 区别： consul：服务发现/全局的分布式 key-value 存储。自带 DNS 查询服务，可以跨数据中心。提供节点的健康检查，可以实现动态的 consul 节点增减，docker 官方的用例推荐。 etcd：服务发现/全局的分布式 key-value 存储。静态的服务发现，如果实现动态的新增etcd节点，需要依赖第三方组件。 5.4.1 consulconsul 用于微服务下的服务治理，主要特点有：服务发现、服务配置、健康检查、键值存储、安全服务通信、多数据中心等。 注意：overlay 所需内核版本为 3.18+ 安装 consul 12curl -O https://releases.hashicorp.com/consul/1.10.3/consul_1.10.3_linux_amd64.zipunzip consul_1.10.3_linux_amd64.zip 准备 config.json 文件 123456789101112131415{ &quot;advertise_addr&quot;: &quot;192.168.88.130&quot;, # 更改我们向群集中其他节点通告的地址 &quot;bind_addr&quot;: &quot;192.168.88.130&quot;, # 内部群集通信绑定的地址，这是群集中所有其他节点都应该可以访问的IP地址 &quot;data_dir&quot;: &quot;/opt/consul&quot;, # 数据存放目录 &quot;server&quot;: true, # 是否是server agent节点 &quot;node_name&quot;: &quot;server1&quot;, # 节点名称 &quot;enable_syslog&quot;: true, &quot;enable_debug&quot;: true, &quot;log_level&quot;: &quot;info&quot;, # 日志级别 &quot;bootstrap_expect&quot;: 3, # 提供数据中心中预期服务器的数量，即需要3台consul server agent &quot;start_join&quot;: [&quot;192.168.88.130&quot;, &quot;192.168.88.135&quot;, &quot;192.168.88.100&quot;], # 启动时指定节点地址的字符串数组，指定是其他的consul server agent的地址 &quot;retry_join&quot;: [&quot;192.168.88.130&quot;, &quot;192.168.88.135&quot;, &quot;192.168.88.100&quot;], # 允许start_join时失败时，继续重新连接 &quot;ui&quot;: true, # 启动ui界面 &quot;client_addr&quot;: &quot;0.0.0.0&quot; # Consul将绑定客户端接口的地址，包括HTTP和DNS服务器} 启动 consul 查看状态 12345./consul agent -config-dir ./config.json# 查看集群状态./consul members# 查看leader./consul operator raft list-peers 访问 consul ui 配置 docker/daemon.json 123# 虽然是consul集群，但只能填写一个consul节点的信息&quot;cluster-store&quot;: &quot;consul://192.168.88.130:8500&quot;,&quot;cluster-advertise&quot;: &quot;192.168.88.130:2376&quot; 创建 docker 网络，去到其它主机查看 docker 网络可以看到是已经同步的 1docker network create --driver overlay consulnet 在不同的主机创建容器测试是否能够互联 1docker run -d --name centos1 --network consulnet centos:7 在容器内部可以看到有两张网卡，eth0 是 overlay 创建的，即一个 vxlan；eth1 则是服务网关的网卡 5.4.2 etcdetcd 是一个高可以的键值存储系统，主要用于分享配置和服务发现。 安装 etcd 1234567891011ETCD_VER=v3.5.1GITHUB_URL=https://github.com/etcd-io/etcd/releases/downloadDOWNLOAD_URL=${GOOGLE_URL}curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gztar -zvxf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz/tmp/etcd-download-test/etcd --version/tmp/etcd-download-test/etcdctl version/tmp/etcd-download-test/etcdutl version 启动 etcd 12345678910111213141516HOST=192.168.88.130CLUSTER_IPS=(192.168.88.130 192.168.88.135 192.168.88.136)ETCD_DATADIR=/root/etcd/datanohup ./etcd -name node1 \\ -initial-advertise-peer-urls http://$HOST:2380 \\ -listen-peer-urls http://$HOST:2380 \\ -listen-client-urls http://$HOST:2379,http://127.0.0.1:2379 \\ -advertise-client-urls http://$HOST:2379 \\ -initial-cluster-token etcd-cluster \\ -initial-cluster node1=http://${CLUSTER_IPS[0]}:2380,node2=http://${CLUSTER_IPS[1]}:2380,node3=http://${CLUSTER_IPS[2]}:2380 \\ -initial-cluster-state new \\ -data-dir $ETCD_DATADIR &amp;# listen-peer-urls:节点与节点之间数据交换, 因此需要监听在其他节点可以访问的IP地址上，默认端口2380# listen-client-urls:用户客户机访问etcd数据, 一般监听在本地, 如果需要集中管理, 可以监听在管理服务器可以访问的IP地址上，默认端口2379# initial-advertise-peer-urls:表示节点监听其他节点同步信号的地址，默认端口2380# advertise-client-urls:在加入proxy节点后, 会使用该广播地址, 因此需要监听在一个proxy节点可以访问的IP地址上，默认端口2379 查看 etcd 集群信息和健康状态 12./etcdctl member list./etcdctl cluster-health 修改 docker/daemon.json 12&quot;cluster-store&quot;: &quot;etcd://192.168.88.130:2379&quot;,&quot;cluster-advertise&quot;: &quot;192.168.88.130:2376&quot; 查看是否注册到 etcd 12./etcdctl ls //docker 创建 docker 网络 1docker network create --driver overlay etcdnet 创建容器测试是否互通 1docker run -d -it --name centos1 --network etcdnet centos:7 5.5 Flannel与Calico在 Kubernetes 中，使用的网络组件主要为 Flannel 和 Calico 两种，都可以实现不同主机之间容器的通信，使用前提是要有 etcd。 5.5.1 FlannelFlannel 网络架构如下 添加网络配置到 etcd 123# Network:flannel网段# Type:网络类型./etcdctl set /coreos.com/network/config '{ &quot;Network&quot;: &quot;10.0.0.0/24&quot;, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;} }' 安装 Flannel 1curl -O https://github.com/flannel-io/flannel/releases/download/v0.15.0/flannel-v0.15.0-linux-amd64.tar.gz 启动 Flannel，会生成 /run/flannel/subnet.env，路由表也会加上新规则 12# -etcd-endpoints:etcd地址./flanneld -etcd-endpoints &quot;http://192.168.88.30:2379&quot; 生成 docker 参数信息 1./mk-docker-opts.sh -d /root/etcd/docker -c 修改 /usr/lib/systemd/system/docker.service 12345vim /usr/lib/systemd/system/docker.service# 添加EnvironmentFile=/root/etcd/docker# 在ExecStart后加上ExecStart=/usr/bin/dockerd $DOCKER_OPTS 重启 docker 后测试容器是否能够互联 5.5.2 CalicoCalico 是目前企业在 k8s 集群上使用的最多的容器互通网络方案，比起 Flannel 能够实现更过复杂的需求。 Calico 是一种纯三层的解决方案，因此避免了与二层方案相关的数据包封装操作，中间没有任何 NAT 和 overlay，直接走 TCP/IP 协议栈，通过 iptables 实现复杂的网络规则。 Calico 组件如下： Felix：运行在每一台主机中，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。 etcd：分布式键值存储，保证网络数据的一致性。 BIDR（BGP Client）：每一个主机都会有个 BIDR，用来实现不同的路由协议，Calico 监听主机上 Felix 注入的信息，然后通过 BGP 协议告诉其它节点，从而实现互联。 BGP Route Reflector：用于解决网络规模过大的组件。 Calico 工作模式： IPIP：将 IP 数据包再次封装到一个 IP 包里，相当于一个基于网络层的网桥，普通的网桥是基于 Mac 地址的，而 IPIP 能将两端的路由做成一个 tunnel，将其连接。 BGP：即边界网关协议。 在所有主机上下载 Calico Controller 1curl -o calicoctl -O -L &quot;https://github.com/projectcalico/calicoctl/releases/download/v3.21.0/calicoctl&quot; 配置 calicoctl.cfg 12345apiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec: etcdEndpoints: http://192.168.88.30:2379 初始化 Calico 12# ip:宿主机ip./calicoctl node run --ip=192.168.88.30 -c ./calicoctl.cfg 查看 Calico 状态 1./calicoctl node status 由于新版本的 Calico 不再支持 docker 单独使用，但通过插件可以实现 1234git clone https://github.com/projectcalico/libnetwork-plugin.gitcd libnetwork-plugin# 下载插件镜像make image 运行 libnetwork 容器 1docker run -d --privileged --name calico-docker-network-plugin --net host --restart always -v /run/docker/plugins:/run/docker/plugins -e ETCD_ENDPOINTS=http://192.168.88.30:2379 calico/libnetwork-plugin 创建 ippool 123456789101112131415# 查看calico目前地址池./calicoctl get ippools# 创建地址池yaml文件cat ip.yamlapiVersion: projectcalico.org/v3kind: IPPoolmetadata: name: ippool-testspec: cidr: 10.0.0.0/24 ipipMode: Nerver natOutgoing: true disabled: false nodeSelector: all()./calicoctl apply -f ip.yaml 创建 GlobalNetworkPolicy 12345678910apiVersion: projectcalico.org/v3kind: GlobalNetworkPolicymetadata: name: gnp-testspec: selector: all ingress: - action: Allow egress: - action: Allow 创建 docker 网络 1docker create network -d calico --ipam-driver calico-ipam --subnet 10.0.0.0/24 caliconet 关闭 ipv6 内核 1echo 1 &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6 创建容器测试 1docker run -d -it --name centos1 --network caliconet centos:7 六. Docker-ComposeCompose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。 6.1 Docker-Compose使用步骤1231.使用 Dockerfile 定义应用程序的环境2.使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行3.执行 docker-compose up 命令来启动并运行整个应用程序 6.2 安装Docker-Compose121.下载curl -L https://get.daocloud.io/docker/compose/releases/download/1.27.4/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose 122.赋权chmod +x /usr/local/bin/docker-compose 6.3 编写Docker-Compose管理MySQL和Nginx容器123456789101112131415161718192021222324252627282930313233341.创建数据卷目录mkdir /root/docker_mysql_nginx/docker_mysql/mysql_datamkdir /root/docker_mysql_nginx/docker_nginx/nginx_confmkdir /root/docker_mysql_nginx/docker_nginx/nginx_html2.编写vim /root/docker_mysql_nginx/docker-compose.ymlversion: '3.8' #指定本 yml 依从的 compose 哪个版本制定的 #可参考https://docs.docker.com/compose/compose-file/services: mysql: #指定服务的名称 restart: always #只要docker启动，这个容器就一起启动 image: daocloud.io/library/mysql:5.7.4 #指定镜像路径 container_name: mysql #指定容器名称 ports: - 3306:3306 #指定端口号的映射 environment: MYSQL_ROOT_PASSWORD: toortoor #指定mysql的root用户登录密码 TZ: Asia/Shanghai #指定时区 volumes: - /root/docker_mysql_nginx/docker_mysql/mysql_data:/var/lib/mysql #映射数据卷 nginx: restart: always image: daocloud.io/library/nginx:1.18.0 container_name: nginx ports: - 80:80 environment: TZ: Asia/Shanghai volumes: - /root/docker_mysql_nginx/docker_nginx/nginx_conf:/etc/nginx - /root/docker_mysql_nginx/docker_nginx/nginx_html:/usr/share/nginx/html 6.4 使用Docker-Compose命令在使用Docker-Compose命令时，系统会在当前目录下寻找yml文件 1231.应用Docker-Compose启动容器docker-compose up -d-d:在后台运行容器 122.关闭并删除容器docker-compose down 123.开启|关闭|重启已由docker-compose管理的容器docker-compose start | stop | restart 124.查看由docker-compose管理的容器docker-compose ps 1235.查看docker-compose日志docker-compose logs -f-f:可以滚动查看 6.5 Docker-Compose配合Dockerfile使用docker-compose.yml 123456789101112version: '3.8'services: nginx: build: context: ./ #指定在当前目录下寻找dockerfile dockerfile: Dockerfile #指定dockerfile文件名 image: nginx:1.18.0 #使用上边制作好的镜像 container_name: nginx ports: - 8080:80 environment: TZ: Asia/Shanghai Dockerfile 12from daocloud.io/library/nginx:1.18.0copy test.html /usr/share/nginx/html/test.html 遇到的错误 12345678910111.ERROR: yaml.scanner.ScannerError: while scanning for the next tokenfound character '\\t' that cannot start any token#是因为yml文件里使用了tab，yml文件格式不允许使用，全部换成空格2.WARNING: Image for service nginx was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.#是因为运行docker-compose时自定义镜像不存在，会帮助生成自定义镜像#提供两种构建方法：#docker-compose build#docker-compose up --build 6.6 Docker-Compose部署wordpress 编写docker-compose.yml 1234567891011121314151617181920212223242526272829version: '3.8'services: db: container_name: mysql image: daocloud.io/library/mysql:latest volumes: - db_data:/var/lib/mysql restart: always ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: toortoor MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: toortoor wordpress: container_name: wordpress image: wordpress:latest ports: - 80:80 restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: toortoor WORDPRESS_DB_NAME: wordpressvolumes: db_data: {} 启动 1docker-compose up -d","link":"/2024/02/18/docker/"},{"title":"RKE1 部署随记","text":"部署 RKE1 前期准备 1234567# RKE1 二进制curl -LO &quot;https://github.com/rancher/rke/releases/download/v1.5.12/rke_linux-amd64&quot;mv rke_linux-amd64 /usr/local/bin/rke &amp;&amp; chmod +x /usr/local/bin/rke# 各节点安装 Dockercurl https://releases.rancher.com/install-docker/20.10.sh | sh 生成配置 1234567891011121314151617181920212223242526272829303132333435cat &lt;&lt;EOF &gt; cluster.yml# 旧版本 rke1 私钥类型不支持 rsa，需要选择 ed25519ssh_key_path: /root/.ssh/id_ed25519nodes: - address: 172.16.0.106 hostname_override: rke1-server-0 internal_address: 172.16.0.106 user: root role: - controlplane - etcd - worker - address: 172.16.0.105 hostname_override: rke1-server-1 internal_address: 172.16.0.105 user: root role: - controlplane - etcd - worker - address: 172.16.0.104 hostname_override: rke1-server-2 internal_address: 172.16.0.104 user: root role: - controlplane - etcd - workerprivate_registries: - url: registry.cn-hangzhou.aliyuncs.com is_default: truekubernetes_version: &quot;v1.20.15-rancher2-2&quot;network: plugin: calicoEOF 安装 RKE1 1rke up --config cluster.yml 方便后续运维配置 123456789101112131415161718192021222324docker cp kube-apiserver:usr/local/bin/kubectl /usr/local/bin/kubectlecho &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrcmkdir ~/.kubemv kube_config_cluster.yml ~/.kube/config# https://www.suse.com/support/kb/doc/?id=000020018# Rancher 2.7.14+/Rancher 2.8.5+, RKE 1.4.19+/RKE 1.5.10+kubectl --kubeconfig $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination &quot;/etc/kubernetes&quot; }}{{ .Source }}{{ end }}{{ end }}')/ssl/kubecfg-kube-node.yaml get secrets -n kube-system full-cluster-state -o json | jq -r .data.\\&quot;full-cluster-state\\&quot; | base64 -d | jq -r .currentState.certificatesBundle.\\&quot;kube-admin\\&quot;.config | sed -e &quot;/^[[:space:]]*server:/ s_:.*_: \\&quot;https://127.0.0.1:6443\\&quot;_&quot; &gt; ~/.kube/config# Without jq commanddocker run --rm --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination &quot;/etc/kubernetes&quot; }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro --entrypoint bash $(docker inspect $(docker images -q --filter=label=org.opencontainers.image.source=https://github.com/rancher/hyperkube) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get secret -n kube-system full-cluster-state -o json | jq -r .data.\\&quot;full-cluster-state\\&quot; | base64 -d | jq -r .currentState.certificatesBundle.\\&quot;kube-admin\\&quot;.config | sed -e &quot;/^[[:space:]]*server:/ s_:.*_: \\&quot;https://127.0.0.1:6443\\&quot;_&quot;' &gt; ~/.kube/config# Earlier versions of Rancher and RKEkubectl --kubeconfig $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination &quot;/etc/kubernetes&quot; }}{{ .Source }}{{ end }}{{ end }}')/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\&quot;full-cluster-state\\&quot; | jq -r .currentState.certificatesBundle.\\&quot;kube-admin\\&quot;.config | sed -e &quot;/^[[:space:]]*server:/ s_:.*_: \\&quot;https://127.0.0.1:6443\\&quot;_&quot; &gt; ~/.kube/config# Without jq commanddocker run --rm --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination &quot;/etc/kubernetes&quot; }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro --entrypoint bash $(docker inspect $(docker images -q --filter=label=org.opencontainers.image.source=https://github.com/rancher/hyperkube.git) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\&quot;full-cluster-state\\&quot; | jq -r .currentState.certificatesBundle.\\&quot;kube-admin\\&quot;.config | sed -e &quot;/^[[:space:]]*server:/ s_:.*_: \\&quot;https://127.0.0.1:6443\\&quot;_&quot;' &gt; ~/.kube/configcurl https://rancher-mirror.rancher.cn/helm/get-helm-3.sh | INSTALL_HELM_MIRROR=cn bash -s -- --version v3.17.1echo &quot;source &lt;(helm completion bash)&quot; &gt;&gt; ~/.bashrc 常见问题如果是 CentOS 和 RHEL 系统，默认不允许使用 root 用户进行安装，报错信息如下： 1WARN[0000] Failed to set up SSH tunneling for host [x.x.x.x]: Can’t retrieve Docker Info ，Failed to dial to /var/run/docker.sock: ssh: rejected: administratively prohibited (open failed) 需要准备其他用户： 1groupadd rancher &amp;&amp; useradd rancher -g rancher &amp;&amp; usermod -aG docker rancher 如果出现下面错误，是由于指定的 ssh_key_path 文件对应的主机不正确或对应的用户名不正确，可以检查下节点对应用户的 ~/.ssh/authorized_keys 文件是否正确： 1WARN[0000] Failed to set up SSH tunneling for host [x.x.x.x]: Can't retrieve Docker Info: error during connect: Get &quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info&quot;: Unable to access node with address [x.x.x.x:22] using SSH. Please check if you are able to SSH to the node using the specified SSH Private Key and if you have configured the correct SSH username. Error: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain 如果出现下面错误： 1WARN[0000] Failed to set up SSH tunneling for host [x.x.x.x]: Can't retrieve Docker Info: error during connect: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info: Unable to access the service on /var/run/docker.sock. The service might be still starting up. Error: ssh: rejected: connect failed (open failed) 需要在 /etc/ssh/sshd_config 文件中添加以下内容： 1AllowTcpForwarding yes 清理 iptables 规则1234567iptables -F \\ &amp;&amp; iptables -X \\ &amp;&amp; iptables -Z \\ &amp;&amp; iptables -F -t nat \\ &amp;&amp; iptables -X -t nat \\ &amp;&amp; iptables -Z -t nat \\ &amp;&amp; docker restart kube-proxy 清理节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#!/bin/bashKUBE_SVC='kubeletkube-schedulerkube-proxykube-controller-managerkube-apiserver'for kube_svc in ${KUBE_SVC};do # 停止服务 if [[ `systemctl is-active ${kube_svc}` == 'active' ]]; then systemctl stop ${kube_svc} fi # 禁止服务开机启动 if [[ `systemctl is-enabled ${kube_svc}` == 'enabled' ]]; then systemctl disable ${kube_svc} fidone# 停止所有容器docker stop $(docker ps -aq)# 删除所有容器docker rm -f $(docker ps -qa)# 删除所有容器卷docker volume rm $(docker volume ls -q)# 卸载mount目录for mount in $(mount | grep tmpfs | grep '/var/lib/kubelet' | awk '{ print $3 }') /var/lib/kubelet /var/lib/rancher;do umount $mount;done# 备份目录mv /etc/kubernetes /etc/kubernetes-bak-$(date +&quot;%Y%m%d%H%M&quot;)mv /var/lib/etcd /var/lib/etcd-bak-$(date +&quot;%Y%m%d%H%M&quot;)mv /var/lib/rancher /var/lib/rancher-bak-$(date +&quot;%Y%m%d%H%M&quot;)mv /opt/rke /opt/rke-bak-$(date +&quot;%Y%m%d%H%M&quot;)# 删除残留路径rm -rf /etc/ceph \\ /etc/cni \\ /opt/cni \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/log/containers \\ /var/log/kube-audit \\ /var/log/pods \\ /var/run/calico \\ /usr/libexec/kubernetes# 清理网络接口no_del_net_inter='lodocker0ethensbond'network_interface=`ls /sys/class/net`for net_inter in $network_interface;do if ! echo &quot;${no_del_net_inter}&quot; | grep -qE ${net_inter:0:3}; then ip link delete $net_inter fidone# 清理残留进程port_list='804436443237623792380847290991025010254'for port in $port_list;do pid=`netstat -atlnup | grep $port | awk '{print $7}' | awk -F '/' '{print $1}' | grep -v - | sort -rnk2 | uniq` if [[ -n $pid ]]; then kill -9 $pid fidonekube_pid=`ps -ef | grep -v grep | grep kube | awk '{print $2}'`if [[ -n $kube_pid ]]; then kill -9 $kube_pidfi# 清理Iptables表## 注意：如果节点Iptables有特殊配置，以下命令请谨慎操作sudo iptables --flushsudo iptables --flush --table natsudo iptables --flush --table filtersudo iptables --table nat --delete-chainsudo iptables --table filter --delete-chainsystemctl restart docker# 重启节点reboot","link":"/2024/09/05/RKE1-%E9%83%A8%E7%BD%B2%E9%9A%8F%E8%AE%B0/"},{"title":"CKA","text":"记录刷题- - Job：创建一个固定结束次数的并行 Job，共 2 个 pod，运行 45 completion，Job pod 打印“Beijing”，镜像自选。 12345678910111213141516171819apiVersion: batch/v1kind: Jobmetadata: name: jobspec: parallelism: 2 completions: 45 restartPolicy: Never template: metadata: name: job spec: containers: - name: job image: busybox:latest command: - 'sh' - '-c' - 'echo Beijing' initContainer：pod 的 container 在启动时会检查 volume 中某个目录是否存在某个文件，若不存在则退出，若存在，则运行。要求修改 pod 的 yaml，通过 initContainer 创建该文件，使pod运行。 1 将名为 ek8s-node-1 的 node 设置为不可用，并重新调度该 node 上所有运行的 pods。 1234# cordon:将node设置为不可用，将阻止新pod调度到该节点之上，但不会影响任何已经在其上的 pod，这是重启节点或者执行其他维护操作之前的一个有用的准备步骤，DaemonSet创建的pod能够容忍节点的不可调度属性。kubectl cordon ek8s-node-1# drain:从节点安全的驱逐所有pods。kubectl drain ek8s-node-1 --delete-local-data --ignore-daemonsets --force 在现有的 namespace app-team1 中创建一个名为 cicd-token 的新 ServiceAccount，创建一个 deployment-clusterrole 且只能够创建 Deployment、DaemonSet、StatefulSet 资源的 Cluster Role，并将新的 deployment-clusterrole 绑定到 cicd-token。 12345kubectl create sa cicd-token -n app-team1kubectl create clusterrole deployment-clusterrole --verb=create --resource=Deployment,StatefulSet,DaemonSetkubectl create rolebinding cicd-rolebinding --clusterrole=deployment-clusterrolr --serviceaccount=app-team1:cicd-token k8s master 1.20.0 升级到 1.20.1，并升级 kubelet、kubectl。 12345678910# 将节点设为不可调度性kubectl cordon mk8s-master-0kubectl drain mk8s-master-0 --igonre-daemonsets# 升级apt-get updateapt-get -y install kubeadm=1.20.1-00kubeadm update apply v1.20.1 --etcd-upgrade=falseapt-get -y install kubelet=1.20.1-00 kubectl=1.20.1-00# 将不可调度性去除kubectl uncordon mk8s-master-0 etcd 备份/恢复。 1234# 备份ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; snapshot save &lt;backup-file-location&gt;# 恢复ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; snapshot restore &lt;backup-file-location&gt; 在 internal 的命名空间下创建一个名为 allow-port-from-namespace 的 NetworkPolicy，此 NetworkPolicy 允许 internal 中的 pod 访问 echo 命名空间中 9000 的端口。 123# 先获取echo租户的labelskubectl get ns echo --show-labelsecho-key: echo-value 1234567891011121314151617apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-port-from-namespace namespace: internalspec: podSelector: {} policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: echo-key: echo-value ports: - protocol: TCP port: 9000 在 ing-internal 下创建一个 ingress 名为 ping，通过 5678 端口暴露 hello svc 下的 /hello。 123456789101112131415apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ping namespace: ing-internalspec: rules: - http: paths: - path: /hello backend: service: name: hello port: number: 5678 将 deployment webserver 扩展至 6 pods。 1kubectl scale deployment webserver --replicas 6 创建一个名为 nginx-kusc00401 的 pod，镜像为 nginx，调度到 disk=ssd 的节点上。 1234567891011121314apiVersion: v1kind: Podmetadata: name: nginx-kusc00401spec: containers: - name: nginx image: nginx:1.20.1 imagePullPolicy: Always ports: - name: http containerPort: 80 NodeSelector: disk=ssd 将一个现有的 pod 集成到 k8s 内置日志记录体系结构中（kubectl logs），使用 busybox 来将名为 sidecar 容器添加到名为 11-factor-app 的 pod 中，sidecar 需运行 /bin/sh -c tail -n+1 -f /var/log/11-factor-app.log ，使用安装在 /var/log 的 volume，使日志文件 11-factor-app.log 可用于 sidecar 容器。 12345678910111213141516171819202122232425262728293031323334apiVersion: v1kind: Podmetadata: name: 11-factor-appspec: volumes: - name: log emptyDir: {} containers: - name: count image: busybox command: - /bin/sh - -c - &gt; i=0; while true; do echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/11-factor-app.log; i=$((i+1)); sleep 1; done volumeMounts: - name: log mountPath: /var/log - name: sidecar image: busybox command: - /bin/sh - -c - 'tail -n+1 -f /var/log/11-factor-app.log' volumeMounts: - name: log mountPath: /var/log 通过 pod label name=cpu-utilizer 寻找 cpu 占用最多的 pod，并写入 /opt/tmp/tmp.txt。 1kubectl top pod -A -l name=cpu-utilizer --sort-by=cpu &gt;&gt; /opt/tmp/tmp.txt","link":"/2024/02/18/cka/"},{"title":"Jenkins","text":"Jenkins是一个开源的持续集成的服务器，Jenkins开源帮助我们自动构建各类项目。Jenkins强大的插件式，使得Jenkins可以集成很多软件，可能帮助我们持续集成我们的工程项目。 一、什么是CI、CD Devops也就是开发运维一体化，而随着Devops的兴起，出现了持续集成、持续交付以及持续部署的新方法，传统的软件开发和交付方法正在迅速变得过时。从历史上看，在敏捷时代，大多数公司会每月、每季度、每两年甚至每年发布部署或发布软件。然而，在DevOps时代，每周、每天、甚至每天多次。当SaaS正在占领世界时，我们可以轻松地动态更新应用程序，而无需强迫客户下载新组件。很多时候，他们甚至都不会意识到正在发生变化。开发团队通过软件交付流水线（Pipeline）实现自动化，以缩短交付周期，大多数团队都有自动化流程来检查代码并部署到新环境。 持续集成的重点是将各个开发人员的工作集合到一个代码仓库中。通常，每天都要进行几次，主要目的是尽早发现集成错误，使团队更加紧密结合，更好地协作。 持续交付的目的是最小化部署或释放过程中固有的摩擦。它的实现通常能够将构建部署的每个步骤自动化，以便任何时刻能够安全地完成代码发布（理想情况下）。 持续部署是一种更高程度的自动化，无论何时对代码进行重大更改，都会自动进行构建/部署。 而Jenkins就是来实现以上持续集成工作的服务器。 二、持续集成环境搭建Jenkins流程图： 2.1 Gitlab代码托管服务器安装Gitlab 是一个用于仓库管理系统的开源项目，使用Git作为代码管理工具，并在此基础上搭建起来的Web服务。 开启postfix服务（支持gitlab发信功能） 12systemctl start postfixsystemctl enable postfix 配置gitlab的yum源 123456789vim /etc/yum.repos.d/gitlab-cd.repo[gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/gpgcheck=0enabled=1yum clean allyum makecache 安装gitlab 1yum -y install gitlab-ce 修改gitlab配置文件 123456789vim /etc/gitlab/gitlab.rb#用户访问所使用的URL，域名或者IP地址external_url 'http://192.168.88.133'#时区gitlab_rails['time_zone'] = 'Asia/Shanghai'#启用SMTP邮箱功能gitlab_rails['smtp_enable'] = 'ture'#使用SSH协议拉取代码所使用的连接端口gitlab_rails['gitlab_shell_ssh_port'] = '22' 刷新配置 1gitlab-ctl reconfigure 启动服务 123gitlab-ctl restartgitlab-ctl status...... docker-compose.yaml 123456789101112131415161718version: '3.8'services: gitlab: container_name: gitlab image: gitlab/gitlab-ce:latest restart: always environment: GITLAB_OMNIBUS_CONFIG: | external_url 'http://192.168.88.30' gitlab_rails['time_zone'] = 'Asia/Shanghai' gitlab_rails['gitlab_shell_ssh_port'] = '22' ports: - '80:80' - '23:22' volumes: - '/root/cicd/gitlab/config:/etc/gitlab' - '/root/cicd/gitlab/logs:/var/log/gitlab' - '/root/cicd/gitlab/data:/var/opt/gitlab' 进入容器查看默认用户名和修改密码 123456789gitlab-rails console# 查找root用户u=User.where(id:1).first# 修改密码u.password='toortoor'# 确认修改密码u.password_confirmation='toortoor'# 保存修改u.save 2.2 Gitlab创建组、用户、项目Gitlab创建组： 添加组 设置组名和权限 Gitlab创建用户： 添加用户 配置选项 将用户添加到项目组 Gitlab创建项目： 在指定组中添加项目 设置项目名称 2.5 项目上传到Gitlab 2.4 Jenkins安装 安装JDK 1yum -y install java-1.8.0-openjdk* 放行端口 12firewall-cmd --zone=public --add-port=8888/tcp --permanentfirewall-cmd --reload 添加Jenkins官方源并安装 123wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.reporpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.keyyum -y install jenkins 修改配置文件 123vim /etc/sysconfig/jenkinsJENKINS_USER=&quot;root&quot;JENKINS_PORT=&quot;8888&quot; 查看初始密码 12cat /var/lib/jenkins/secrets/initialAdminPassword...... docker-compose.yaml 1234567891011version: '3.8'services: jenkins: container_name: jenkins image: jenkins:2.60.3 restart: always ports: - '8080:8080' - '50000:50000' volumes: - '/root/cicd/jenkins/home:/var/jenkins_home' 2.5 Jenkins中文插件安装 由于Jenkins官方下载插件很慢，我们修改为国内Jenkins插件地址，jenkins -&gt; Manage jenkins -&gt; Manage Plugins 更换配置文件中的地址 12345cp /var/lib/jenkins/updates/default.json /var/lib/jenkins/updates/default.json.backupsed -i 's/https:\\/\\/updates.jenkins.io\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g' /var/lib/jenkins/updates/default.json &amp;&amp; sed -i 's/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g' /var/lib/jenkins/updates/default.jsonsystemctl restart jenkins 安装中文插件 2.6 Jenkins用户权限管理 由于Jenkins功能较为简介，都要靠安装插件来丰富体验，所以用户管理需要安装role-based插件 在Configure Global Security开启刚刚安装的插件 在Manage and Assign Roles中创建角色baserole并分配具体权限 用户没加入角色前是没有访问权限的，将用户加入到角色baserole即可，但只有部分权限 2.7 Jenkins安装凭证管理插件 安装Credential Binding插件，安装完就可以看到多了凭据的功能 2.8 Jenkins普通用户密码认证 为了Jenkins能够拉取gitlab的代码，需要安装git插件 1yum -y install git 创建普通用户凭证，注意这里的用户是gitlab创建好的用户 创建项目添加普通用户凭证 build now构建 Jenkins主机上查看是否构建成功 2.9 Jenkins使用ssh免密认证 在Jenkins主机上生产公钥私钥 1ssh-keygen -t rsa 在gitlab上添加公钥 在Jenkins上添加ssh凭证 创建项目添加ssh凭证 build now构建项目后 2.10 Jenkins安装Maven 下载maven 1234wget https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gzmkdir maventar -xf apache-maven-3.6.3-bin.tar.gzcp apache-maven-3.6.3-bin.tar.gz/* maven/ 配置环境变量 123456vim /etc/profileexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdkexport MAVEN_HOME=/root/mavenexport PATH=$PATH:$JAVA_HOME/bin:/$MAVEN_HOME/binsource /etc/profilemvn -v 全局工具配置里新增jdk和maven 在系统配置里添加三个变量 修改maven配置文件 123456789mkdir /root/repovim /root/maven/conf/settings.xml&lt;localRepository&gt;/root/repo&lt;/localRepository&gt;&lt;mirror&gt; &lt;id&gt;aliyunmaven&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;阿里云公共仓库&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt;&lt;/mirror&gt; 测试maven构建项目 2.11 Tomcat安装和配置 安装jdk和tomcat 123456yum -y install java-1.8.0-openjdk*wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gztar -xf apache-tomcat-9.0.43.tar.gzmkdir /root/tomcatmv apache-tomcat-9.0.43.tar.gz/* /root/tomcat./root/tomcat/bin/startup.sh 添加tomcat管理用户 123456789vim /root/tomcat/conf/tomcat-users.xml &lt;role rolename=&quot;tomcat&quot;/&gt; &lt;role rolename=&quot;role1&quot;/&gt; &lt;role rolename=&quot;manager-gui&quot;/&gt; &lt;role rolename=&quot;manager-script&quot;/&gt; &lt;role rolename=&quot;admin-gui&quot;/&gt; &lt;role rolename=&quot;admin-script&quot;/&gt; &lt;role rolename=&quot;manager-status&quot;/&gt; &lt;user username=&quot;tomcat&quot; password=&quot;toortoor&quot; roles=&quot;manager-gui,manager-script,tomcat,admin-gui,admin-script&quot;/&gt; 12345vim /root/tomcat/webapps/manager/META-INF/context.xml&lt;!-- &lt;Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot; allow=&quot;127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1&quot; /&gt;--&gt; 测试 三、Jenkins构建项目Jenkins构建的项目类型主要为以下三种： 自由风格软件项目（freestyle project） Maven项目（maven project） 流水线项目（pipeline project） 3.1 自由风格软件项目构建 要将项目部署到tomcat服务器上，需要安装deploy to container插件 Jenkins -&gt; 新建items 使用ssh免密登录来拉取代码 使用maven编译打包 部署 再次构建后可以回到tomcat查看是否部署成功 3.2 Maven项目构建 安装Maven Integration插件 创建maven项目 构建设置，其余设置都相同 构建后查看是否部署成功 3.3 Pipeline project简介pipeline就是一套运行在Jenkins上的工作流框架，将独立运行的单个或多个节点的任务连接起来，实现复杂流程的编排和可视化的工作。 优点有： 自动地为所有分支创建流水线构建过程并拉取请求。 在流水线上代码复查/迭代 (以及剩余的源代码)。 对流水线进行审计跟踪。 该流水线的真正的源代码，可以被项目的多个成员查看和编辑。 3.4 Pipeline流水线项目构建 安装pipeline插件 新建流水线项目 声明式流水线 3.5 Jenkinsfile脚本文件pipeline脚本内容放在Jenkins服务器不好管理，所以就在项目添加一个Jenkinsfile文件并推送到gitlab上，实现直接拉取执行。 在项目下添加一个Jenkinsfile，将pipeline内容复制进去并推送到gitlab上 在pipeline项目中修改为在gitlab拉取Jenkinsfile文件 四、Jenkins构建细节4.1 Jenkins常用的构建触发器Jenkins内置4种构建触发器： 触发远程构建 其它工程构建后触发 定时构建 轮询SCM 4.2 触发远程构建 在项目中设置触发器 在浏览器输入JENKINS_URL/job/pipeline-project/build?token=TOKEN_NAME即可触发 4.3 其它工程构建后触发是指某一工程构建后才会触发构建，这里测试freestyle工程构建后触发pipeline构建 在pipeline配置构建后触发 freestyle工程构建后就会触发pipeline构建 4.4 定时构建五个*分别代表分时日月周，H/2则代表每分时日月周 4.5 轮询SCM和定时构建一样是设置时间来进行构建，不过轮询只有在代码仓库发生变化后才会触发，相当于定时检查 注意：轮询会定时扫描仓库的代码，增大系统的开销，不建议使用 4.6 Gitlab Hook自动触发构建SCM轮询是Jenkins服务器主动检测gitlab中的代码有没有发生变化，而gitlab的webhook可以实现代码变更后向Jenkins服务器主动发送构建请求，实现自动构建。 在Jenkins安装gitlab和gitlab hook插件 在gitlab配置中允许发出请求 在项目中添加第一步中的地址 配置Jenkins允许接受gitlab发送过来的请求 4.7 Jenkins参数化构建前边实操演示的都是默认从master下拉取代码，如果要实现在其他分支拉取代码，就需要设置参数化构建。 在Jenkins的项目中添加参数，这里选择字符串参数 修改Jenkinsfile文件中拉取代码部分的master为变量，并推送到gitlab 在项目中添加一个v1分支 在Jenkins中拉取v1代码构建 4.8 配置邮件服务器发送构建结果 安装Email Extension插件 在Jenkins中配置 在项目中创建一个email.html文件，添加以下内容并推送到gitlab中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;UTF-8&quot;&gt;&lt;title&gt;${PROJECT_NAME}-第${BUILD_NUMBER}次构建日志&lt;/title&gt;&lt;/head&gt; &lt;body leftmargin=&quot;8&quot; marginwidth=&quot;0&quot; topmargin=&quot;8&quot; marginheight=&quot;4&quot;&gt; &lt;table width=&quot;95%&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif&quot;&gt; &lt;tr&gt; &lt;td&gt;(本邮件是程序自动下发的，请勿回复！)&lt;br/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;h2&gt; &lt;font color=&quot;#0000FF&quot;&gt;构建结果 - ${BUILD_STATUS}&lt;/font&gt; &lt;/h2&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;br /&gt; &lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建信息&lt;/font&gt;&lt;/b&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;项目名称 ： ${PROJECT_NAME}&lt;/li&gt; &lt;li&gt;构建编号 ： 第${BUILD_NUMBER}次构建&lt;/li&gt; &lt;li&gt;触发原因： ${CAUSE}&lt;/li&gt; &lt;li&gt;构建日志： &lt;a href=&quot;${BUILD_URL}console&quot;&gt;${BUILD_URL}console&lt;/a&gt;&lt;/li&gt; &lt;li&gt;构建 Url ： &lt;a href=&quot;${BUILD_URL}&quot;&gt;${BUILD_URL}&lt;/a&gt;&lt;/li&gt; &lt;li&gt;工作目录 ： &lt;a href=&quot;${PROJECT_URL}ws&quot;&gt;${PROJECT_URL}ws&lt;/a&gt;&lt;/li&gt; &lt;li&gt;项目 Url ： &lt;a href=&quot;${PROJECT_URL}&quot;&gt;${PROJECT_URL}&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;Changes Since Last Successful Build:&lt;/font&gt;&lt;/b&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;历史变更记录 : &lt;a href=&quot;${PROJECT_URL}changes&quot;&gt;${PROJECT_URL}changes&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format=&quot;Changes for Build #%n:&lt;br /&gt;%c&lt;br /&gt;&quot;,showPaths=true,changesFormat=&quot;&lt;pre&gt;[%a]&lt;br /&gt;%m&lt;/pre&gt;&quot;,pathFormat=&quot; %p&quot;} &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建情况总览:&lt;/font&gt;&lt;/b&gt;${TEST_COUNTS,var=&quot;fail&quot;}&lt;br/&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;textarea cols=&quot;80&quot; rows=&quot;30&quot; readonly=&quot;readonly&quot; style=&quot;font-family: Courier New&quot;&gt;${BUILD_LOG,maxLines=23}&lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 在Jenkinsfile中添加发送email功能，post即指构建后操作 123456789post { always { emailext( subject: '构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!', body: '${FILE,path=&quot;email.html&quot;}', to: 'chenqiming13@qq.com' ) }} 构建测试是否收到邮件 五、Jenkins+SonarQube代码审查 sonarqube是一个用于管理代码质量的开放平台，可以快速定位代码中潜在的错误。 环境要求： JDK11 PostgreSQL 5.1 安装PostgreSQL 新版本的sonarqube不再支持MySQL，所以在Jenkins服务器上安装PostgreSQL并创建一个sonar数据库 1234567891011121314151617181920212223#安装postgresqlyum -y install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-redhat-repo-latest.noarch.rpmyum -y install postgresql96-server postgresql96-contrib#初始化postgresqlpostgresql-9.6-setup initdbsystemctl enable postgresql-9.6.servicesystemctl start postgresql-9.6.service#初始化之后会自动创建postgres用户，切换用户su - postgres#psql进入命令行模式配，\\q退出psqlalter user postgres with password 'toortoor';create database sonar;create user sonar;alter user sonar with password 'toortoor';alter role sonar createdb;alter role sonar superuser;alter role sonar createrole;alter database sonar owner to sonar;\\q 开启远程访问和远程连接 12345vim /var/lib/pgsql/9.6/data/postgresql.conflisten_addresses = '*'vim /var/lib/pgsql/9.6/data/pg_hba.confhost all all 127.0.0.1/32 trusthost all all 192.168.88.1/32 trust 重启服务 5.2 安装SonarQube 安装sonarqube 123456wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.7.1.42226.zipunzip sonarqube-8.7.1.42226.zipmv sonarqube-8.7.1.42226 sonarqubeuseradd sonarchown -R sonar:sonar sonarqube/*mv sonarqube /opt/sonarqube 修改sonar配置文件 123456vim sonarqube/conf/sonar.propertiessonar.jdbc.username=sonarsonar.jdbc.password=toortoorsonar.jdbc.url=jdbc:postgresql://localhost/sonarqube?currentSchema=my_schemasonar.jdbc.url=jdbc:postgresql://localhost/sonar?currentSchema=publicsonar.web.port=9000 启动sonarqube 123#只能用sonar用户启动su sonar./opt/sonarqube/bin/linux-x86-64/sonar.sh start 遇到的问题 123456#sonar用户线程数不够，*代表所有用户cat /etc/security/limits.conf* soft nofile 65536* hard nofile 65536* soft noproc 65535* hard noproc 65535 deploy.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374apiVersion: apps/v1kind: Deploymentmetadata: name: sonarqube labels: app: sonarqubespec: replicas: 1 selector: matchLabels: app: sonarqube template: metadata: name: sonarqube labels: app: sonarqube spec: initContainers: - name: init-sysctl image: busybox imagePullPolicy: IfNotPresent command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;] securityContext: privileged: true containers: - name: sonarqube image: sonarqube:lts-community imagePullPolicy: IfNotPresent ports: - containerPort: 9000 env: # 此处用到了集群现有的pg # 数据库用户sonarqube # 数据库名sonarqube - name: SONARQUBE_JDBC_USERNAME value: &quot;sonarqube&quot; - name: SONARQUBE_JDBC_PASSWORD value: &quot;dangerous&quot; - name: SONARQUBE_JDBC_URL value: &quot;jdbc:postgresql://dcs-installer-gitlab-postgresql.dcs-system:5432/sonarqube&quot; livenessProbe: httpGet: path: /sessions/new port: 9000 initialDelaySeconds: 60 periodSeconds: 30 readinessProbe: httpGet: path: /sessions/new port: 9000 initialDelaySeconds: 60 periodSeconds: 30 failureThreshold: 6 resources: limits: cpu: 2000m memory: 2048Mi requests: cpu: 1000m memory: 1024Mi ---apiVersion: v1kind: Servicemetadata: name: sonarqubespec: type: NodePort selector: app: sonarqube ports: - port: 9000 targetPort: 9000 protocol: TCP 5.3 Jenkins整合SonarQube 安装sonarqube scanner插件 在Jenkins安装sonarqube scanner 在sonarqube中生成密钥 在系统配置中配置sonarqube server，利用刚刚生成的密钥 5.4 实现代码审查非流水线项目添加代码审查 在项目中添加构建步骤 123456789101112131415161718192021#Must be unique in a given SonarQube instance# SonarQube创建项目时的keysonar.projectKey=freestyle_project#This is the name and version displayed in the SonarQube UI.# 项目名称sonar.projectName=freestyle_project# 项目版本sonar.projectVersion=1.0#Path is relative to the sonar-project.properties file.#This property is optional if sonar.modules is set.sonar.sources=.# 忽略扫描的目录sonar.exclusions=**/test/**,**/target/**sonar.java.source=11sonar.java.target=11#Encoding of the source code.Default is default system encoding.sonar.sourceEncoding=UTF-8 构建项目 回到sonarqube就可以看到提交的代码审查了 流水线项目添加代码审查 在项目中创建sonar-project.properties 1234567891011121314151617#Must be unique in a given SonarQube instancesonar.projectKey=pipeline_project#This is the name and version displayed in the SonarQube UI.sonar.projectName=pipeline_projectsonar.projectVersion=1.0#Path is relative to the sonar-project.properties file.#This property is optional if sonar.modules is set.sonar.sources=.sonar.exclusions=**/test/**,**/target/**sonar.java.source=11sonar.java.target=11#Encoding of the source code.Default is default system encoding.sonar.sourceEncoding=UTF-8 修改Jenkinsfile文件 123456789101112stage('code checking'){ steps { script { //引入scanner工具 scannerHome = tool 'sonar-scanner' } //引入sonarqube服务器环境 withSonarQubeEnv('sonarqube') { sh &quot;${scannerHome}/bin/sonar-scanner&quot; } }} 将Jenkinsfile和sonar-project.properties推送到gitlab 构建项目后在sonarqube就可以看到代码审查了 六、Jenkins+Docker+SpringCloud微服务持续集成 大致流程： 开发人员push代码到gitlab。 Jenkins服务器进行编译打包并构建镜像推送到harbor镜像仓库（服务器的JAVA环境要与项目对应）。 Jenkins服务器触发远程命令使生产服务器（tomcat）从私有仓库拉取镜像。 生产服务器（tomcat）生成容器，项目上线。 用户访问。 6.1 Harbor部署 下载 1curl -O https://github.com/goharbor/harbor/releases/download/v2.3.4/harbor-offline-installer-v2.3.4.tgz 修改 harbor.yml 123456789101112131415161718192021222324252627282930313233343536cp harbor.yml.tmpl harbor.ymlegrep -v '^#|^$|*#' harbor.ymlhostname: 192.168.88.30http: port: 81harbor_admin_password: toortoordatabase: password: toortoor max_idle_conns: 100 max_open_conns: 900data_volume: /root/cicd/harbor/datatrivy: ignore_unfixed: false skip_update: false insecure: falsejobservice: max_job_workers: 10notification: webhook_job_max_retry: 10chart: absolute_url: disabledlog: level: info local: rotate_count: 50 rotate_size: 200M location: /root/cicd/harbor/var_version: 2.3.0proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy docker-compose.yaml 文件中指定 harbor 访问端口为 80，避免与 gitlab 冲突， 修改为 81 通过脚本部署 12./prepare./install.sh 创建私有仓库 修改 daemon.json，让 docker 信任此仓库 1234{ &quot;registry-mirrors&quot;: [&quot;https://5v5rh037.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [&quot;192.168.88.30:81&quot;]} 登录远程镜像仓库 1docker login -u admin -p toortoor 192.168.88.30:81 6.2 提交代码到Gitlab 6.3 Jenkins创建流水线工程 创建工程 在项目根目录下创建 Jenkinsfile，通过语法生成器进行编写 Jenkinsfile 拉取代码部分 在流水线工程中创建参数，如果有多个服务就可以根据参数选择进行构建 Jenkinsfile 添加编译打包微服务工程部分，以及通过 Dockerfile 构建镜像部分 在 pom.xml 中添加 Dockerfile 依赖 12345678910&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.10&lt;/version&gt; &lt;configuration&gt; &lt;repository&gt;${project.artifactId}&lt;/repository&gt; &lt;buildArgs&gt; &lt;JAR_NAME&gt;target/${project.build.finalname}.jar&lt;/JAR_NAME&gt; &lt;/buildArgs&gt; &lt;/plugin&gt; 在项目根目录添加 Dockerfile 123456FROM openjdk:11ARG JAR_NAMEWORKDIR /usr/src/myappEXPOSE 8080COPY ./${JAR_NAME} /usr/src/myapp/ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/usr/src/myapp/${JAR_NAME}&quot;] Jenkinsfile 添加上传镜像至 Harbor 部分 在 Jenkins 中添加 Harbor 账户凭证，以便于上传代码，这里用到的是 Harbor 的用户名和密码 保留好凭证ID 在语法生成器中生成新语法用于登录 Harbor 仓库 Jenkins 安装 Publish Over SSH 插件，能够对远程主机发送 shell 命令，安装完后先给远程主机发送公钥 1ssh-copy-id root@192.168.88.30 发送完后在系统配置进行配置 接着生成流水线语法，去远程执行脚本文件 test.sh test.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#!/bin/bashharbor_url=$1project_name=$2image_name=$3service_port=$4# 删除none容器echo &quot;docker rmi none...&quot;docker images | grep none | awk '{ print &quot;docker rmi &quot;$3 }' | sh &amp;&gt;/dev/null# 检查是否有已运行容器num=`docker ps | grep $project_name | wc -l`if [ $num -gt 0 ]then docker stop $project_name &amp;&amp; docker rm $project_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker stop $project_name &amp;&amp; docker rm $project_name [OK]&quot; else echo &quot;ERROR: docker stop $project_name &amp;&amp; docker rm $project_name&quot; fifi# 检查是否有重命名镜像temp=`docker images | grep $image_name | awk '{ print $1&quot;:&quot;$2 }' | wc -l`if [ $temp -eq 1 ]then docker rmi $image_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker rmi $image_name [OK]&quot; else echo &quot;ERROR: docker rmi $image_name&quot; fifi# 登录镜像仓库echo &quot;docker login repository...&quot;docker login -u admin -p toortoor $harbor_url &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to login repository&quot;else echo &quot;docker login repository [OK]&quot;fi# 拉取镜像echo &quot;docker pull image...&quot;docker pull $image_name &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to pull image&quot;else echo &quot;docker pull image [OK]&quot;fi# 运行容器echo &quot;docker run container...&quot;docker run -d --name $project_name -p $service_port:8080 $image_name &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to run container&quot;else echo &quot;docker run container [OK]&quot;fi 最终的 Jenkinsfile 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// git凭证iddef git_auth = &quot;bad37d0d-7260-43a2-a350-9abf6e99753a&quot;// git凭证urldef git_url = &quot;http://192.168.88.30/test_group/test_project.git&quot;// Harbor地址def harbor_url = &quot;192.168.88.30:81&quot;// 镜像仓库名称def harbor_repository = &quot;test&quot;// Harbor用户凭证IDdef harbor_auth = &quot;c147d241-a254-48d6-be1c-0d5f0964ce9a&quot;// 镜像版本号def image_version = &quot;1.0&quot;// 拉取的微服务名称def project_name = &quot;test&quot;// 微服务所需端口号def service_port = &quot;8081&quot;node { // 拉取代码 stage('Pull Code') { git branch: &quot;${branch}&quot;, credentialsId: &quot;${git_auth}&quot;, url: &quot;${git_url}&quot; } // 编译，打包微服务工程，构建镜像 stage('Mvn clean package and Docker build') { sh &quot;mvn clean package dockerfile:build&quot; } // 上传镜像至Harbor stage('Push image to Harbor') { // 镜像命名，打标签 def image_name = &quot;${project_name}:${image_version}&quot; sh &quot;docker tag ${project_name}:latest ${harbor_url}/${harbor_repository}/${image_name} &amp;&amp; docker rmi ${project_name}:latest&quot; // 登录Harbor仓库 withCredentials([usernamePassword(credentialsId: &quot;${harbor_auth}&quot;, passwordVariable: 'harbor_password', usernameVariable: 'harbor_user')]) { sh &quot;docker login -u ${harbor_user} -p ${harbor_password} ${harbor_url}&quot; } // 镜像上传 sh &quot;docker push ${harbor_url}/${harbor_repository}/${project_name}:${image_version}&quot; } // 远程连接服务器拉取镜像运行 stage('Pull image and Running container') { // 获取镜像命名 def image_name = &quot;${project_name}:${image_version}&quot; sshPublisher(publishers: [sshPublisherDesc(configName: '192.168.88.30', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: './root/test.sh $harbor_url $project_name $harbor_url/$harbor_repository/image_name $service_port', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) }} 6.4 微服务架构CICD优化从以上配置中可以看到，每次构建都只能选择一个服务，且部署的服务器也只有一台，是不符合生产环境的，主要需求有以下三个： 多个微服务同时构建流水线工程 批量处理镜像 将微服务部署服务器集群 安装 Extended Choice Parameter 插件，实现多个微服务同时构建 在创建流水线时设置选项 将公钥下发，并在系统设置的 Publish over SSH 中添加多台主机 在流水线工程中添加多一个多选项配置，用于选择要部署的服务器 修改流水线语法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// git凭证iddef git_auth = &quot;bad37d0d-7260-43a2-a350-9abf6e99753a&quot;// git凭证urldef git_url = &quot;http://192.168.88.30/test_group/test_project.git&quot;// Harbor地址def harbor_url = &quot;192.168.88.30:81&quot;// 镜像仓库名称def harbor_repository = &quot;test&quot;// Harbor用户凭证IDdef harbor_auth = &quot;c147d241-a254-48d6-be1c-0d5f0964ce9a&quot;// 镜像版本号def image_version = &quot;1.0&quot;node { // 获取服务名 def selectedProjectNames = &quot;${project_name}&quot;.split(&quot;,&quot;) // 获取集群地址 def selectedServices = &quot;${publish_server}&quot;.split(&quot;,&quot;) // 拉取代码 stage('Pull Code') { git branch: &quot;${branch}&quot;, credentialsId: &quot;${git_auth}&quot;, url: &quot;${git_url}&quot; } // 编译，打包微服务工程，构建镜像 stage('Mvn clean package and Docker build') { sh &quot;mvn clean package dockerfile:build&quot; } // 上传镜像至Harbor stage('Push image to Harbor') { for(int i=0; i&lt;selectedProjectNames.length; i++) { // 获取每个选项 def project_info = selectedProjectNames[i] // 获取选项中的微服务名称 def current_project_name = &quot;${project_info}&quot;.split(&quot;@&quot;)[0] // 镜像命名，打标签 def image_name = &quot;${current_project_name}:${image_version}&quot; sh &quot;docker tag ${current_project_name}:latest ${harbor_url}/${harbor_repository}/${image_name} &amp;&amp; docker rmi ${current_project_name}:latest&quot; // 登录Harbor仓库 withCredentials([usernamePassword(credentialsId: &quot;${harbor_auth}&quot;, passwordVariable: 'harbor_password', usernameVariable: 'harbor_user')]) { sh &quot;docker login -u ${harbor_user} -p ${harbor_password} ${harbor_url}&quot; } // 镜像上传 sh &quot;docker push ${harbor_url}/${harbor_repository}/${current_project_name}:${image_version}&quot; } } // 远程连接服务器拉取镜像运行 stage('Pull image and Running container') { for(int i=0; i&lt;selectedProjectNames.length; i++) { // 获取每个选项 def project_info = selectedProjectNames[i] // 获取选项中的微服务名称 def current_project_name = &quot;${project_info}&quot;.split(&quot;@&quot;)[0] // 获取选项中的微服务所需端口 def current_project_port = &quot;${project_info}&quot;.split(&quot;@&quot;)[1] // 需要拉取的镜像 def image_name = &quot;${current_project_name}:${image_version}&quot; // 遍历所有服务器，分别部署 for(int j=0; j&lt;selectedServices.length; j++) { // 获取当前服务器 def current_server = selectedServices[j] // 加上参数配置，根据配置文件的不同选择不同的服务器部署 if(current_server == &quot;192.168.88.31&quot;) { activeProfile = activeProfile+&quot;serviceName-server1&quot; }else if(current_server == &quot;192.168.88.32&quot;) { activeProfile = activeProfile+&quot;serviceName-server2&quot; } // 远程执行脚本 sshPublisher(publishers: [sshPublisherDesc(configName: &quot;${current_server}&quot;, transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: './root/test.sh $harbor_url $current_project_name $harbor_url/$harbor_repository/$image_name $current_project_port $activeProfile', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) } } }} 修改 test.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/bin/bashharbor_url=$1project_name=$2image_name=$3service_port=$4profile=$6# 删除none容器echo &quot;docker rmi none...&quot;docker images | grep none | awk '{ print &quot;docker rmi &quot;$3 }' | sh &amp;&gt;/dev/null# 检查是否有已运行容器num=`docker ps | grep $project_name | wc -l`if [ $num -gt 0 ]then docker stop $project_name &amp;&amp; docker rm $project_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker stop $project_name &amp;&amp; docker rm $project_name [OK]&quot; else echo &quot;ERROR: docker stop $project_name &amp;&amp; docker rm $project_name&quot; fifi# 检查是否有重命名镜像temp=`docker images | grep $image_name | awk '{ print $1&quot;:&quot;$2 }' | wc -l`if [ $temp -eq 1 ]then docker rmi $image_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker rmi $image_name [OK]&quot; else echo &quot;ERROR: docker rmi $image_name&quot; fifi# 登录镜像仓库echo &quot;docker login repository...&quot;docker login -u admin -p toortoor $harbor_url &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to login repository&quot;else echo &quot;docker login repository [OK]&quot;fi# 拉取镜像echo &quot;docker pull image...&quot;docker pull $image_name &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to pull image&quot;else echo &quot;docker pull image [OK]&quot;fi# 运行容器echo &quot;docker run container...&quot;docker run -d --name $project_name -p $service_port:8080 $image_name $profile &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to run container&quot;else echo &quot;docker run container [OK]&quot;fi 七、基于K8S的CICD基于 Kubernetes 平台来实现 CICD 功能。 7.1 Jenkins对接K8S 首先通过 bitnami helm 部署 Jenkins（Master），需开放 jnlp 50000 端口 1234567# 添加repo源helm repo add bitnami https://charts.bitnami.com/bitnami# 下载chart到本地helm pull bitnami/jenkins# 根据需求修改values.yaml# 部署helm install jenkins jeknins/ RBAC 授权，SA 名为 jenkins，后续会用到 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647apiVersion: v1kind: ServiceAccountmetadata: name: jenkins namespace: nextcloud---kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: jenkins namespace: nextcloudrules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;pods/exec&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;pods/log&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;] - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: jenkins namespace: nextcloudroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkinssubjects: - kind: ServiceAccount name: jenkins namespace: nextcloud 获取刚刚创建的 ca.crt 信息 1kubectl get secret jenkins-token-xxx -oyaml | awk '/ca.crt/{ print $2 }' | base64 -d 在 Jenkins 创建凭据 添加云，这里用到刚刚创建的凭据 设置 Pod Template，即 Jenkins Slave，将包含 jnlp 一个容器，来实现编译打包等功能。 这里需挂载宿主机的 docker.sock、docker 和 kubectl 的二进制文件。 然后使用到刚刚创建的 SA。 测试链接。 7.2 构建前准备需要在项目的根目录下准备好 Dockerfile、Jenkinsfile、deploy.yaml。 Jenkinsfile 123456789101112131415161718192021222324252627282930313233343536373839404142// gitlab及harbur凭据认证等信息def git_auth = &quot;764f29f3-8955-4551-a23a-659aa4c8deaa&quot;def git_url = &quot;http://192.168.159.12:30098/root/nextcloud.git&quot;def harbor_url = &quot;192.168.159.101&quot;def harbor_repository = &quot;nextcloud&quot;def harbor_auth = &quot;7ebc36f8-73a3-417f-864f-24856490b1a2&quot;def project_name = &quot;nextcloud&quot;// jenkins-slave为pod template中的namenode('jenkins-slave') { // jnlp为pod template中的容器名称 container('jnlp') { // pipeline步骤 stage('Git Clone') { git branch: &quot;${branch}&quot;, credentialsId: &quot;${git_auth}&quot;, url: &quot;${git_url}&quot; } stage('Docker Build') { sh &quot;docker build -t ${harbor_url}/${harbor_repository}/${project_name}:${branch}-${version} .&quot; } stage('Docker Push') { withCredentials([usernamePassword(credentialsId: &quot;${harbor_auth}&quot;, passwordVariable: 'harbor_password', usernameVariable: 'harbor_user')]) { sh &quot;docker login -u ${harbor_user} -p ${harbor_password} ${harbor_url}&quot; } sh &quot;docker push ${harbor_url}/${harbor_repository}/${project_name}:${branch}-${version}&quot; sh &quot;docker rmi ${harbor_url}/${harbor_repository}/${project_name}:${branch}-${version}&quot; } } // 在deploy.yaml中将镜像tag设为了build-tag,方便通过sed修改 stage('Sed YAML') { sh &quot;sed -i 's#build-tag#${branch}-${version}#g' deploy.yaml&quot; } stage('Deploy to K8s') { sh &quot;kubectl apply -f deploy.yaml&quot; }} 7.3 创建流水线项目这里添加了两个字符参数，用于构建前选择分支和 tag 的标签定义。 构建测试，会发现集群中创建了一个 jenkins-slave-xxx 的 pod，pod 中包含 jnlp 一个容器，通过观察日志可发现与 Jenkins Master 进行了连接。 Jenkins 中查看流水线进度。 流水线结束后，通过 kubectl 可看到项目已部署，且使用的镜像是刚刚构建好的。","link":"/2024/02/18/jenkins/"},{"title":"etcd leader选举","text":"etcd 是基于 raft 算法进行选举，而 raft 是一种管理日志一致性的协议，将系统中的角色分为三个 leader: 接受客户端的请求，并向 follower 发送同步请求日志 follower: 接收 leader 同步的日志 candidate: 候选者角色，在选举过程中发挥作用 leader 选举 raft 是通过心跳机制来触发 leader 的选举，每一个实例(例如 etcd pod)启动后都会初始化为一个 follower，leader 则会周期性的向所有 follower 发送心跳包，在 etcd 的编排中就能看到相关的参数 如果 follower 如果在选举超时时间内没有收到 leader 的心跳包，就会等待一个随机的时间，然后发起 leader 选举。每个 follower 都有一个时钟，这个时钟是一个随机的值，集群中谁的时钟先跑完，那么就由谁来发起 leader 选举 该 follower 会将当前的任期(term) + 1 然后转化为 candidate，先给自己投票然后向集群中的其他 follower 发送 RequestVote RPC 那么最终的结果会有三种: 自己赢得了最多的票数，成为 leader 收到了 leader 的消息，表示已经有其他服务抢先成为了 leader 没有服务获得最高票数，即选举失败，会等待选举时间超时后进行下一次选举 在 raft 协议中，所有的日志条目都只会是 leader 往 follower 写入，且 leader 的日志只增不减，所以能被选举成为 leader 的节点，一定包含了所有已经提交的日志条目","link":"/2024/03/19/etcd-leader%E9%80%89%E4%B8%BE/"},{"title":"kube-proxy工作原理","text":"kube-proxy 以 DaemonSet 的形式运行在集群的所有节点中，负责管理集群外到 Service，以及 Service 到 Pod 的流量转发。 kube-proxy 有三种模式: userspace iptables ipvs 在运行了 kube-proxy 的节点上，可以通过下面的命令查看 kube-proxy 的运行模式: 1curl localhost:10249/proxyMode iptables通过 kube-proxy 的 cm 可以查看 mode 我们都知道 SVC 有多种类型，常用的有 ClusterIP、NodePort、Headless、LoadBalancer 等等，这里我们通过创建类型的 SVC，查看 kube-proxy 下发的iptables 规则 ClusterIP 通过 iptables-save 命令打印这个 SVC 的规则 PodIP: 10.233.74.106ClusterIP: 10.233.54.175 1234567891011121314# 将来自 Pod IP 10.233.74.106 的流量标记为服务 default/nginx，并转发到 KUBE-MARK-MASQ 链中-A KUBE-SEP-EUB75IW55XDW6EHN -s 10.233.74.106/32 -m comment --comment &quot;default/nginx:tcp-80&quot; -j KUBE-MARK-MASQ# 将匹配的 TCP 流量进行 DNAT，转发到后端 Pod 10.233.74.106:80-A KUBE-SEP-EUB75IW55XDW6EHN -p tcp -m comment --comment &quot;default/nginx:tcp-80&quot; -m tcp -j DNAT --to-destination 10.233.74.106:80# 匹配目标地址为服务 default/nginx 的 cluster IP 10.233.54.175 的 TCP 80 端口流量-A KUBE-SERVICES -d 10.233.54.175/32 -p tcp -m comment --comment &quot;default/nginx:tcp-80 cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-XX3NNXKPRC7N6OJH# 标记来自外部网络（除了 PodCIDR 10.233.64.0/18）的、目标地址为服务 default/nginx 的 cluster IP 10.233.54.175 的 TCP 80 端口的流量-A KUBE-SVC-XX3NNXKPRC7N6OJH ! -s 10.233.64.0/18 -d 10.233.54.175/32 -p tcp -m comment --comment &quot;default/nginx:tcp-80 cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ# 将匹配的流量转发到 KUBE-SEP-EUB75IW55XDW6EHN 链中，进行 DNAT 地址转换-A KUBE-SVC-XX3NNXKPRC7N6OJH -m comment --comment &quot;default/nginx:tcp-80 -&gt; 10.233.74.106:80&quot; -j KUBE-SEP-EUB75IW55XDW6EHN NodePort 把这个 SVC 类型改成 NodePort 通过 iptables-save 命令打印这个 SVC 的规则，多了两条规则 12345# 用于标记来自外部的、目标地址为服务 default/nginx 的 NodePort 流量-A KUBE-EXT-XX3NNXKPRC7N6OJH -m comment --comment &quot;masquerade traffic for default/nginx:tcp-80 external destinations&quot; -j KUBE-MARK-MASQ# 用于匹配目标端口为 32594 的 TCP 流量，并将该流量转发到上面的规则进行 MASQ 标记-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/nginx:tcp-80&quot; -m tcp --dport 32594 -j KUBE-EXT-XX3NNXKPRC7N6OJH IPVS通过 kube-proxy 的 cm 可以查看 mode 来自 GPT 的回答: IPVS 是 Linux 内核中实现的虚拟服务器技术，它可以将网络流量负载均衡到多台服务器上，从而提高服务性能和可靠性。 ClusterIP PodIP: 10.233.76.138ClusterIP: 10.233.27.36 通过 ipvsadm 命令查看这个虚拟服务器 12345678910Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn# 将目标地址为 nginx.default.svc.cluster.lo:80 的 TCP 流量转发到后端 Pod 10.233.76.138:80TCP nginx.default.svc.cluster.lo rr -&gt; 10-233-76-138.nginx.default. Masq 1 0 0# 将目标地址为 10.233.27.36:80 的 TCP 流量转发到后端 Pod 10.233.76.138:80TCP 10.233.27.36:80 rr -&gt; 10.233.76.138:80 Masq 1 0 0 NodePort 通过 ipvsadm 命令查看这个虚拟服务器，多了三条规则 1234567891011# 10.233.74.64 为 nodelocaldns 网卡的 IP，由于 NodePort 是全监听，所以也有对应的规则TCP 169.254.25.10:30229 rr -&gt; 10.233.76.138:80 Masq 1 0 0# 节点IPTCP 192.168.159.11:30229 rr -&gt; 10.233.76.138:80 Masq 1 0 0# 10.233.74.64 为 tun0 网卡的 IP，由于 NodePort 是全监听，所以也有对应的规则TCP 10.233.74.64:30229 rr -&gt; 10.233.76.138:80 Masq 1 0","link":"/2024/03/30/kube-proxy%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"},{"title":"Kubernetes","text":"一、概念1.1 k8s概述Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。 容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？ 这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 简称 k8s，主要功能有： 服务发现和负载均衡 存储编排 自动部署和回滚 自动完成装箱计算 自我修复 密钥与配置管理 1.2 k8s组件一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。 工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。 为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。 1.2.1 控制平面组件kube-apiserver： API 服务器是 Kubernetes 控制面的组件，该组件公开了 Kubernetes API。API 服务器是 Kubernetes 控制面的前端。 etcd： etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。 kube-scheduler： 控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。 调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 kube-controller-manager： 在主节点上运行控制器的组件。 控制器包括： 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应 任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod) 服务帐户和令牌控制器（Service Account &amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌 cloud-controller-manager： 云控制器管理器是指嵌入特定云的控制逻辑的控制平面组件。云控制器管理器允许您链接聚合到云提供商的应用编程接口中，并分离出相互作用的组件与您的集群交互的组件。 下面的控制器都包含对云平台驱动的依赖： 节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器 1.2.2 Node组件节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 kubelet： 一个在集群中每个节点（node）上运行的代理。它保证容器（containers）都运行在 Pod 中。 kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。 容器运行时（Container Runtime）： 容器运行环境是负责运行容器的软件，例如 Docker 。 1.2.3 插件（Addons）插件使用 Kubernetes 资源（DaemonSet、Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。 DNS： 尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有集群 DNS， 因为很多示例都需要 DNS 服务。 Web 界面（仪表盘）： Dashboard 是Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 容器资源监控： 容器资源监控将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。 集群层面日志： 集群层面日志机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。 1.3 Pod1.3.1 Pod概念Pod 是 k8s 中的最小单元，一个 Pod 包含一个或多个容器，一个 Pod 不会跨越多个节点（Node）。 而每个 Pod 里都会有一个根容器 pause，Pod 中的其他容器都共享 pause 根容器的网络栈和volume挂载卷，因此容器之间的通信和数据交换会更为高效。 1.3.2 RC、RS、Deploymentk8s 管理 Pod 主要用到以下三个组件： Replication Controller（RC）：用来确保容器应用的副本数始终保持在用户定义的副本数，如果有容器异常退出，会自动创建新的 Pod 来代替，如果有异常多出的容器也会自动回收。 ReplicaSet（RS）：相比 RC 多了支持 selector，推荐使用 RS。 Deployment：用来管理 RS。 来看看 Deployment 和 RS 是如何实现更新的： Deployment 创建新的 RS RS1删除一个容器，接着 RS2 新建一个新版本的 Pod 全部更新完之后，RS1 并不会删除，而是保留着处于停用状态，如果新版本出了问题需要回滚，就可以反过来操作实现回滚 1.3.3 Horizontal Pod AutoscalerHorizontal Pod Autoscaler（HPA）在k8s集群中用于 Pod 水平自动弹性伸缩，它是基于 CPU 和内存利用率对 Deployment 和 RS 中的 Pod 数量进行自动扩缩容（除了 CPU 和内存利用率之外，也可以基于其他应程序提供的度量指标 custom metrics 进行自动扩缩容）。 假如 HPA 检测到当前 Deployment 和 RS 所管理的 Pod 的 CPU 或内存使用率超过了设定之后，就会创建新的 Pod 来实现降压，新建 Pod 的数量限制由 Max 和 Min 设定。 1.3.4 StatefulSetRS 和 Deployment 都是面向无状态的服务，它们所管理的 Pod 的 IP、名字，启停顺序等都是随机的，而 StatefulSet 是有状态的集合，管理所有有状态的服务，比如 MySQL、MongoDB 集群等。 StatefulSet 的特点有： Pod 的一致性：包含次序（启停顺序，例如 mysql -&gt; php-fpm -&gt; nginx 的启动顺序）、网络一致性（与 Pod 相关，与被调度的 Node 节点无关）。 稳定的存储：即 Pod 重新调度之后还是访问到相同的持久化数据，基于 PVC（PV 是集群中由管理员提供或使用存储类动态提供的一块存储。它是集群中的资源，就像节点是集群资源一样。而 PVC 是用户对存储的请求。它类似于 Pod，Pod 消耗 Node 资源，而 PVC 消耗 PV 资源。） 来实现。 稳定的次序：对于N个副本的 StatefulSet，每个 Pod 都在 [0，N) 的范围内分配一个数字序号，且是唯一的。 稳定的网络：Pod 的 hostname 模式为：( StatefulSet 名称 ) - ( 序号 )。 1.3.5 DaemonSetDaemonSet 确保全部或者一部分 Node 上运行一个 Pod 的副本，当有 Node 加入集群时，也会为他们新增一个 Pod，当这些 Node 退出集群时，这些 Pod 也会被回收。 DaemonSet 的一些典型用法： 运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph等。 运行日志收集daemon，例如fluentd、logstash等。 运行监控daemon，例如 Prometheus 的 Node exporter、zabbix等。 1.3.6 Job和Cron JobJob 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 这里可能有人会想，那在 linux 上直接执行脚本不就行了吗？其实这就会有个问题，如果脚本执行失败那么久退出且不会再执行了，需要执行就必须手动，但 Job 设置的任务只有在正常执行结束后才会结束，否则一直执行到成功为止。Job 也可以设置成功的次数要达到几次才允许退出。 Cron Job 是基于时间管理控制 Job，即在给定的时间只运行一次、周期性的在指定时间运行。 1.3.7 ServicePod 的生命是有限的，如果 Pod 重启 IP 也可能会发生变化。如果我们将 Pod 的 IP 写死，Pod 如果挂了或重启，其它的服务也会不可用。我们可以把我们的服务（各种 Pod）注册到服务发现中心去，让服务发现中心去动态更新其它服务的配置就可以了，k8s 就给我们提供了这么一个服务，即 Service。 我们这样就可以不用去管后端的 Pod 如何变化，只需要指定 Service 的地址就可以了，因为我们在中间添加了一层服务发现的中间件，Pod 销毁或者重启后，把这个 Pod 的地址注册到这个服务发现中心去。 1.4 k8s的网络通讯方式k8s 的网络模型假定了所有的 Pod 都在一个可以直接连通的扁平化网络空间中，在这 GCE（Google Compute Engine）里面是现成的网络模型。 Flannel 是 CoreOS 团队针对 Kubernetes 设计一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建 Docker 容器都具有全集群唯一的虚拟 IP 地址。而且它还能在这些 IP 地址之间建立一个覆盖网络（Overlay Network），通过覆盖网络将数据包原封不动的传递到目标容器内。 通过一个架构图来看看不同情况下的通讯是怎么样的： 通讯情况主要分为以下几种： 同一 Pod 不同容器之间的通信：采用 pause 网络栈。 同一 Node 不同 Pod 之间的通信：通过 docker0 网桥进行通信。 不同 Node 的 Pod 之间的通信：不同 Node 的 Pod 之间的通信是 k8s 网络通信的难点，是通过 Flannel 网络通讯方式来实现的，通讯的过程分为以下几个步骤： 数据包从 Node1 的 Pod 到达 docker0 网桥 Flanneld 会开启一个 Flannel0 的网桥，用来抓取到达 docker0 的数据，可以理解为一个钩子函数 Flanneld 会有很多路由表信息，是存储在 etcd 中由 Flanneld 自动获取的，通过路由表知道了转发信息后就通过物理网卡进行转发 发送到目标主机的物理网卡后，就会再通过 Flanneld -&gt; Flannel0网桥 -&gt; docker0网桥，最终到达目的 Pod Pod 与 service 之间的通信：采用各节点的 iptables 规则来实现。 Pod 到外网：通过 Flanneld 到达物理网卡，经过路由选择后，iptables 执行 Masquerade，把Pod 的虚拟 IP 改为 物理网卡的 IP，在向外网服务器发出请求。 外网到 Pod：通过 service 进行访问，一般使用 NodePort。 二、k8s部署本次 k8s 部署为以下环境 2.1 基本环境配置（所有节点） 在各主机设置主机名以及host文件解析 1234567hostnamectl set-hostname k8s-master01hostnamectl set-hostname k8s-node01hostnamectl set-hostname k8s-node02vim /etc/hosts192.168.88.10 k8s-master01192.168.88.20 k8s-node01192.168.88.21 k8s-node02 安装依赖 1yum -y install conntrack ntpdate ntp ipvsadm ipset jp iptables curl stsstat libseccomp wget vim net-tools git 设置防火墙为iptables并设置空规则 12345systemctl stop firewalld &amp;&amp; systemctl disable firewalldyum -y install iptables-servicessystemctl start iptablessystemctl enable iptablesiptables -F &amp;&amp; service iptables save 关闭虚拟内存和selinux 12swapoff -a &amp;&amp; sed -i '/ swap / s/^\\(.*\\)$/#/g' /etc/fstabsetenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 调整内核参数 1234567891011121314151617cat &gt; kubernetes.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ipv6tables=1net.ipv4.ip_forward=1net.ipv4.tcp_tw_recycle=0vm.swappiness=0 # 禁用swap，只有系统OOM时才允许使用vm.overcommit_memory=1 # 不检查物理内存是否够用vm.panic_on_oom=0 # 开启OOMfs.inotify.max_user_instances=8192fs.inotify.max_user_watches=1048576fs.file-max=52706963fs.nr_open=52706963net.ipv6.conf.all.disable_ipv6=1net.netfilter.nf_conntrack_max=2310720EOFcp kubernetes.conf /etc/sysctl.d/kubernetes.confsysctl -p /etc/sysctl.d/kubernetes.conf 调整系统时区 1234567# 设为中国/上海timedatectl set-timezone Asia/Shanghai# 将当前UTC时间写入硬件时钟timedatectl set-local-rtc 0# 重启依赖于时间的服务systemctl restart rsyslogsystemctl restart crond 关闭不需要的服务 12# 关闭邮件服务systemctl stop postfix &amp;&amp; systemctl disable postfix 设置rsyslog和systemd journald 1234567891011121314151617181920212223242526272829# 持久化保存日志目录mkdir /var/log/journalmkdir /etc/systemd/journald.conf.dcat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt;EOF[Journal]#持久化保存到磁盘Storage=persistent#压缩历史日志Compress=yes SyncIntervalSec=5mRateLimitInterval=30sRateLimitBurst=1000#最大占用空间 10GSystemMaxUse=10G#单日志文件最大 200MSystemMaxFileSize=200M#日志保存时间 2 周MaxRetentionSec=2week#不将日志转发到 syslogForwardToSyslog=noEOFsystemctl restart systemd-journald 升级内核 12345678910# 导入公钥rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org# 安装elrepo源yum -y install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm# 升级内核yum --enablerepo=elrepo-kernel install -y kernel-lt# 设置开机从新内核启动grub2-set-default &quot;CentOS Linux (5.4.116-1.e17.elrepo.x86_64) 7 (Core)&quot;# 查看内核启动项grub2-editenv list 2.2 kubeadm部署（所有节点） kube-proxy开启ipvs前置条件 123456789101112131415modprobe br_netfiltervim /etc/sysconfig/modules/ipvs.modules#!/bin/bashipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack&quot;for kernel_module in ${ipvs_modules};do /sbin/modinfo -F filename ${kernel_module} &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ${kernel_module} fidonechmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack 安装docker 123456789101112131415161718192021yum -y install yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repoyum makecache fastyum -y install docker-ce docker-ce-climkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF{ &quot;registry-mirrors&quot;: [&quot;http://f1361db2.m.daocloud.io&quot;], &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;:&quot;json-file&quot;, &quot;log-opts&quot;:{ &quot;max-size&quot;:&quot;100m&quot; }}EOFmkdir -p /etc/systemd/system/docker.service.dsystemctl daemon-reload &amp;&amp; systemctl start docker &amp;&amp; systemctl enable docker 2.3 安装kubeadm（所有节点） 安装 kubeadm kubectl kubelet 123456789101112cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum -y install kubeadm kubectl kubeletsystemctl enable kubelet 通过kubeadm查看目前各镜像版本 12345678kubeadm config images listk8s.gcr.io/kube-apiserver:v1.21.0k8s.gcr.io/kube-controller-manager:v1.21.0k8s.gcr.io/kube-scheduler:v1.21.0k8s.gcr.io/kube-proxy:v1.21.0k8s.gcr.io/pause:3.4.1k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns/coredns:v1.8.0 编写脚本，通过阿里镜像安装 1234567891011121314151617181920212223242526vim k8s_images.sh#!/bin/bashapiserver_var=v1.21.0controller_manager_var=v1.21.0scheduler_var=v1.21.0proxy_var=v1.21.0pause_var=3.4.1etcd_var=3.4.13-0coredns_var=v1.8.0image_aliyun=(kube-apiserver:$apiserver_var kube-controller-manager:$controller_manager_var kube-scheduler:$scheduler_var kube-proxy:$proxy_var pause:$pause_var etcd:$etcd_var coredns/coredns:$coredns_var)for image in ${image_aliyun[@]}do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$image docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$image k8s.gcr.io/${image} docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imagedone./k8s_images.sh# 问题# 由于杭州阿里源里目前没有coredns:v1.8.0版本，所以在北京阿里源下载docker pull registry.cn-beijing.aliyuncs.com/dotbalo/coredns:1.8.0docker tag registry.cn-beijing.aliyuncs.com/dotbalo/coredns:1.8.0 k8s.gcr.io/coredns/coredns:v1.8.0docker rmi registry.cn-beijing.aliyuncs.com/dotbalo/coredns:1.8.0 2.4 初始化Master节点1kubeadm config print init-defaults &gt; kubeadm.config.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546vim kubeadm.config.ymlapiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: # 改成master节点IP地址 advertiseAddress: 192.168.88.10 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: node taints: null---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: {}dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: 1.21.0networking: dnsDomain: cluster.local podSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/12scheduler: {}# 添加这一段---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationkubeproxy: config: mode: ipvs 123456kubeadm init --config=kubeadm.config.yml --upload-certs | tee kubeadm-init.log...# 初始化后操作mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config 2.5 部署网络123mkdir -p k8s/plugin/flannel &amp;&amp; cd k8s/plugin/flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl create -f kube-flannel.yml kube-flannel.yml文件内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223---apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/defaultspec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: &quot;/etc/cni/net.d&quot; - pathPrefix: &quot;/etc/kube-flannel&quot; - pathPrefix: &quot;/run/flannel&quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny'---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: flannelrules:- apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged']- apiGroups: - &quot;&quot; resources: - pods verbs: - get- apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch- apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | { &quot;name&quot;: &quot;cbr0&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ] } net-conf.json: | { &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } }---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.14.0-rc1 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.14.0-rc1 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;, &quot;NET_RAW&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 部署完后可以看到 flannel 网卡 通过命令查看各模块运行情况 1kubectl get pod -n kube-system 2.6 添加Node节点 查看kubeadm-init.log 123...kubeadm join 192.168.88.10:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:37c9645a7a2ee75301f132537240157ecd4f464494022cc442f77489c1978989 在node主机上执行 12kubeadm join 192.168.88.10:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:37c9645a7a2ee75301f132537240157ecd4f464494022cc442f77489c1978989 在master主机查看是否加入成功 1234kubectl get node...kubectl get pod -n kube-system -o wide... 三、k8s资源清单在 k8s 中，一般使用 yaml 格式的文件来创建符合我们预期期望的 pod，这样的 yaml 文件我们一般称为资源清单。 3.1 资源类型名称空间级别 工作负载型资源：Pod、RS、Deployment、StatefulSet、DaemonSet、Job、CronJob 服务发现及负载均衡型资源：Service 配置与存储型资源：Volume、CSI 特殊类型的存储卷：ConfigMap(当配置中心来使用的资源类型)、Secret(保存敏感数据)、DownwarAPI(把外部环境中的信息输出给容器) 集群级资源：Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding 元数据型资源：HPA、PodTemplate、LimitRange 3.2 常用字段必要属性 参数名 字段类型 说明 apiVersion String K8S API 的版本，目前基本是v1，可以用 kubectl api-versions 命令查询 kind String 这里指的是 yaml 文件定义的资源类型和角色，比如: Pod metadata Object 元数据对象，固定值写 metadata metadata.name String 元数据对象的名字，这里由我们编写，比如命名Pod的名字 metadata.namespace String 元数据对象的命名空间，由我们自身定义 spec Object 详细定义对象，固定值写Spec spec.containers[] list 这里是Spec对象的容器列表定义，是个列表 spec.containers[].name String 这里定义容器的名字 spec.containers[].image String 这里定义要用到的镜像名称 spec 主要对象 参数名 字段类型 说明 spec.containers[].name String 定义容器的名字 spec.containers[].image String 定义要用到的镜像的名称 spec.containers[].imagePullPolicy String 定义镜像拉取策略，有 Always，Never，IfNotPresent 三个值课选 （1）Always：意思是每次尝试重新拉取镜像 （2）Never：表示仅使用本地镜像 （3）IfNotPresent：如果本地有镜像就是用本地镜像，没有就拉取在线镜像。上面三个值都没设置的话，默认是 Always spec.containers[].command[] List 指定容器启动命令，因为是数组可以指定多个，不指定则使用镜像打包时使用的启动命令 spec.containers[].args[] List 指定容器启动命令参数，因为是数组可以指定多个 spec.containers[].workingDir String 指定容器的工作目录 spec.containers[].volumeMounts[] List 指定容器内部的存储卷配置 spec.containers[].volumeMounts[].name String 指定可以被容器挂载的存储卷的名称 spec.containers[].volumeMounts[].mountPath String 指定可以被容器挂载的容器卷的路径 spec.containers[].volumeMounts[].readOnly String 设置存储卷路径的读写模式，true 或者 false，默认为读写模式 spec.containers[].ports[] List 指定容器需要用到的端口列表 spec.containers[].ports[].name String 指定端口名称 spec.containers[].ports[].containerPort String 指定容器需要监听的端口号 spec.containers[].ports.hostPort String 指定容器所在主机需要监听的端口号，默认跟上面 containerPort 相同，注意设置了 hostPort 同一台主机无法启动该容器的相同副本（因为主机的端口号不能相同，这样会冲突） spec.containers[].ports[].protocol String 指定端口协议，支持TCP和UDP，默认值为TCP spec.containers[].env[] List 指定容器运行千需设置的环境变量列表 spec.containers[].env[].name String 指定环境变量名称 spec.containers[].env[].value String 指定环境变量值 spec.containers[].resources Object 指定资源限制和资源请求的值（这里开始就是设置容器的资源上限） spec.containers[].resources.limits Object 指定设置容器运行时资源的运行上限 spec.containers[].resources.limits.cpu String 指定CPU的限制，单位为 core 数，将用于 docker run –cpu-shares 参数 spec.containers[].resources.limits.memory String 指定 MEM 内存的限制，单位为 MIB，GIB spec.containers[].resources.requests Object 指定容器启动和调度室的限制设置 spec.containers[].resources.requests.cpu String CPU请求，单位为 core 数，容器启动时初始化可用数量 spec.containers[].resources.requests.memory String 内存请求，单位为 MIB，GIB 容器启动的初始化可用数量 额外的参数项 参数名 字段类型 说明 spec.restartPolicy String 定义Pod重启策略，可以选择值为 Always、OnFailure、Never，默认值为 Always。1. Always：Pod一旦终止运行，则无论容器是如何终止的，kubelet 服务都将重启它。2. OnFailure：只有 Pod 以非零退出码终止时，kubelet 才会重启该容器。如果容器正常结束（退出码为0），则 kubelet 将不会重启它。3. Never：Pod 终止后，kubelet 将退出码报告给 Master，不会重启该 Pod。 spec.nodeSelector Object 定义 Node 的 Label 过滤标签，以 key:value 格式指定 spec.imagePullSecrets Object 定义pull 镜像是使用 secret 名称，以 name:secretkey 格式指定 spec.hostNetwork Boolean 定义是否使用主机网络模式，默认值为 false。设置 true 表示使用宿主机网络，不使用 docker 网桥，同时设置了 true 将无法在同一台宿主机上启动第二个副本。 辅助命令 12# 查看资源对象用法kubectl explain &lt;资源对象&gt; 例子： 创建 yaml 文件 123456789101112vim nginx.yaml# 此处只有必要字段apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: containers: - name: nginx image: daocloud.io/library/nginx:latest 创建pod 1kubectl apply -f nginx.yaml 查看状态 123456789# 查看是否生成podkubectl get pod...# 查看pod详细信息kubectl describe pod nginx-pod...# 测试能否访问curl 10.244.1.2 # 此处为flannel分配的地址... 3.3 容器生命周期每一个 Pod 被成功创立之前，都会进行初始化，会运行零个或若干个 init 容器，init 容器运行完就释放，接着才会运行 main 主容器，当然在 init 容器运行之前会先运行 pause 容器，以保证存储和网络的可用。 init 容器与普通的容器非常相似，除了以下两点： init 容器总是运行到完成。 每个 init 容器都要在下一个容器启动之前完成。 如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 “Never”，Kubernetes 不会重新启动 Pod。 如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。 每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时， Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。 init 容器的使用 因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势： Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。 例如，没有必要仅为了在安装过程中使用类似 sed、awk、python 或 dig 这样的工具而去 FROM 一个镜像来生成一个新的镜像。 Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。 应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。 Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可以访问应用容器不能访问的 Secret 的权限。 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。 一旦前置条件满足，Pod 内的所有的应用容器会并行启动。 init 容器使用例子 编写 yaml 文件 12345678910111213141516171819202122232425vim busybox.yamlapiVersion: v1kind: Pod# 定义了一个 Pod 叫 busybox-pod，busybox 封装了 linux 的大量命令metadata: name: busybox-pod labels: app: busybox# 定义该 Pod 下的资源，也就是容器spec: # 定义了一个 busybox 容器 containers: - name: busybox image: busybox:latest command: ['sh','-c','echo The busybox app is running! &amp;&amp; sleep 3600'] # 定义 init 容器，init 容器全部执行完前不会执行 busybox 容器 initContainers: # 第一个 init 容器，如果检查 service 中没有注册 myservice，则休眠2秒继续执行，直到成功 - name: init-myservice image: busybox command: ['sh','-c','until nslookup myservice; do echo waiting for myservice; sleep 2; done'] # 第一个 init 容器运行完就运行该 init 容器，检查 service 中有没有注册 mydb - name: init-mydb image: busybox command: ['sh','-c','until nslookup mydb; do echo waiting for mydb; sleep 2; done'] 运行 1kubectl create -f busybox.yaml 可以看到一直处于 Init:0/2 状态，因为第一个 init 容器一直没完成 编写 yaml 文件添加 service 1234567891011121314151617181920vim service.yamlapiVersion: v1kind: Servicemetadata: name: myservicespec: ports: - protocol: TCP port: 80 targetPort: 9376---apiVersion: v1kind: Servicemetadata: name: mydbspec: ports: - protocol: TCP port: 80 targetPort: 9377 运行 1kubectl create -f service.yaml 可以看到两个 init 容器都执行完，main 主容器也就运行了 3.4 探针探针（probe）是由 kubelet 对容器执行的定期诊断。 要执行诊断，kubelet 调用由容器实现的 Handler （处理程序）。有三种类型的处理程序： ExecAction： 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 TCPSocketAction： 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction： 对容器的 IP 地址上指定端口和路径执行 HTTP Get 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。 每次探测都将获得以下三种结果之一： Success（成功）：容器通过了诊断。 Failure（失败）：容器未通过诊断。 Unknown（未知）：诊断失败，因此不会采取任何行动。 探针可以分为以下三种： livenessProbe：指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 Success。 readinessProbe：指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 Failure。 如果容器不提供就绪态探针，则默认状态为 Success。 startupProbe: 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被禁用，直到此探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其重启策略进行重启。 如果容器没有提供启动探测，则默认状态为 Success。 3.4.1 readinessProbe就绪检测 编写 yaml 文件 123456789101112131415161718192021vim readinessProbe.yamlapiVersion: v1kind: Podmetadata: name: readiness-httpget-pod namespace: defaultspec: containers: - name: readiness-httpget-container image: daocloud.io/library/nginx:latest # 调用readinessProbe探针 readinessProbe: httpGet: # 检查80端口 port: 80 # 检查该目录下的网页 path: /test.html # 容器启动1秒后才进行检测 initialDelaySeconds: 1 # 每3秒检测一次 periodSeconds: 3 生成 Pod 1kubectl create -f readinessProbe.yaml 可以看到虽然 Pod 运行了，但 ready 状态是不对的 进入容器创建文件，问题即可解决 12kubectl exec -it readiness-httpget-pod -- /bin/shecho 'this is test' &gt; /usr/share/nginx/html/test.html 3.4.2 livenessProbe存活检测livenessProbe-exec 编写 yaml 文件 123456789101112131415161718vim livenessProbe_exec.yamlapiVersion: v1kind: Podmetadata: name: liveness-exec-pod namespace: defaultspec: containers: - name: liveness-exec-container image: busybox command: ['sh','-c','touch /tmp/test; sleep 60; rm -rf /tmp/test; sleep 3600'] # 查看本地是否有镜像，有则使用，没有则下载 imagePullPolicy: IfNotPresent livenessProbe: exec: command: ['test','-e','/tmp/test'] initialDelaySeconds: 1 periodSeconds: 3 生成 Pod 1kubectl create -f livenessProbe.yaml 可以看到当 test 文件给删除的时候，Pod 就会重启，重启后又会有 test 文件，一直循环下去 livenessProbe-httpget 编写 yaml 文件 12345678910111213141516171819202122vim livenessProbe_httpget.yamlapiVersion: v1kind: Podmetadata: name: liveness-httpget-pod namespace: defaultspec: containers: - name: liveness-httpget-container image: daocloud.io/library/nginx:latest ports: - name: http containerPort: 80 # Pod生成1秒之后会每3秒检测一次是否存在index.html文件，如果超过10秒没有则重启Pod livenessProbe: httpGet: port: http path: /index.html initialDelaySeconds: 1 periodSeconds: 3 # 允许超时时间 timeoutSeconds: 10 创建容器，可以看到是在正常运行的 1kubectl create -f livenessProbe_httpget.yaml 删除 index.html 文件，可以看到会执行重启 12kubectl exec -it liveness-httpget-pod -- /bin/shrm -rf /usr/share/nginx/html/index.html livenessProbe-tcp 编写 yaml 文件 12345678910111213141516vim livenessProbe_tcp.yamlapiVersion: v1kind: Podmetadata: name: liveness-tcp-pod namespace: defaultspec: containers: - name: liveness-tcp-container image: daocloud.io/library/nginx:latest livenessProbe: initialDelaySeconds: 5 timeoutSeconds: 1 tcpSocket: port: 8080 periodSeconds: 3 生成 Pod，会发现一直处于重启状态 1kubectl create -f livenessProbe_tcp.yaml 3.4.3 Start 和 StopStart 和 Stop 是指 Pod 在生成后执行和结束前执行的命令 编写 yaml 文件 1234567891011121314151617vim start_stop.ymlapiVersion: v1kind: Podmetadata: name: start-stop-pod namespace: defaultspec: containers: - name: start-stop-container image: daocloud.io/library/nginx:latest lifecycle: postStart: exec: command: ['/bin/sh','-c','echo this is postStart test &gt; /usr/share/message'] preStop: exec: command: ['/bin/sh','-c','echo this is preStop test &gt; /usr/share/message'] 生成 Pod，进入 Pod 可以看到 Start 执行的命令，由于 Pod 停止后就没有了，所以看不到 Stop 执行的命令 1kubectl exec -it start-stop-pod -- /bin/sh 四、k8s控制器4.1 什么是控制器k8s 中内置了很多 controller，用来控制 Pod 的具体状态和行为。 控制器的类型有： ReplicationController（已弃用） 和 ReplicaSet Deployment DaemonSet StateFulSet Job 和 CronJob Horizontal Pod Autoscaling 4.2 RS 与 Deploymentk8s 管理 Pod 主要用到以下三个组件： Replication Controller（RC）：用来确保容器应用的副本数始终保持在用户定义的副本数，如果有容器异常退出，会自动创建新的 Pod 来代替，如果有异常多出的容器也会自动回收。 ReplicaSet（RS）：相比 RC 多了支持 selector，推荐使用 RS。 Deployment：用来管理 RS。 RS 单独使用 编写 yaml 文件 1234567891011121314151617181920212223vim rs_test.yamlapiVersion: apps/v1kind: ReplicaSetmetadata: # RS名称 name: rs-testspec: # 设置副本数 replicas: 3 selector: # 设置标签 matchLabels: tier: rs-test # 设置模板，会根据此模板生成Pod template: metadata: # 与上边RS设置的标签相对应，说明根据该模板创建的Pod都由标签相对RS管理 labels: tier: rs-test spec: containers: - name: rs-nginx-container image: daocloud.io/library/nginx:latest 生成 RS，可以看到会根据 replicas设置的数量创建 Pod 1kubectl create -f rs_test.yaml 删除这些 Pod，RS 也会按照副本数重新建立新的 Pod 1kubectl delete pod --all RS 与 Deployment 编写 yaml 文件 1234567891011121314151617181920212223242526vim nginx_deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment# Deployment详细信息spec: # 副本数 replicas: 3 # 选择标签 selector: # 标签匹配，与Deployment对应 matchLabels: app: nginx-deployment # Pod模板 template: metadata: # 定义标签 labels: app: nginx-deployment spec: containers: - name: nginx-deployment-container image: daocloud.io/library/nginx:latest ports: - containerPort: 80 生成 Deployment 12kubectl create -f nginx_deployment.yaml --recordrecord:记录命令，方便每次 reversion 的变化 可以看到会生成对应的 RS 和 Pod 扩容 1kubectl scale deployment nginx-deployment --replicas 5 更新镜像 1kubectl set image deployment/nginx-deployment nginx-deployment-container=daocloud.io/library/nginx:1.19.1 可以看到会创建一个新的 RS，以实现灰度更新 回滚 1kubectl rollout undo deployment/nginx-deployment 查看回滚状态 1kubectl rollout status deployment/nginx-deployment 查看历史版本 1kubectl rollout history deployment/nginx-deployment 回到历史指定版本 1kubectl rollout undo deployment/nginx-deployment --to-revision=1 暂停更新 1kubectl rollout pause deployment/nginx-deployment 4.3 DaemonSetDaemonSet 确保全部或者一部分 Node 上运行一个 Pod 的副本，当有 Node 加入集群时，也会为他们新增一个 Pod，当这些 Node 退出集群时，这些 Pod 也会被回收。 DaemonSet 的一些典型用法： 运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph等。 运行日志收集daemon，例如fluentd、logstash等。 运行监控daemon，例如 Prometheus 的 Node exporter、zabbix等。 编写 yaml 文件 12345678910111213141516171819202122vim daemonset_test.yamlapiVersion: apps/v1kind: DaemonSetmetadata: # DaemonSet名称 name: daemonset-test # 设置标签 labels: app: daemonset-nginxspec: selector: # 该标签要与上边标签一致 matchLabels: name: daemonset-nginx template: metadata: labels: name: daemonset-nginx-pod spec: containers: - name: daemonset-nginx-container image: daocloud.io/library/nginx:latest 生成 DaemonSet 1kubectl create -f daemonset_test.yaml 删除一个 Pod 之后 DaemonSet 为保证每个节点都至少有一个副本，就会重新创建新的 Pod 4.4 Job 和 CronJob4.4.1 JobJob 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 Job spec spec.template格式同 Pod 相同 RestartPolicy仅支持Never和OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 spec.completions标志 Job 结束时需要成功运行的 Pod 个数，默认为1 spec.parallelism标志并行运行的 Pod 个数，默认为1 spec.activeDeadlineSeconds标志失败 Pod 的重试最大时间，超过该时间就不会再重试 编写 yaml 文件 1234567891011121314151617vim job.yamlapiVersion: batch/v1kind: Jobmetadata: name: jobspec: template: metadata: name: job-pod spec: containers: - name: job-pod-container image: perl imagePullPolicy: IfNotPresent # 通过perl语言进行圆周率计算，计算小数点后2000位 command: ['perl','-Mbignum=bpi','-wle','print bpi(2000)'] restartPolicy: Never 生成 Job，可以看到开始是 Running，接着就是 Completed，代表 Job 已经完成 1kubectl create -f job.yaml 查看日志可以看到计算好的圆周率 4.4.2 CronJobCron Job 是基于时间管理控制 Job，即在给定的时间只运行一次、周期性的在指定时间运行。 CronJob spec 所有 CronJob 的 schedule: 时间都是基于 kube-controller-manager 的时区。 .spec.schedule 是 .spec 需要的域。它使用了 Cron 格式串，例如 0 * * * * or @hourly ，做为它的任务被创建和执行的调度时间。 .spec.jobTemplate是任务的模版，是必须项。 .spec.startingDeadlineSeconds 域是可选项。它表示任务如果由于某种原因错过了调度时间，开始该任务的截止时间的秒数。过了截止时间，CronJob 就不会开始任务。 不满足这种最后期限的任务会被统计为失败任务。如果该域没有声明，那任务就没有最后期限。 .spec.suspend域也是可选的。如果设置为 true ，后续发生的执行都会挂起。 这个设置对已经开始的执行不起作用。默认是关闭的。 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit是可选的。 这两个字段指定应保留多少已完成和失败的任务。 默认设置为3和1。限制设置为0代表相应类型的任务完成后不会保留。 并发性规则 .spec.concurrencyPolicy声明了 CronJob 创建的任务执行时发生重叠如何处理。 spec 仅能声明下列规则中的一种： Allow (默认)：CronJob 允许并发任务执行。 Forbid： CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行。 Replace：如果新任务的执行时间到了而老任务没有执行完，CronJob 会用新任务替换当前正在运行的任务。 并发性规则仅适用于相同 CronJob 创建的任务。如果有多个 CronJob，它们相应的任务总是允许并发执行的。 编写 yaml 文件 12345678910111213141516171819202122vim crontjob.yamlapiVersion: batch/v1kind: CronJobmetadata: name: cronjobspec: # CronJob必要字段，格式为分时日月周 schedule: &quot;*/1 * * * *&quot; # job模板 jobTemplate: spec: template: spec: containers: - name: crontab-pod image: busybox imagePullPolicy: IfNotPresent command: - /bin/sh - -c - date; echo 'Welcome My K8s' restartPolicy: OnFailure 生成 CronJob，可以看到每分钟都会创建一个 Job 和 Pod 1kubectl create -f cronjob.yaml 五、Service5.1 什么是Service 到这里我们都知道，Deployment 会根据 replicas保证 Pod 的数量，当上图的其中一个 php-fpm Pod 出现问题时，就会新建一个来代替。但这时候会会出现一个问题，新的 Pod 的地址与旧的很可能不一样，那 nginx 也无法连接到新的 Pod，除非修改 nginx 的配置，这样的 k8s 集群效率是非常低的。 Service 就能解决该问题，在 nginx 和 php-fpm 中间添加一个 Service，由该 Service 来管理各 php-fpm Pod 的信息，每个 Pod 会设置一个标签，只要与 SVC 中设置的标签相匹配，就可以进行管理，其它 Pod（例如下图的 nginx）想要访问该 SVC 管理的 Pod，只要通过 SVC 的地址就可以访问到。 Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。 5.2 Service发布服务（服务类型)对一些应用的某些部分（如前端），可能希望将其暴露给 Kubernetes 集群外部 的 IP 地址。 Kubernetes ServiceTypes 允许指定你所需要的 Service 类型，默认是 ClusterIP。 Type 的取值以及行为如下： ClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ServiceType。 NodePort：通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到自动创建的 ClusterIP 服务。 通过请求 &lt;节点 IP&gt;:&lt;节点端口&gt;，你可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。 ExternalName：通过返回 CNAME 和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。 无需创建任何类型代理。 ClusterIP NodePort LoadBalancer ExternalName 5.3 虚拟 IP 和 Service 代理在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。 kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 代理模式分类： userspace 代理模式 在该模式下，所有操作都要通过 kube-proxy 进行一个代理的操作，kube-proxy 的压力相对会较大。 iptables 代理模式 在该模式下，所有的访问都通过 iptables 来处理，kube-proxy 的压力就会减小很多。 IPVS 代理模式 在该模式下，iptables 换成了 IPVS，通过内核模块来实现负载均衡。 在 ipvs 模式下，kube-proxy 监视 Kubernetes 服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保 IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。 IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。 与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。 IPVS 提供了更多选项来平衡后端 Pod 的流量。 这些是： rr：轮替（Round-Robin） lc：最少链接（Least Connection），即打开链接数量最少者优先 dh：目标地址哈希（Destination Hashing） sh：源地址哈希（Source Hashing） sed：最短预期延迟（Shortest Expected Delay） nq：从不排队（Never Queue） 注意：当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。 5.4 ClusterIPClusterIP 主要在每个 Node 节点使用 iptables / IPVS，将发向 ClusterIP 对应端口的数据，转发到 kube-proxy 中，kube-proxy 内部可以实现负载均衡，并可以查询到该 Service 下对应 Pod 的地址和端口，进而把数据转发给对应的 Pod 的地址和端口。 示例 创建 Deployment 1234567891011121314151617181920212223242526vim nginx_svc_deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment namespace: defaultspec: replicas: 3 selector: # 创建两个标签 matchLabels: app: nginx version: latest template: metadata: labels: app: nginx version: latest spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest # 释放的端口 ports: - name: http-port containerPort: 80 创建 Service 12345678910111213141516vim clusterip_svc.yamlapiVersion: v1kind: Servicemetadata: name: nginx-clusterip-service namespace: defaultspec: type: ClusterIP # 用来匹配Pod中的标签，全部匹配才会管理该Pod selector: app: nginx version: latest ports: - name: http-port port: 80 targetPort: 80 可以看到 Deployment 和 SVC 都已经创建成功，且直接访问 SVC 的 ClusterIP 地址就可以访问到 Pod 了 测试负载均衡，在各容器内部创建一个 html 页面 1234kubectl exec -it nginx-deployment-*** -- /bin/bashecho 'this is node01-pod01' &gt; /usr/share/nginx/html/pod.htmlecho 'this is node02-pod01' &gt; /usr/share/nginx/html/pod.html... 可以看到是实现了负载均衡的 5.5 无头服务（Headless Services）有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（spec.clusterIP）的值为 &quot;None&quot; 来创建 Headless Service。 可以使用无头 Service 与其他服务发现机制进行接口，而不必与 Kubernetes 的实现捆绑在一起。 对这无头 Service 并不会分配 Cluster IP，kube-proxy 不会处理它们， 而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了选择算符。 示例 创建 Headless Service 1234567891011121314vim headless_service.yamlapiVersion: v1kind: Servicemetadata: name: headless-service namespace: defaultspec: selector: app: nginx version: latest clusterIP: &quot;None&quot; ports: - port: 80 targetPort: 80 可以看到是没有分配 VIP 的 安装 bind-utils 工具来测试无头服务的作用，可以看到即使没有了 VIP，但依旧可以通过域名来访问到不同的 Pod 123456yum -y install bind-utils# k8s内部使用dns访问格式：SVC名称.命名空间.svc.集群域(默认集群域为cluster.local)# dig命令可以用来获取域名的详细信息# 10.244.0.37是其中一个coredns的地址# -t A:显示A记录，A记录是将域名指向一个IPv4地址，即一个域名解析到一个IP地址dig -t A headless-service.default.svc.cluster.local @10.244.0.37 5.6 NodePortNodePort 的原理在于在 Node 上开放了一个端口，将该端口的流量导入到 kube-proxy 中，然后再由 kube-proxy 传送给不同的 Pod。 示例 创建 Service，这里的标签仍与上面 ClusterIP 中的 Deployment 创建的 Pod 相匹配 123456789101112131415vim nodeport_svc.yamlapiVersion: v1kind: Servicemetadata: name: nginx-nodeport-service namespace: defaultspec: type: NodePort selector: app: nginx version: latest ports: - name: http-port port: 80 targetPort: 80 可以看到会暴露一个端口，外部通过这个端口就可以访问到内部的 Pod，且是每一个 Node 都开启了这个端口，所以也可以实现负载均衡 在 iptables / IPVS 规则中可以看到开启该端口的规则 12iptables -t nat -nvl | grep 31419ipvsadm -Ln | grep 31419 5.7 LoadBalancerLoadBalancer 就是在 NodePort 的基础上，通过 LAAS 来实现负载均衡，用户指要访问 LAAS即可，LAAS 会将请求通过调度转发给不同的 Node。 5.8 ExternalNameExternalName 通过返回 CNAME 和它的值，将服务映射到 ExternalName 字段的内容，ExternalName 没有 selector，也没有端口的设置，对于运行在集群之外的服务，ExternalName 是通过该外部服务的别名来提供服务的。 当这个 Service 创建成功时，就会有 externalname-service.default.svc.cluster.local 的 fqdn 被创建，如果有用户访问到这个 fqdn，就会被改写成 my.database.example.com，这就是 DNS 内部的一个 CNAME 记录，也就是别名记录。 示例 创建测试用的 Pod 123456789101112apiVersion: v1kind: Podmetadata: name: curl-podspec: containers: - name: curl-pod-container # curl镜像包含测试网络和DNS的工具 image: docker.io/appropriate/curl imagePullPolicy: IfNotPresent command: ['sh', '-c'] args: ['echo &quot;curl test&quot;; sleep 3600'] 创建 ExternalName 12345678apiVersion: v1kind: Servicemetadata: name: externalname-svcspec: type: externalname # 引入百度的网址 externalName: www.baidu.com 进入 Pod 内部测试可以看到，通过 nslookup 可以解析到百度的地址 1nolookup SVC名称.命名空间.svc.集群域 5.9 ingress5.9.1 什么是ingressingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。 ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。 ingress 由两部分构成： ingress controller：将新加入的 ingress 转化成 Nginx 的配置文件并使之生效。 ingress 服务：将 Nginx 的配置抽象成一个 ingress 对象，每添加一个新的服务只需写一个新的 ingress 的 yaml 文件即可。 ingress controller 主要有两种，nginx-ingress和traefik-ingress，这里主要讲nginx-ingress。 ingress 官方网址：https://kubernetes.github.io/nginx-ingress ingress GitHub 网址：https://github.com/kubernetes/nginx-ingress nginx-ingress功能 nginx-ingress主要负责向外暴露服务，同时提供负载均衡的功能。 Nginx 对后端运行的服务（Service1、Service2）提供反向代理，在配置文件中配置了域名与后端服务 Endpoints 的对应关系。客户端通过使用 DNS 服务或者直接配置本地的 hosts 文件，将域名都映射到 Nginx 代理服务器。当客户端访问 service1.com 时，浏览器会把包含域名的请求发送给 Nginx 服务器，Nginx 服务器根据传来的域名，选择对应的 Service，这里就是选择 Service1 后端服务，然后根据一定的负载均衡策略，选择 Service1 中的某个容器接收来自客户端的请求并作出响应。过程很简单，Nginx 在整个过程中仿佛是一台根据域名进行请求转发的“路由器”。 nginx-ingress工作过程 nginx-ingress模块在运行时主要分为三个主体： Store：Store 会与 APIServer 以协程的 Pod 方式进行一个监听状态，发生新的事件会写入循环队列里。 NginxController：NginxController 会监听循环队列里的事件，发生一个循环就会更新一个事件，并写入 SyncQueue 里。 SyncQueue：SyncQueue 协程会定期拉取需要执行的任务（如果有必要的则直接从 Store 拉取过来进行修改），接着判断是否需要 reload nginx，最后会以 nginx 模块运行。 5.9.2 ingress规则每个 HTTP 规则都包含以下信息： 可选的 host。在此示例中，未指定 host，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 host（例如 www.cqm.com），则 rules 适用于该 host。 路径列表 paths（例如，/testpath）,每个路径都有一个由 serviceName 和 servicePort 定义的关联后端。 在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。 backend（后端）是 Service 文档 中所述的服务和端口名称的组合。 与规则的 host 和 path 匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的 backend。 路径类型 Ingress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 pathType 的路径无法通过合法性检查。当前支持的路径类型有三种： ImplementationSpecific：对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。 Exact：精确匹配 URL 路径，且区分大小写。 Prefix：基于以 / 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 / 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。 5.9.3 部署nginx-ingress 通过 Bare-metal（NodePort） 方式部署，先下载 yaml 文件 123456789# 地址一# https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.46.0/deploy/static/provider/baremetal/deploy.yaml# 地址二# https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/provider/baremetal/deploy.yaml# 由于镜像地址也在国外，所以通过国内镜像源拉取docker pull registry.aliyuncs.com/google_containers/nginx-ingress-controller:v0.46.0docker tag registry.aliyuncs.com/google_containers/nginx-ingress-controller:v0.46.0 k8s.gcr.io/ingress-nginx/controller:v0.46.0 获取镜像地址，下载好需要的镜像，并导入镜像到其它 node 上 通过 deploy.yaml 文件生成 svc 1kubectl create -f deploy.yaml 5.9.4 ingress HTTP代理访问 ingress-nginx会根据配置好的 yaml 文件，自动配置 nginx.conf 和虚拟主机文件。 创建 deployment1、deployment2、service1、service2 1234567891011121314151617181920212223242526272829303132333435vim nginx_deployment_svc1.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment1spec: replicas: 2 selector: matchLabels: name: nginx1 template: metadata: labels: name: nginx1 spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service1spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx1 1234567891011121314151617181920212223242526272829303132333435vim nginx_deployment_svc2.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment2spec: replicas: 2 selector: matchLabels: name: nginx2 template: metadata: labels: name: nginx2 spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service2spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx2 创建 nginx-ingress1、nginx-ingress2 123456789101112131415161718192021222324252627282930313233343536373839vim nginx_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: nginx-ingress1spec: rules: # 设置虚拟主机，通过该域名访问 - host: www.cqm1.com http: paths: # 路径列表 - path: /pod.html # 路径类型 pathType: ImplementationSpecific # 后端 backend: # 指定连接哪个SVC serviceName: nginx-service1 servicePort: 80---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: nginx-ingress2spec: rules: - host: www.cqm2.com http: paths: - path: /pod.html pathType: ImplementationSpecific backend: service: name: nginx-service2 port: number: 80 创建 123kubectl create -f nginx_deployment_svc1.yamlkubectl create -f nginx_deployment_svc2.yamlkubectl create -f nginx_ingress.yaml 给每个 Pod 写入信息，方便查看负载均衡 12kubectl exec -it nginx-deployment-... -- /bin/bashecho 'this is nginx-pod-1' &gt; /usr/share/nginx/html/pod.html 查看 ingress-nginx 所创建的 svc 暴露的端口，以及 service1、service2、nginx-ingress1、nginx-ingress2 123kubectl get svc -n ingress-nginxkubectl get svckubectl get ingress 在 /etc/hosts 文件下写入域名与 IP 绑定，访问测试，可以看到实现了负载均衡 可以进入 ingress 控制器内部查看 nginx 配置，可以看到自动添加的代理配置 12kubectl exec -it -n ingress-nginx ingress-nginx-controller-... -- /bin/bashcat nginx.conf 5.9.5 ingress HTTPS代理访问 创建私钥 12openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=nginxsvc/O=nginxsvc&quot;kubectl create secret tls tls-secret --key tls.key --cert tls.crt 创建 deployment3、service3、nginx-ingress3 1234567891011121314151617181920212223242526272829303132333435vim nginx_deployment_svc3.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment3spec: replicas: 2 selector: matchLabels: name: nginx3 template: metadata: labels: name: nginx3 spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service3spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx3 12345678910111213141516171819vim nginx_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: nginx-ingress3spec: tls: - hosts: - www.cqm3.com secretName: tls-secret rules: - host: www.cqm3.com http: paths: - path: /pod.html pathType: ImplementationSpecific backend: serviceName: nginx-service3 servicePort: 80 创建 1kubectl create -f nginx_deployment_svc3.yaml nginx_ingress.yaml 在每个 Pod 写入 pod.html，便于查看负载均衡效果，并测试 12kubectl exec -it ingress-nginx-... -- /bin/bashecho 'this is node01-pod' &gt; /usr/share/nginx/html/pod.html 5.9.5 Nginx进行基础认证（BasicAuth） 通过 Apache 创建用户认证文件 123yum -y install httpdhtpasswd -c auth cqmkubectl create secret generic basic-auth --from-file=auth 创建 ingress 1234567891011121314151617181920212223vim nginx_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-nginx-auth # 添加基础认证字段 annotations: nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - cqm'spec: rules: - host: auth.cqm.com http: paths: - path: /pod.html pathType: ImplementationSpecific backend: service: # 这里使用HTTP代理实验的service1 name: nginx-service1 port: number: 80 创建 ingress 后测试 5.9.6 Nginx重写 名称 描述 类型 nginx.ingress.kubernetes.io/rewrite-target 必须重定向流量的目标 URI string nginx.ingress.kubernetes.io/ssl-redirect 指示位置部分是否仅可访问 SSL（当 Ingress 包含证书时默认为 True） bool nginx.ingress.kubernetes.io/force-ssl-redirect 即使 Ingress 未启用 TLS，也强制重定向到 HTTPS bool nginx.ingress.kubernetes.io/app-root 定义控制器必须重定向的应用程序根，如果它在“/”上下文中 string nginx.ingress.kubernetes.io/use-regex 指示 Ingress 上定义的路径是否使用正则表达式 bool 先准备好转发后的 deployment、ingress等，这里用上边的 ingress1，用户访问 www.rewritecqm.com 时就跳转到 www.cqm1.com 编写重写 ingress 1234567891011121314151617181920vim rewrite_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: rewrite-ingress annotations: # 访问 www.rewritecqm.com:30779 将跳转到 http://www.cqm1.com:30779/pod.html nginx.ingress.kubernetes.io/rewrite-target: http://www.cqm1.com:30779/pod.htmlspec: rules: - host: www.rewritecqm.com http: paths: - path: /pod.html pathType: Prefix backend: service: name: nginx-service1 port: number: 80 测试 六、k8s存储6.1 配置存储卷配置存储卷并不是用来进行容器间相互交互或 Pod 间数据共享的，而是用于向各个 Pod 注入配置信息的，主要分为以下三种： ConfigMap：可传递普通信息 Secret：可传递密码等敏感的配置信息 DownwardAPI：可传递 Pod 和容器自身的运行信息 6.1.1 ConfigMap许多应用程序都会从配置文件、命令行参数或环境变量中读取配置信息。 ConfigMap API 给我们提供了向容器内部注入信息的机制，ConfigMap 可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制对象。 ConfigMap 的创建方式有三种，分别为基于目录、文件和字面值来创建。 基于目录创建 ConfigMap 创建指定目录，下载官方测试文件 1234mkdir -p /root/k8s/plugin/configmap/dir || cd /root/k8s/plugin/configmap/dirwget https://kubernetes.io/examples/configmap/game.propertieswget https://kubernetes.io/examples/configmap/ui.propertieskubectl create configmap configmap1 --from-file=./ 查看命令 12kubectl describe cm configmap1kubectl get cm configmap1 -o yaml 基于文件创建 ConfigMap 通过 game.properties 和 ui.properties 创建 1kubectl create configmap configmap2 --from-file=game.properties --from-file=ui.properties 基于字面值创建 ConfigMap 通过--from-literal=键名=键值来创建 1kubectl create configmap configmap3 --from-literal=special.how=very 6.1.1.1 Pod中使用ConfigMap使用 ConfigMap 代替环境变量 创建两个 ConfigMap 123456789vim special_cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: special-cm namespace: defaultdata: special.how: very special.type: charm 12345678vim env_cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: env-cm namespace: defaultdata: log_level: INFO 将这两个 ConfigMap 注入到 Pod中 1234567891011121314151617181920212223242526272829303132vim pod1.yamlapiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: pod1-container image: busybox imagePullPolicy: IfNotPresent command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;env&quot;] # env:取某个 ConfigMap 中的某个键值 env: - name: SPECIAL_HOW_KEY # 定义环境变量 valueFrom: # 表示从 ConfigMap 中引用 configMapKeyRef: # 要引用的 ConfigMap 名称 name: special-cm # 引用 ConfigMap 中的哪个键值对 key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-cm key: special.type # envFrom:取某个 ConfigMap 的所有键值 envFrom: - configMapRef: name: env-cm restartPolicy: Never 生成 Pod 后查看日志，可以看到注入成功 1kubectl logs pod1 使用 ConfigMap 设置命令行参数 1234567891011121314151617181920212223242526vim pod2.yamlapiVersion: v1kind: Podmetadata: name: pod2spec: containers: - name: pod2-container image: busybox imagePullPolicy: IfNotPresent command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_HOW_KEY) $(SPECIAL_TYPE_KEY)&quot;] env: - name: SPECIAL_HOW_KEY valueFrom: configMapKeyRef: name: special-cm key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-cm key: special.type envFrom: - configMapRef: name: env-cm restartPolicy: Never 将 ConfigMap 数据添加到一个卷中 创建 Pod 123456789101112131415161718192021vim pod3.yamlapiVersion: v1kind: Podmetadata: name: pod3spec: containers: - name: pod3-container image: busybox imagePullPolicy: IfNotPresent command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 1200&quot; ] # 挂载config-volume，挂载在/etc/config目录下 volumeMounts: - name: config-volume mountPath: /etc/config # 创建一个卷，将special-cm内容写入config-volume卷 volumes: - name: config-volume configMap: name: special-cm restartPolicy: Never 进入 Pod，在挂载目录下查看是否写入到 config 文件里 6.1.1.2 ConfigMap热更新本例子通过 ConfigMap 来实现热更新，可以实现热更新 nginx.conf，但需进入容器内部重载配置文件，所以通过热更新一个 html 来展示效果。 编写 yaml 文件，并创建 123456789101112131415161718192021222324252627282930313233343536373839404142vim hotupdate.yaml# ConfigMapapiVersion: v1kind: ConfigMapmetadata: name: nginx-hotupdate-cm namespace: defaultdata: test.html: 'this is the fist test'---# DeploymentapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment namespace: defaultspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest ports: - containerPort: 80 # 挂载下边创建的volume volumeMounts: - name: test-html-volume mountPath: /usr/share/nginx/html # 创建一个volume，通过ConfigMap方式挂载，使用上面创建的nginx-hotupdate-cm volumes: - name: test-html-volume configMap: name: nginx-hotupdate-cm 1kubectl apply -f hotupdate.yaml 测试是否能够访问到 修改 ConfigMap，并再次访问 1kubectl edit cm nginx-hotupdate-cm 6.1.2 SecretSecret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。 Secret 的类型 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 6.1.2.1 OpaqueOpaque 使用base64编码存储信息，可以通过 base64 --decode 解码获得原始数据，因此安全性弱。 使用示例 生成 base64 编码 1234echo 'admin' | base64YWRtaW4Kecho '12345' | base64MTIzNDUK 编写 yaml 文件 123456789vim opaque.yamlapiVersion: v1kind: Secretmetadata: name: secret-opaquetype: Opaquedata: username: YWRtaW4K password: MTIzNDUK 1kubectl apply -f opaque.yaml 挂载到 volume 中使用，查看测试可以看到挂载后的信息被解码了 12345678910111213141516171819vim opaque-pod1.yamlapiVersion: v1kind: Podmetadata: name: opaque-pod1spec: containers: - name: opaque-pod1-nginx image: daocloud.io/library/nginx:latest # 挂载下边创建的volume-opaque volumeMounts: - name: volume-opaque mountPath: /etc/secrets readOnly: yes # 创建一个secret类型的volume，引用上边创建好的secret-opaque volumes: - name: volume-opaque secret: secretName: secret-opaque 导入到环境变量中并测试 123456789101112131415161718192021222324252627282930313233343536vim opaque-pod2.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: opaque-deploymentspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: name: opaque-pod2 labels: app: nginx spec: containers: - name: opaque-pod2-nginx image: daocloud.io/library/nginx:latest ports: - containerPort: 80 # 将secret导入环境变量中 env: # 命名变量 - name: USER # 引用secret-opaque中的username的键值 valueFrom: secretKeyRef: name: secret-opaque key: username - name: PASSWORD # 引用secret-opaque中的password的键值 valueFrom: secretKeyRef: name: secret-opaque key: password 6.1.2.2 ImagePullSecret可以使用下面两种 type 值之一来创建 Secret，用以存放访问 Docker 仓库来下载镜像的凭据。 kubernetes.io/dockercfg kubernetes.io/dockerconfigjson kubernetes.io/dockerconfigjson 创建 Secret 示例 创建 Secret 1kubectl create secret docker-registry cqmregistry --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL 在 Pod 中运用 1234567891011apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest # 调用创建好的Secret imagePullSecrets: - name: cqmregistry 6.1.2.3 Downward API有时候 Pod 需要获取自身的信息，这时候 Downward API 就派上用场了。 Downward API 是通过 fieldRef 参数获取信息的。 示例一 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: test-pod1spec: containers: - name: test-pod1-container image: busybox imagePullPolicy: IfNotPresent command: [ 'sh', '-c' ] args: [ 'echo &quot;EnvPodName:${EnvPodName} EnvNodeName:${EnvNodeName}&quot;, sleep 3600' ] env: - name: EnvPodName valueFrom: fieldRef: fieldPath: metadata.name - name: EnvNodeName valueFrom: fieldRef: fieldPath: spec.nodeName 示例二 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: test-pod2spec: containers: - name: test-pod2-container image: busybox imagePullPolicy: IfNotPresent command: [ 'sh', '-c' ] args: [ 'echo &quot;EnvPodName:${EnvPodName} EnvNodeName:${EnvNodeName}&quot;, sleep 3600' ] volumeMounts: - name: test-volume mountPath: /test volumes: - name: test-volume downwardAPI: items: - path: 'PodName' fieldRef: fieldPath: metadata.name - path: 'NodeName' fieldRef: fieldPath: spec.nodeName 6.2 本地存储卷Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用程序带来一些问题。问题之一是当容器崩溃时文件丢失。kubelet 会重新启动容器， 但容器会以干净的状态重启。 第二个问题会在同一 Pod 中运行多个容器并共享文件时出现。 Kubernetes Volume 这一抽象概念能够解决这两个问题。 Kubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 当 Pod 不再存在时，Kubernetes 也会销毁临时卷；不过 Kubernetes 不会销毁持久卷。对于给定 Pod 中任何类型的卷，在容器重启期间数据都不会丢失。 卷的类型分为很多种，可通过官方文档查看：https://kubernetes.io/zh/docs/concepts/storage/volumes/ 6.2.1 emptyDir当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。 需要注意的是，容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。 emptyDir 的一些用途： 缓存空间，例如基于磁盘的归并排序。 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。 编写 yaml 文件 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: emptydir-podspec: containers: - name: emptydir-container1 image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /empty1 name: emptydir-volume - name: emptydir-container2 image: busybox imagePullPolicy: IfNotPresent command: ['/bin/sh','-c','sleep 6000'] volumeMounts: - mountPath: /empty2 name: emptydir-volume volumes: - name: emptydir-volume emptyDir: {} 进入 Pod 中的不同容器查看是否共享同一个 volume， 6.2.2 hostPathhostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中，类似 docker 中的 volume 挂载。 hostPath 的一些用法有： 运行一个需要访问 Docker 内部机制的容器；可使用 hostPath 挂载 /var/lib/docker 路径。 在容器中运行 cAdvisor（容器监控工具） 时，以 hostPath 方式挂载 /sys。 允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。 除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。 支持的 type 值如下： 取值 行为 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。 DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 Directory 在给定路径上必须存在的目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 File 在给定路径上必须存在的文件。 Socket 在给定路径上必须存在的 UNIX 套接字。 CharDevice 在给定路径上必须存在的字符设备。 BlockDevice 在给定路径上必须存在的块设备。 注意：具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为，即假如同一个 template 创建出来的 Pod 分配在了不同的 Node 上时，会因为节点的不同而产生不同的行为。 编写 yaml 文件 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: hostpath-podspec: containers: - name: hostpath-pod-container image: daocloud.io/library/nginx:latest volumeMounts: - mountPath: /test name: hostpath-volume volumes: - name: hostpath-volume hostPath: # 宿主机被挂载目录 path: /data # 如果没有/data目录则会被创建 type: DirectoryOrCreate 查看是否挂载成功 需要注意的是，在FileOrCreate下，如果被挂载的目录不存在，那么不会自动创建该目录， 为了确保这种模式能够工作，可以尝试把文件和它对应的目录分开挂载。 hostPath FileOrCreate 配置示例 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: hostpath-fileorcreate-podspec: containers: - name: hostpath-fileorcreate-pod-container image: daocloud.io/library/nginx:latest volumeMounts: - mountPath: /test name: mydir - mountPath: /test/test.txt name: myfile volumes: - name: mydir hostPath: path: /data # 如果宿主机没有/data目录则会自动创建 type: DirectoryOrCreate - name: myfile hostPath: path: /data/test.txt # 因为上面已经确保会有/data目录，所以该文件的挂载不受影响 type: FileOrCreate 6.3 持久存储卷6.3.1 PV和PVCPV 和 PVC 是 k8s 提供的两个 api 资源。 PV 持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者使用存储类来动态供应，PV 是集群资源，和普通的 Volume 一样，也是使用卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 PVC 持久卷申领（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。Pod 会耗用 Node 资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存），同样 PVC 申领也可以请求特定的大小和访问模式。 PV 的供应方式有两种： 静态供应 集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，并且对集群用户可用（可见）。PV 卷对象存在于 Kubernetes API 中，可供用户消费（使用）。 动态供应 如果管理员所创建的所有静态 PV 卷都无法与用户的 PersistentVolumeClaim 匹配， 集群可以尝试为该 PVC 申领动态供应一个存储卷。 这一供应操作是基于 StorageClass 来实现的：PVC 申领必须请求某个 存储类，同时集群管理员必须 已经创建并配置了该类，这样动态供应卷的动作才会发生。 如果 PVC 申领指定存储类为 &quot;&quot;，则相当于为自身禁止使用动态供应的卷。 为了基于存储类完成动态的存储供应，集群管理员需要在 API 服务器上启用 DefaultStorageClass 准入控制器。 举例而言，可以通过保证 DefaultStorageClass 出现在 API 服务器组件的 --enable-admission-plugins 标志值中实现这点；该标志的值可以是逗号 分隔的有序列表。关于 API 服务器标志的更多信息，可以参考 kube-apiserver 文档。 绑定 通俗理解就是一旦 PV 与 PVC 进行了绑定，那么该 PV 就无法与其它 PVC 进行绑定了。 保护 当一个 PV 与 PVC 绑定之后，假设 Pod 被删除，那么该 PV 与 PVC 依旧会是一个绑定的关系。 持久卷的类型 PV 持久卷是用插件的形式来实现的。Kubernetes 目前支持以下插件： awsElasticBlockStore - AWS 弹性块存储（EBS） azureDisk - Azure Disk azureFile - Azure File cephfs - CephFS volume csi - 容器存储接口 (CSI) fc - Fibre Channel (FC) 存储 flexVolume - FlexVolume flocker - Flocker 存储 gcePersistentDisk - GCE 持久化盘 glusterfs - Glusterfs 卷 hostPath - HostPath 卷 （仅供单节点测试使用；不适用于多节点集群； 请尝试使用 local 卷作为替代） iscsi - iSCSI (SCSI over IP) 存储 local - 节点上挂载的本地存储设备 nfs - 网络文件系统 (NFS) 存储 portworxVolume - Portworx 卷 quobyte - Quobyte 卷 rbd - Rados 块设备 (RBD) 卷 storageos - StorageOS 卷 vsphereVolume - vSphere VMDK 卷 访问模式 PV 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。 如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为 对应卷所支持的模式值。 例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器 上以只读的方式导出。每个 PV 卷都会获得自身的访问模式集合，描述的是 特定 PV 卷的能力。 访问模式有： ReadWriteOnce：卷可以被一个节点以读写方式挂载； ReadOnlyMany：卷可以被多个节点以只读方式挂载； ReadWriteMany：卷可以被多个节点以读写方式挂载。 对于不同类型的存储卷访问模式也有不同，如下表 卷插件 ReadWriteOnce ReadOnlyMany ReadWriteMany AWSElasticBlockStore ✓ - - AzureFile ✓ ✓ ✓ AzureDisk ✓ - - CephFS ✓ ✓ ✓ Cinder ✓ - - CSI 取决于驱动 取决于驱动 取决于驱动 FC ✓ ✓ - FlexVolume ✓ ✓ 取决于驱动 Flocker ✓ - - GCEPersistentDisk ✓ ✓ - Glusterfs ✓ ✓ ✓ HostPath ✓ - - iSCSI ✓ ✓ - Quobyte ✓ ✓ ✓ NFS ✓ ✓ ✓ RBD ✓ ✓ - VsphereVolume ✓ - - (Pod 运行于同一节点上时可行) PortworxVolume ✓ - ✓ ScaleIO ✓ ✓ - StorageOS ✓ - - 类 每个 PV 可以属于某个类（Class），通过将其 storageClassName 属性设置为某个 StorageClass 的名称来指定。 特定类的 PV 卷只能绑定到请求该类存储卷的 PVC 申领。 未设置 storageClassName 的 PV 卷没有类设定，只能绑定到那些没有指定特定存储类的 PVC 申领。 回收策略 目前的回收策略有： Retain（保留） – 手动回收 Recycle（回收）– 基本擦除 (rm -rf /thevolume/*) Delete（删除）– 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除 目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。 阶段（状态） 每个卷会处于以下阶段（Phase）之一： Available（可用）– 卷是一个空闲资源，尚未绑定到任何申领； Bound（已绑定）– 该卷已经绑定到某申领； Released（已释放）– 所绑定的申领已被删除，但是资源尚未被集群回收； Failed（失败）– 卷的自动回收操作失败。 示例一 这个实例是先后创建 PV、PVC、Deployment 创建 PV 1234567891011121314apiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfs server: 192.168.88.100 创建 PVC 1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-pvcspec: accessModes: - ReadWriteMany storageClassName: nfs resources: requests: storage: 1Gi 创建 Deployment 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: Deploymentmetadata: name: test-deploymentspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: name: test-deployment-pod labels: app: nginx spec: containers: - name: test-deployment-pod-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent volumeMounts: - name: test-volume mountPath: /usr/share/nginx/html volumes: - name: test-volume persistentVolumeClaim: claimName: test-pvc 示例二 这个实例会由 StatefulSet 自动创建 PVC 部署 NFS 服务器，并在每个节点安装nfs-utils 部署 PV，这里创建了四个 PV 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv1spec: # PV容量大小 capacity: storage: 1Gi # 服务模式为RWO，卷可以被一个节点以读写方式挂载 accessModes: - ReadWriteOnce # 回收策略为Retain，即手动回收 persistentVolumeReclaimPolicy: Retain # 类为nfs storageClassName: nfs # 制定nfs服务器地址和被挂载目录 nfs: path: /nfs1 server: 192.168.88.100---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv2spec: capacity: storage: 2Gi accessModes: - ReadOnlyMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /nfs2 server: 192.168.88.100---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv3spec: capacity: storage: 3Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /nfs3 server: 192.168.88.100---apiVersion: v1kind: PersistentVolumemetadata: name: slow-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: slow nfs: path: /slow server: 192.168.88.100 创建无头 SVC、StatefulSet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657apiVersion: v1kind: Servicemetadata: name: nfs-pv-svc labels: app: nginxspec: ports: - port: 80 targetPort: 80 # 不分配ip地址，为StatefulSet使用 clusterIP: None # 匹配标签 selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: nfs-pv-statefulsetspec: # 匹配标签 selector: matchLabels: app: nginx serviceName: nfs-pv-svc replicas: 3 template: metadata: name: nfs-pv-pod # pod标签 labels: app: nginx spec: containers: - name: nfs-pv-pod-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 挂载持久卷 volumeMounts: - name: nfs-pv-volume mountPath: /usr/share/nginx/html # PVC模板 volumeClaimTemplates: - metadata: name: nfs-pv-volume spec: # 匹配规则 accessModes: [ 'ReadWriteOnce' ] storageClassName: 'nfs' resources: requests: storage: 1Gi 先后创建后查看 Pod 创建情况，可以看到只创建了一个 Pod，因为现有的 PV 只有一个符合匹配条件，而 StatefulSet 是前一个 Pod 创建成功才会创建下一个，因为第二个 Pod 创建不成功，所以第三个 Pod 不会被创建 进入容器内部可以看到挂载成功，去到 NFS 服务器的/nfs1目录下创建test.html文件测试成功 删除创建失败的 Pod 以及对应的 PVC，创建新的两个 PV，以满足 StatefulSet 的挂载要求，剩下的 Pod 就会逐一创建成功 删除上面测试的 Pod，StatefulSet 会重新创建一个 Pod，且数据不会丢失 这里会有个问题，如果删除了 StatefulSet，那么对应的 Pod 也会被删除，可是已经绑定的 PV 并不会删除，这里就需要手动回收了。 手动回收 删除 StatefulSet 删除 PVC 修改 PV 1234kubectl edit pv pv名称# 删除ClaimRef的信息ClainRef:... 6.3.2 StorageClass以上的方法都是静态创建的 PV，会出现 PVC 找不到条件符合的 PV 进行绑定。 而 StorageClass 的作用是根据 PVC 的需求动态创建 PV。 示例一 k8s 在 1.20 版本开始就禁用了 selfLink，所以需在配置文件添加以下内容 StorageClass 是通过存储分配器来动态创建 PV 的，但 k8s 内部的存储分配器不支持 NFS，所以首先要安装 NFS 存储分配器 12345678910111213141516171819202122232425262728293031323334353637383940vim nfs_provisioner.yamlkind: DeploymentapiVersion: apps/v1metadata: name: nfs-client-provisionerspec: replicas: 1 selector: matchLabels: app: nfs-client-provisioner strategy: # 设置升级策略为删除再创建(默认为滚动更新) type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /nfs-provision env: # nfs存储分配器名称 - name: PROVISIONER_NAME value: nfs-client # nfs服务器地址 - name: NFS_SERVER value: 192.168.88.100 # nfs共享目录 - name: NFS_PATH value: /nfs volumes: - name: nfs-client-root nfs: server: 192.168.88.100 path: /nfs 给 NFS 存储分配器授权 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061vim nfs_provisioner_rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner namespace: default---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner namespace: defaultrules: - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner namespace: defaultsubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 创建 StorageClass 1234567891011vim nfs_storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: # storageclass名称 name: nfs-storageclass# nfs存储分配器名称provisioner: nfs-clientparameters: # 表示pvc删除时，所绑定的pv不会被保留，true相反 archieveOnDelete: 'false' 创建 PVC，可以看到 NFS 存储分配器已经自动创建了 PV 与之绑定 12345678910111213vim nfs_storageclass_pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-storageclass-pvcspec: accessModes: - ReadWriteMany # storageclass名称 storageClassName: 'nfs-client' resources: requests: storage: 500Mi 创建一个 Deployment 测试是否能用这个 PVC 1234567891011121314151617181920212223242526vim nfs_provisioner_deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nfs-provisioner-deploymentspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nfs-provisioner-deployment-nginx image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent volumeMounts: - name: nfs-storageclass-pvc mountPath: /usr/share/nginx/html/test volumes: - name: nfs-storageclass-pvc persistentVolumeClaim: claimName: nfs-storageclass-pvc 在 NFS 服务器的共享目录下创建一个 test.html 访问 Deployment 创建出来的 Pod，可以看到是可以访问的 6.4 StatefulSetStatefulSet 是一种提供排序和唯一性保证的特殊 Pod 控制器，当有部署顺序、持久数据或固定网络等相关的特殊需求时，可以用 StatefulSet 控制器来进行控制。 StatefulSet 提供有状态服务，主要功能如下： 实现稳定的持久化存储：通过 PVC 来实现，Pod 之间不能共用一个存储卷，每个 Pod 都要有一个自己专用的存储卷。 实现稳定的网络标识：Pod 重新调度后其 PodName 和 HostName 不变，通过无头 SVC 来实现。 实现有序部署、有序伸缩：Pod 是有顺序的，只有前一个 Pod 创建成功才会创建下一个，直到最后。 实现有序收缩、有序删除：从最后一个开始，依次删除到第一个。 无头 SVC ：为 Pod 生成可以解析的 DNS 记录。 示例一 创建无头 SVC 1234567891011121314vim statefulset_nginx_svc.yamlapiVersion: v1kind: Servicemetadata: name: statefulset-nginx-svcspec: selector: app: nginx clusterIP: None ports: - protocol: TCP port: 8080 targetPort: 80 type: ClusterIP 创建 StatefulSet 12345678910111213141516171819202122232425262728293031323334vim statefulset_nginx.yamlapiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-nginxspec: # 匹配无头SVC serviceName: statefulset-nginx-svc replicas: 4 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: statefulset-nginx-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 # PVC模板 volumeClaimTemplates: - metadata: name: statefulset-nginx-pvc spec: accessModes: [ 'ReadWriteOnce' ] storageClassName: 'nfs-storageclass' resources: requests: storage: 50Mi 可以看到 Pod 是有序创建的，且每个 Pod 都是单独使用一个 PVC 和 PV，删除 StatefulSet 也可以看到 Pod 是有序删除的，且删除后 PVC 与 PV 依旧存在，重新生成 StatefulSet 可以继续使用这些 PVC 和 PV 有序创建 PVC 与 PV 有序删除 创建一个 Pod 用来测试无头 SVC 提供的 DNS 服务 1234567891011apiVersion: v1kind: Podmetadata: name: test-podspec: containers: - name: test-pod-container image: appropriate/curl:latest imagePullPolicy: IfNotPresent command: [ 'sh', '-c' ] args: [ 'echo &quot;this is test&quot;; sleep 36000' ] 进入 Pod 后可以通过nslookup测试，形式为{ServiceName}.{NameSpace}.svc.{ClusterDomain} 通过域名也可以访问各个 Pod，形式为{PodName}{ServiceName}.{NameSpace}.svc.{ClusterDomain} 七、k8s调度器scheduler 是 k8s 集群的调度器，对每一个新创建的 Pod 或者是未被调度的 Pod，scheduler 会选择一个最优的 Node 去运行这个 Pod。然而，Pod 内的每一个容器对资源都有不同的需求，而且 Pod 本身也有不同的资源需求。因此，Pod 在被调度到 Node 上之前， 根据这些特定的资源调度需求，需要对集群中的 Node 进行一次过滤。 在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、 亲和以及反亲和要求、数据局域性、负载间的干扰等等。 调度流程： 过滤：调度器会将所有满足 Pod 调度需求的 Node 选出来。 打分：调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。 7.1 亲和性与反亲和性7.1.1 Node亲和性节点亲和性是通过pod.spec.nodeAffinity来实现的，包括以下两种： preferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 requiredDuringSchedulingIgnoredDuringExecution硬策略 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: node-required-podspec: containers: - name: node-required-pod-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent # 亲和性设置 affinity: # 节点亲和性设置 nodeAffinity: # 硬策略 requiredDuringSchedulingIgnoredDuringExecution: # 节点选择器 nodeSelectorTerms: # 匹配表达式 - matchExpressions: # 制定key，即以这个key的键值为匹配条件 # 通过kubectl get node --show-labels可以获得更多标签 - key: kubernetes.io/hostname # In:label的值在某个列表中 # NotIn:label的值不在某个列表中 # Gt:label的值大于某个值 # Lt:label的值小于某个值 # Exists:某个label存在 # DoesNotExist:某个label不存在 # 这里设置的意思是，根据kubernetes.io/hostname的键值，永远不分配到键值为k8s-node01的节点上 operator: NotIn values: - k8s-node01 创建后可以看到 Pod 不会被创建在 k8s-node01 节点上 preferredDuringSchedulingIgnoredDuringExecution软策略 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: node-preferred-podspec: containers: - name: node-preferred-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent affinity: nodeAffinity: # 软策略 preferredDuringSchedulingIgnoredDuringExecution: # 权重，范围为1-100 - weight: 1 # 偏向于 preference: # 匹配表达式 matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node03 可以看到虽然要求部署到 k8s-node03 节点上，但由于本环境没有该节点，所以就被分配到其它的节点了 软硬合体版 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: node-preandreq-podspec: containers: - name: node-preandreq-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - { key: cpu, operator: In, values: [4core] } preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - { key: disktype, operator: In, values: [ssd] } 软策略和硬策略一起使用就会先满足硬策略再分析软策略，可以看到现有的节点没有可以满足以上条件的，所以 Pod 一直处于 Pending 状态 7.1.2 Pod亲和性有时候需要将某些 Pod 与正在运行的已具有某些特质的 Pod 调度到一起，因此就需要 Pod 亲和性调度方式。 Pod 亲和性是通过spec.affinity.podAffinity/podAntiAffinity来实现的，前者为亲和性，后者为反亲和性，包括以下两种： preferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 requiredDuringSchedulingIgnoredDuringExecution硬策略 首先创建一个标签为app:nginx的 Pod 1234567891011apiVersion: v1kind: Podmetadata: name: test-pod labels: app: nginxspec: containers: - name: test-pod-container-nginx image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent 创建硬策略亲和性 Pod 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: pod-requiredspec: containers: - name: pod-required-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent # 亲和性设置 affinity: # Pod亲和性设置 podAffinity: # 硬策略 requiredDuringSchedulingIgnoredDuringExecution: # 标签选择器 - labelSelector: # 列出规则 matchExpressions: # 甄选app这个标签 - key: app # In:label的值在某个列表中 # NotIn:label的值不在某个列表中 # Exists:某个label存在 # DoesNotExist:某个label不存在 # 表示硬性匹配标签为app=nginx的这个Pod operator: In # 键值 values: - nginx # 分配到与这个Pod所属的节点kubernetes.io/hostname值相同的节点上，可以理解为分配到同一节点上，但可能有多个 topologyKey: kubernetes.io/hostname 可以看到会分配在同一节点上 删除该 Pod，将podAffinity改为podAntiAffinity，将不会分配到一起 7.2 污点和容忍度污点（taint）表示一个节点上存在不良状况，污点会影响 Pod 的调度，其定义方式如下 1kubectl taint node {节点名称} {污点名称}={污点值}:{污点的影响} 污点的影响有三种： NoExecute：不将 Pod 调度到具备该污点的节点上，如果 Pod 已经在该节点运行，则会被驱逐。 NoSchedule：不将 Pod 调度到具备该污点的节点上，如果 Pod 已经在该节点运行，不会被驱逐。 PreferNoSchedule：不推荐将 Pod 调度到具备该污点的节点上。 添加污点 1kubectl taint node k8s-node01 cpu=1:NoExecute 删除污点 1kubectl taint node k8s-node01 cpu=1:NoExecute- 示例一 给 k8s-node01 打上污点 1kubectl taint node k8s-node01 cpu=1:NoExecute 创建 Pod 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: Podmetadata: name: toleration-podspec: containers: - name: toleration-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent # 添加节点亲和性硬策略，让该Pod调度到k8s-node01上 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node01 # 容忍设置 tolerations: # 容忍污点 - key: &quot;cpu&quot; # Equal:指等于该值 # Exist:指有该key就可，值无所谓 operator: &quot;Equal&quot; value: &quot;1&quot; # 污点影响 effect: &quot;NoExecute&quot; # 容忍时间，超过该事件就会被驱除，只有污点影响设置为NoExecute才可用 tolerationSeconds: 36 可以看到该 Pod 依旧可以调度到 k8s-node01 节点上，但超过 3600 秒后就被驱除了 容忍度设置一般用于 DaemonSet 控制器，因为 DaemonSet 控制器下的应用一般是为节点本身提供服务的。 7.3 优先级和抢占式调度当集群的资源（CPU、内存、磁盘等）不足时，新 Pod 的创建会一直处于 Pending 的状态，默认情况下，除了系统外的 Pod，其它 Pod 的优先级都是相同的，如果调高了 Pod 的优先级，那么节点就会将低优先级的 Pod 驱逐，腾出空间给优先级高的 Pod，这就被称为抢占式调度。 示例一 要调整优先级，需要先创建 PriorityClass 12345678910apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata: name: test-priorityclass# 优先级设置value: 1000000# 是否在全局下使用，只可设置一个globalDefault: false# 描述description: &quot;this priorityclass is test&quot; 在 Pod 中调用 12345678910apiVersion: v1kind: Podmetadata: name: priorityclass-podspec: containers: - name: priorityclass-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresentpriorityClassName: test-priorityclass 7.4 为Pod设置计算资源容器运行时会提供一些机制来限制容器可以使用的计算资源（CPU、内存和磁盘等），Pod 模板中也提供了这个功能，主要如下 1234567891011121314# 计算资源设置resources: # 资源限制设置，超过设置的值容器会被停止 limits: # cpu限制 cpu: # 内存限制 memory: # 资源请求设置，至少需要多少资源容器才会被运行 requests: # cpu请求 cpu: # 内存请求 memory: 示例一 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: resources-podspec: containers: - name: resources-container # 压测工具 image: vish/stress imagePullPolicy: IfNotPresent # 进行压测，阈值为150Mi,每一秒增加5Mi压力 args: [ '-mem-total','150Mi','-mem-alloc-size','5Mi','-mem-alloc-sleep','1s' ] resources: limits: cpu: '1' memory: '100Mi' requests: cpu: '200m' memory: '50Mi' 可以看到 Pod 最初可以运行，但20秒后就不行了 7.5 命名空间管理命名空间的主要作用是对 k8s 集群的资源进行划分，这种划分是一种逻辑划分，用于实现多租户的资源隔离。 命名空间的创建 1kubectl create namespace 命名空间名称 命名空间的资源配额 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: ResourceQuotametadata: name: namespace:spec: hard: # 计算资源配额 limits.cpu: limits.memory: requests.cpu: requests.memory: # 存储资源配额 # 在所有PVC中，存储资源的需求不能超过该值 requests.storage: # 允许存在的PVC数量 persistentvolumeclaims: # 允许与{storage-class-name}相关的PVC总量 {storage-class-name}.storageclass.storage.k8s.io/requests.storage: # 对象数量配额 # 允许存在ConfigMap数量 configmaps: # 允许存在的非终止状态的Pod数量 pods: # 允许存在的RC数量 replicationcontrollers: # 允许存在的资源配额数量 resourcequotas: # 允许存在的SVC数量 services: # 允许存在的LoadBalancer类型的SVC数量 services.loadbalancers: # 允许存在的NodePort类型的SVC数量 services.nodeports: # 允许存在的Secret数量 secrets: 7.5.1 命名空间的资源配额示例一 先创建一个命名空间 1kubectl create namespace test-ns 创建资源配额 12345678910apiVersion: v1kind: ResourceQuotametadata: name: test-rq namespace: test-nsspec: hard: pods: '2' services: '1' persistentvolumeclaims: '4' 创建一个 Deployment 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: test-ns-depolyment namespace: test-nsspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: test-ns-depolyment-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 配额限制的 Pod 数量是2，但 Deployment 设置的副本数是3，可以看到是创建不了第三个 Pod 的 7.5.2 命名空间单个资源的资源配额通过设置资源配额，可以限定一个命名空间下使用的资源总量，但这只是总量限制，对于单个资源没有限制，有时候一个 Pod 就可能用完整个命名空间所指定的资源，为了避免，可以通过LimitRange来对单个资源进行限定。 设置容器的限额范围 1234567891011121314151617181920apiVersion: v1kind: LimitRangemetadata: name: limitrange-container namespace: test-nsspec: limits: - max: cpu: '200m' memory: '300Mi' min: cpu: '100m' memory: '150Mi' default: cpu: '180m' memory: '250Mi' defaultRequest: cpu: '110m' memory: '160Mi' type: Container 当 LimitRange 创建成功后，创建该命名空间下的 Pod 时，各个容器的resource.limits和resource.requests必须满足 Max 和 Min 之间的范围。 而default和defaultRequest是指，当创建 Pod 时没用设置限额，则根据此属性来设置限额。 设置 Pod 的限额范围 1234567891011121314apiVersion: v1kind: LimitRangemetadata: name: limitrange-pod namespace: test-nsspec: limits: - max: cpu: '1' memory: '600Mi' min: cpu: '500m' memory: '300Mi' type: Pod 当 LimitRange 创建成功后，创建该命名空间下的 Pod 时，各个 Pod 的resource.limits和resource.requests必须满足 Max 和 Min 之间的范围。 设置 PVC 的限额范围 123456789101112apiVersion: v1kind: LimitRangemetadata: name: limitrange-pvc namespace: test-nsspec: limits: - max: storage: 1Gi min: storage: 200Mi type: PersistentVolumeClaim 设置 Pod 或容器的比例限额范围 12345678910apiVersion: v1kind: LimitRangemetadata: name: limitrange-ratio namespace: test-nsspec: limits: - maxLimitRequestRatio: memory: 2 type: Pod 设置比例限额可以限制 Pod 或容器设置的请求资源和上限资源的比值，在该示例中，Pod 所有容器的resources.limits.memory的总和要 = Pod 所有容器的resources.requests.memory的总和的两倍，即上限必须是需求的两倍，Pod 才能够创建成功。 7.6 标签、选择器、注解7.6.1 标签k8s 的标签（label）是一种语义化标记标签，可以附加到 k8s 对象上，对它们进行标记和划分。 标签的形式是键值对，每个资源对象都可以拥有多个标签，但每个键都只能有一个值。 对于标签的设置，是通过metadata属性中实现的，如下 123456metadata: name: labels: key1: value1 key2: value2 ... 而对于已有的资源，可以通过以下命令添加或删除标签 12kubectl label 资源类型 资源名称 标签名=标签值kubectl label 资源类型 资源名称 标签名=标签值- 7.6.2 选择器通过标签选择器（selector）就可以快速查找到指定标签的资源。 通过-l查找方式如下 1kubectl get pod -l 标签名=/!=标签值 通过in notin查找 1kubectl get pod -l '标签名1 in/notin (标签值1,标签值2)' 每种基于控制器的对象也可以使用标签来选择需要操作的 Pod，如 Job、Deployment、DaemonSet 等都可以在spec中指定选择器，以查找到符合条件的 Pod，如下 12345678spec: selector: matchLabels: app: nginx release: stable matchExpressions: - { key: env, operator: In, values: [dev] } - { key: track, operator: Exists } 在创建 SVC 时，都需要制定标签选择器来确定需要控制的资源，如下 12345678apiVersion: v1kind: Servicemetadata: name: svcspec: selector: app: nginx release: stable 在创建 PVC 时，除了用类匹配之外，也可以用标签来匹配适合的 PV，如下 12345678910111213141516171819202122232425apiVersion: v1kind: PersistentVolumemetadata: name: pv labels: pvnumber: pv01spec: capacity: storage: 1Gi accessModes: - ReadWriteMany storageClassName: nginx---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvcspec: selector: matchLabels: pvnumber: pv01 resources: requests: storage: 1Gi storageClassName: nginx 7.6.3 注解注解（annotation）也是一种类似标签的机制，但只是给资源添加更多信息的方式，类似注释。 设置注解，是通过metadata来实现的，如下 1234567891011apiVersion: v1kind: Podmetadata: name: pod annotation: team: 'cqm' phone: '123456' email: '123456@789.com' ...spec: ... 八、API ServerAPI Server 是集群内布各个组件通信的中介，也是外部控制的入口，k8s 的安全机制基本都是围绕着保护 API Server 来设计的，通过认证、鉴权、准入控制三步来保证 API Server 的安全。 我们在使用 k8s 时，都是通过kubectl工具来访问 API Server 的，kubectl把命令转换为对 API Server 的 REST API 调用。 8.1 身份认证身份认证主要用于确定用户能不能访问，是访问 API Server 的第一个关卡。 通过命令可以看到认证情况，可以看到是通过 6443 端口进行访问的。 要访问 API Server，首先就要进行身份认证，k8s 的身份认证分为以下两类： 常规用户认证：主要提供于普通用户或独立于 k8s 之外的其他外部应用使用，以便能从外部访问 API Server。 ServiceAccount 认证：主要提供于内部的 Pod 使用。 8.1.1 常规用户认证常规用户认证主要有三种方式： HTTPS 证书认证：基于 CA 证书签名的数字证书认证。 HTTP 令牌认证：通过令牌来识别用户。 HTTP Base 认证：通过用户名和密码认证。 令牌认证是最实用也最普及的方式，首先生成一个随机令牌 1head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 接着给 k8s 创建令牌认证文件 123vim /etc/kubernetes/pki/token_auth_file# 格式为:令牌1,用户1,用户IDec0f09bf9a3c40f9db1cc0f8e6664c12,cqm,1 认证文件创建好后，在 API Server 启动文件中进行引用 123vim /etc/kubernetes/manifests/kube-apiserver.yaml# 在spec中添加--token-auth-file=/etc/kubernetes/pki/token_auth_file 接着就可以用认证好的用户访问 API Server 来获取 Pod 的信息了 1curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer ec0f09bf9a3c40f9db1cc0f8e6664c12&quot; 但可以看到还是失败，因为还没有进行授权，后边将进行授权。 8.1.2 ServiceAccount认证ServiceAccount 认证主要提供于集群内布的 Pod 中的进程使用，常规用户认证是不限制命名空间的，但 ServiceAccount 认证的局限于它所在的命名空间中。 默认 ServiceAccount 每个命名空间都有个默认的 ServiceAccount，如果创建 Pod 时没用指定，那么就会使用默认的 ServiceAccount。 通过命令可以看到默认的 ServiceAccount 创建一个 Pod 进行测试 1234567891011apiVersion: v1kind: Podmetadata: name: sa-podspec: containers: - name: sa-pod-container image: appropriate/curl:latest imagePullPolicy: IfNotPresent command: ['sh', '-c'] args: ['echo &quot;this is sa test&quot;; sleep 3600'] 查看 Pod 的详细信息，可以看到被挂载了一个 Secret 类型的卷，实际上这里面就存放了 ServiceAccount 的认证信息 进入容器内部，通过以上地址映射令牌在进行访问 API Server，可以看到由于未授权依旧不行 12curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; 自定义 ServiceAccount 如果某些 Pod 需要访问 API Server，通常会让它引用自定义 ServiceAccount，并为其授权。 首先创建一个自定义 ServiceAccount 1234apiVersion: v1kind: ServiceAccountmetadata: name: my-serviceaccount 创建好后可以查看详细信息，包含了证书和令牌等 创建 Pod 引用 123456789101112apiVersion: v1kind: Podmetadata: name: my-sa-podspec: serviceAccountName: my-serviceaccount containers: - name: my-sa-pod-container image: appropriate/curl:latest imagePullPolicy: IfNotPresent command: ['sh', '-c'] args: ['echo &quot;this is sa test&quot;; sleep 3600'] 就可以看到 Pod 中已经引用了 8.2 RBAC授权k8s 中有基于属性的访问控制（ABAC），基于角色的访问控制（RBAC），基于 HTTP 回调机制的访问控制（Webhook）、Node 认证等授权模式，但1.6版本开始就默认启用 RBAC 模式。 RBAC 授权主要分为两步： 角色定义：指定角色名称，定义允许访问哪些资源以及允许的访问方式。 角色绑定：将角色与用户（常规用户或 ServiceAccount）进行绑定。 而角色定义和角色绑定又分为两种： 只有单一指定命名空间访问权限的角色：角色定义关键字为 Role，角色绑定关键字为 RoleBinding。 拥有集群级别（不限命名空间）访问权限的角色：角色定义为 ClusterRole，角色绑定关键字为 ClusterRoleBinding。 8.2.1 普通角色的定义与绑定普通角色定义 创建一个普通角色 1234567891011121314apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: # 角色名 name: rbac-role namespace: default# 角色规则定义rules: # 表示可以对哪些API组的资源进行操作，这里为空即不限制- apiGroups: [&quot;&quot;] # 可以访问的资源列表，这里为Pod resources: [&quot;pods&quot;] # 可以进行的访问方式 verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 访问方式可以通过开启kubectl反向代理查看，其中的verb就是可以访问的方式 12kubectl proxy --port:8080curl http://localhost:8080/{APIVersion} 普通角色绑定 定义角色后就可以进行绑定角色，绑定可以针对常规用户认证，也可以这对 ServiceAccount 认证。 创建角色绑定 123456789101112131415161718192021apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbac-rolebinding namespace: default# 将角色绑定给那些认证主体，它是一个数组，将常规用户认证cqm和ServiceAccount认证my-serviceaccount加入到rabc-role角色中subjects: # 常规用户认证绑定- kind: User name: cqm apiGroup: &quot;&quot; # ServiceAccount认证绑定- kind: ServiceAccount name: my-serviceaccount apiGroup: &quot;&quot;# 要绑定的角色roleRef: # 普通角色 kind: Role name: rbac-role apiGroup: &quot;&quot; 尝试用之前创建的常规用户认证和 ServiceAccount 认证来访问 API Server，可以发现就可以访问了 1curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer ec0f09bf9a3c40f9db1cc0f8e6664c12&quot; 12curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; 8.2.2 集群角色的定义与绑定集群角色与普通角色的区别如下： 使用的关键字不同：普通角色使用 Role，绑定使用 RoleBinding，集群角色使用 ClusterRole，绑定使用 ClusterRoleBinding。 集群角色不属于任何命名空间，模板也不需要指定命名空间，普通角色要求指定命名空间，如果没指定九默认 default。 集群角色可以访问所有命名空间下的资源，也可以访问不在命名空间下的资源。 集群角色创建和绑定如下 123456789101112131415161718192021222324252627282930# 集群角色定义apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: # 集群角色名 name: rbac-clusterrole# 规则rules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]---# 集群角色绑定apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: rbac-clusterrolebindingsubjects:- kind: User name: cqm apiGroup: &quot;&quot;- kind: ServiceAccount name: my-serviceaccount apiGroup: &quot;&quot; # 由于my-serviceaccount是某个命名空间下的，所以要指定命名空间 namespace: defaultroleRef: kind: ClusterRole name: clusterrole apiGroup: &quot;&quot; 这样一来用户（cqm，my-serviceaccount）都可以访问全局的资源，当然 ClusterRole 也可以和 RoleBinding 绑定在一起，因为 ClusterRole 是不限命名空间的，如果既想给某个认证主体绑定 ClusterRole，又想限制它能访问的命名空间，就可以通过与 RoleBinding 绑定来实现，本例中是指rbac-clusterrole的角色在绑定rbac-rolebinding后，可以访问在default命名空间下的任何资源，如下 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: rbac-clusterrole...---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbac-rolebinding namespace: default... 8.3 创建一个用户只能管理指定的命名空间在实际的生产环境中，Master 的管理者可能有很多个，但不可能给人人都赋予 root 的权限，那么就可以创建新用户给其管理指定命名空间的权限。 创建新用户，这时候该用户是使用不了 k8s 的 123useradd cqmpasswd cqm... 创建证书请求 123456789101112131415161718vim cqm-csr.json{ &quot;CN&quot;: &quot;cqm&quot;, &quot;hosts&quot;: [], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ]} 下载证书生成工具 12345678wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl_1.6.0_linux_amd64wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssljson_1.6.0_linux_amd64wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl-certinfo_1.6.0_linux_amd64mv cfssl_1.6.0_linux_amd64 cfsslmv cfssl-certinfo_1.6.0_linux_amd64 cfssl-certinfomv cfssljson_1.6.0_linux_amd64 cfssljsonchmod a+x cfssl*mv cfssl* /usr/local/bin 生成证书 12cd /etc/kubernetes/pkicfssl gencert -ca=ca.crt -ca-key=ca.key -profile=kubernetes ~/cqm-csr.json | cfssljson -bare cqm 设置集群参数 123456export KUBE_APISERVER=&quot;192.168.88.10:6443&quot;kubectl config set-cluster kubernetes \\--certificate-authority=/etc/kubernetes/pki/ca.crt \\--embed-certs=true \\--server=${KUBE_APISERVER} \\--kubeconfig=cqm.kubeconfig 设置客户端认证参数 12345kubectl config set-credentials cqm \\--client-certificate=/etc/kubernetes/pki/cqm.pem \\--client-key=/etc/kubernetes/pki/cqm-key.pem \\--embed-certs=true \\--kubeconfig=cqm.kubeconfig 设置上下文参数 12345kubectl config set-context kubernetes \\--cluster=kubernetes \\--user=cqm \\--namespace=dev \\--kubeconfig=cqm.kubeconfig 进行 RoleBinding，这里的意思是指将常规用户 cqm 与 ClusterRole 的 admin 角色进行绑定，且指定 dev 的命名空间给 cqm，最终效果是 cqm 只能够访问和管理 dev 命名空间下的所有资源 12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cqm-rolebinding namespace: devsubjects:- kind: User name: cqm apiGroup: &quot;&quot;roleRef: kind: ClusterRole name: admin apiGroup: &quot;&quot; 设置默认上下文 12345mkdir /home/cqm/.kubecp cqm.kubeconfig /home/cqm/.kube/configchown cqm:cqm /home/cqm/.kube/configsu cqmkubectl config use-context kubernetes --kubeconfig=/home/cqm/.kube/config 九、k8s扩展9.1 可视化管理——Kubernetes DashboardKubernetes Dashboard 可以实现 k8s 的可视化管理，可以实现对 Pod、控制器、Service 等资源的创建和维护，并对它们进行持续监控。 9.1.1 安装Kubernetes Dashboard 下载 1wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 模板文件如下，将拉取镜像的地址改为国内地址，同时修改 SVC 模式为 NodePort，这样在集群之外也可以访问 Dashboard 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279apiVersion: v1kind: Namespacemetadata: name: kubernetes-dashboard---apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboardtype: Opaque---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboardtype: Opaquedata: csrf: &quot;&quot;---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboardtype: Opaque---kind: ConfigMapapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardrules: - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;, &quot;kubernetes-dashboard-csrf&quot;] verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;kubernetes-dashboard-settings&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] resourceNames: [&quot;heapster&quot;, &quot;dashboard-metrics-scraper&quot;] verbs: [&quot;proxy&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services/proxy&quot;] resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;, &quot;dashboard-metrics-scraper&quot;, &quot;http:dashboard-metrics-scraper&quot;] verbs: [&quot;get&quot;]---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboardrules: - apiGroups: [&quot;metrics.k8s.io&quot;] resources: [&quot;pods&quot;, &quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: registery.cn-hangzhou.aliyuncs.com/google_containers/dashboard:v2.3.1 imagePullPolicy: Always ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kubernetes-dashboard volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard nodeSelector: &quot;kubernetes.io/os&quot;: linux tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---kind: ServiceapiVersion: v1metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboardspec: type: NodePort ports: - port: 8000 targetPort: 8000 selector: k8s-app: dashboard-metrics-scraper---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboardspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dashboard-metrics-scraper template: metadata: labels: k8s-app: dashboard-metrics-scraper annotations: seccomp.security.alpha.kubernetes.io/pod: 'runtime/default' spec: containers: - name: dashboard-metrics-scraper image: registery.cn-hangzhou.aliyuncs.com/google_containers/metrics-scraper:v1.0.6 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /tmp name: tmp-volume securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 serviceAccountName: kubernetes-dashboard nodeSelector: &quot;kubernetes.io/os&quot;: linux tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: tmp-volume emptyDir: {} 安装 1kubectl apply -f Kubernetes-dashboard.yaml 创建完之后可以查看对应的 SVC，就可以通过浏览器访问了 RBAC 授权 12345678910111213141516171819202122# 创建ServiceAccount认证apiVersion: v1kind: ServiceAccountmetadata: name: dashboard-admin namespace: kubernetes-dashboard---# 创建ClusterRoleBinding将dashboard-admin与集群角色cluster-admin进行绑定apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: dashboardadmin-rbacsubjects:- kind: ServiceAccount name: dashboard-admin namespace: kubernetes-dashboardroleRef: apiGroup: &quot;&quot; kind: ClusterRole name: cluster-admin 获取令牌并填入 1kubectl describe secret dashboard-admin -n kubernetes-dashboard 9.1.2 Kubernetes Dashboard使用 创建一个简单的 Pod 查看 Pod 状态 9.2 资源监控——Prometheus和Grafana9.2.1 安装Prometheus 进行 RBAC 授权 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.rbac.yml 配置 ConfigMap 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.configmap.yml 配置 Deployment 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.deployment.yml 配置 SVC 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.service.yml 9.2.2 安装Grafana 配置 Deployment 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/grafana.deployment.yml 配置 SVC 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/grafana.service.yml 9.2.3 Prometheus和Grafana的使用 添加数据源 使用模板，这里使用代号为 315 的 k8s 监控模板 效果 9.3 日志管理——ElasticSearch、Fluentd、Kibanak8s 推荐采用 ElasticSearch、Fluentd、Kibana（简称EFK）三者组合的方式，对集群的日志进行收集和查询，关系如下： ElasticSearch 是一种搜索引擎，用于存储日志并进行查询。 Fluentd 用于将日志消息从 k8s 发送到 ElasticSearch。 Kibana 是一种图形界面，用于查询 ElasticSearch 中的日志。 EFK 之间的交互如下： 容器运行时会将日志输出到控制台，并以 ”-json.log“ 结尾将日志文件存放到 /var/lib/docker/containers 目录中，而 /var/log 是 linux 系统的日志。 在各个 Node 上运行的 Fluentd 将是收集这些日志，并发送给 ElasticSearch。 Kibana 是直接与用户交互的界面，可以查询 ElasticSearch 中的日志。 9.3.1 安装EFK 配置命名空间 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/create-logging-namespace.yaml 配置 Fluentd 的 ConfigMap 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-configmap.yaml 安装 Fluentd 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml 安装 ElasticSearch 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/es-statefulset.yaml 配置 ElasticSearch 的 SVC 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/es-service.yaml 安装 Kibana 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml 配置 Kibana 的 SVC 1kubectl applt -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/kibana-service.yaml 获取 Kibana 访问地址 1kubectl cluster-info | grep Kibana 十、项目部署10.1 无状态项目部署Guestbook 是一个无状态的多层 Web 应用程序，是一个简单的留言板程序，包含一下三个部分，并拥有读写分离机制： 前端应用：Guestbook 留言板应用，将部署多个实例供用户访问。 后端存储（写）：Redis 主应用，用于写入留言信息，只部署一个案例。 后端存储（读）：Redis 从属应用，用域读取留言信息，将部署多个案例。 部署 Redis 主实例 12345678910111213141516171819202122232425262728293031apiVersion: apps/v1kind: Deploymentmetadata: name: redis-master-deployment labels: app: redisspec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: name: redis-master-pod labels: app: redis role: master tier: backend spec: containers: - name: redis-master-container image: daocloud.io/library/redis:latest imagePullPolicy: IfNotPresent resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 部署 Redis 主实例 SVC 12345678910111213141516apiVersion: v1kind: Servicemetadata: name: redis-master-service labels: app: redis role: master tier: backendspec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: master tier: backend 部署 Redis 从属实例 12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: redis-slave-deployment labels: app: redisspec: selector: matchLabels: app: redis role: slave tier: backend replicas: 2 template: metadata: name: redis-slave-pod labels: app: redis role: slave tier: backend spec: containers: - name: redis-slave-container image: daocloud.io/library/redis:latest imagePullPolicy: IfNotPresent resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns ports: - containerPort: 6379 部署 Redis 从属实例 SVC 1234567891011121314apiVersion: v1kind: Servicemetadata: name: redis-slave-service labels: app: redis role: slave tier: backendspec: ports: - port: 6379 targetPort: 6379 selector: app: redis 部署 Guestbook 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: guestbook-deployment labels: app: guestbookspec: selector: matchLabels: app: guestbook tier: frontend replicas: 3 template: metadata: name: guestbook-pod labels: app: guestbook tier: frontend spec: containers: - name: guestbook-container image: kubeguide/guestbook-php-frontend imagePullPolicy: IfNotPresent resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns ports: - containerPort: 80 部署 Guestbook 的 SVC 123456789101112131415apiVersion: v1kind: Servicemetadata: name: guestbook-service labels: app: guestbook tier: frontendspec: type: NodePort selector: app: guestbook tier: frontend ports: - port: 80 nodePort: 30222 10.2 有状态项目部署WordPress 是使用 PHP 开发的开源个人博客平台，是一套非常完善的内容管理系统，支持非常丰富的插件和模板，主要包含以下两个部分： 前端应用：WordPress。 后端应用：MySQL 数据库，使用 PVC 来存储博客的数据。 首先生成一个数据库密码 1echo -n 'toortoor' | base64 创建一个 Secret 存放密码 1234567apiVersion: v1kind: Secretmetadata: name: mysql-secrettype: Qpaquedata: mysql_password: dG9vcnRvb3I= 部署 nfs-client，实现自动分配 PV 给 PVC，步骤参照 6.3.2 部署 MySQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1kind: Servicemetadata: name: mysql-service labels: app: wordpressspec: selector: app: wordpress tier: mysql ports: - port: 3306 clusterIP: None---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-pvc labels: app: wordpressspec: accessModes: - ReadWriteOnce storageClassName: nfs-storageclass resources: requests: storage: 2Gi---apiVersion: apps/v1kind: Deploymentmetadata: name: mysql-deployment labels: app: wordpressspec: replicas: 1 selector: matchLabels: app: wordpress tier: mysql strategy: type: Recreate template: metadata: name: mysql-pod labels: app: wordpress tier: mysql spec: containers: - name: mysql-container image: daocloud.io/library/mysql:5.7 imagePullPolicy: IfNotPresent ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: mysql_password volumeMounts: - name: mysql-datadir mountPath: /var/lib/mysql volumes: - name: mysql-datadir persistentVolumeClaim: claimName: mysql-pvc 部署 WordPress 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778apiVersion: v1kind: Servicemetadata: name: wordpress-service labels: app: wordpressspec: ports: - port: 80 nodePort: 30111 selector: app: wordpress tier: frontend type: NodePort---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: wordpress-pvc labels: app: wordpressspec: storageClassName: nfs-storageclass accessModes: - ReadWriteOnce resources: requests: storage: 2Gi---apiVersion: apps/v1kind: Deploymentmetadata: name: wordpress-deployment labels: app: wordpress tier: frontendspec: selector: matchLabels: app: wordpress tier: frontend strategy: type: Recreate template: metadata: name: wordpress-pod labels: app: wordpress tier: frontend spec: containers: - name: wordpress-container image: daocloud.io/daocloud/dao-wordpress:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: wordpress # 这里用到了MySQL的变量参数 env: # Mysql的SVC名称 - name: WORDPRESS_DB_HOST value: mysql-service - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: mysql_password volumeMounts: - name: wordpress-datadir mountPath: /var/www/html volumes: - name: wordpress-datadir persistentVolumeClaim: claimName: wordpress-pvc 部署 Ingress，通过 www.cqm.com:30111 就能访问 WordPress 123456789101112131415161718apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: wordpress-ingress-nginx labels: app: wordpressspec: rules: - host: www.cqm.com http: paths: - path: /var/www/html pathType: ImplementationSpecific backend: service: name: wordpress-service port: number: 30111 测试 10.3 使用Helm部署项目Helm 是 k8s 的一个子项目，是一种 k8s 包管理平台，它能够定义、部署、升级非常复杂的 k8s 应用集合，并进行版本管理。 Helm 客户端：是一种远程命令客户端工具，主要用于 Chart 文件的创建、打包和发布部署，以及和 Chart 仓库的管理。Helm 发出的请求，根据 Chart 结构生成发布对象，并将 Chart 解析成各个 k8s 资源的实际部署文件，供 k8s 创建相应资源，同时还提供发布对象的更新、回滚、统一删除等功能。 Chart：是应用程序的部署定义，包含各种 yaml 文件，可以采用 TAR 格式打包。 Chart 仓库：Helm 中存放了各种应用程序的 Chart 包以供用户下载，Helm 可以同时管理多个 Chart 仓库，默认情况下管理一个本地仓库和一个远程仓库。 发布对象：在 k8s 集群中部署的 Chart 称为发布对象，Chart 和发布对象的关系类似于镜像和容器，前者是部署的定义，后者是实际部署好的应用程序。 10.3.1 Helm安装 安装 123wget https://get.helm.sh/helm-v3.6.2-linux-amd64.tar.gztar -xf helm-v3.6.2-linux-amd64.tar.gzcp linux-amd64/helm /usr/local/bin/ 设置环境变量 KUBECONFIG 来指定存有 ApiServre 的地址与 token 的配置文件地址，默认为~/.kube/config 1export KUBECONFIG=/root/.kube/config 配置 Helm 仓库 12helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/chartshelm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/ 10.3.2 Helm Chart的基本操作Chart 的创建 创建 Chart 1helm create chart名称 执行后会在当前目录下创建一个文件，可用 tree 命令查看该目录结构 chars：存放该 Chart 以来的所有子 Chart 的目录，这些子 Chart 也拥有这 4 个部分，如果有子 Chart ，则需要在父 Chart 创建一个 requirements.yaml 文件，在文件中记录这些子 Chart。 Chart.yaml：记录该 Chart 的相关信息，并定义在文件中的 .Chart 开头的属性值。 template：存放了 k8s 部署文件的 Helm 模板，其中扩展了 Go Template 语法。 values.yaml：定义在文件中的 .Values 开头的属性值。 _helpers.tpl：它是一个i模板助手文件，用于定义通用信息，然后在其他地方使用。 NOTES.txt：会在 Chart 部署命令后，代入具体的参数值，产生说明信息。 test-connection.yaml：用于定义部署完成后需要执行的测试内容，以便测试是否部署成功。 Chart 的验证 在发布 Chart 之前，可以通过命令检查 Chart 文件是否有误，如下 1helm lint chart目录/ 同时也可以用命令将各项值组合为 k8s 的 yaml 文件，查看是否为预期内容，比如下图就是其中 Deployment 的内容 1helm install --dry-run --debug 发布名称 -name chart目录名称 Chart 的发布 Chart 的发布命令如下 1helm install 发布版本名称 chart文件目录 查看目前发布的版本 1helm list 将 Chart 打包到仓库中 查看仓库命令 1helm repo list 查看仓库中的包 123helm search repo/hub# 也可以在后面加上应用名，如helm search repo/hub nginx 首先打包 1helm package chart目录 安装 Push 插件 1helm plugin install https://github.com/chartmuseum/helm-push.git 上传 1helm push tar包名 仓库 发布版本的更新、回滚和删除 更新 1helm upgrade 发布版本名称 chart目录或tar flags 查看历史版本 1helm history 发布版本名称 回滚 1helm rollback 发布版本名称 版本号 删除 1helm delete 发布版本名称","link":"/2024/02/18/kubernetes/"},{"title":"SUSE 第一周产品部署使用随记","text":"安装单点的 RKE2该 RKE2 作为 local，用于部署 rancher 1234567891011121314151617181920212223242526272829303132333435363738394041# 配置目录和文件准备mkdir -pv /etc/rancher/rke2cat &gt; /etc/rancher/rke2/config.yaml &lt;&lt;EOFtoken: my-shared-secrettls-san: - 172.16.170.200system-default-registry: registry.cn-hangzhou.aliyuncs.comdebug: trueEOF# 安装 rke2-server 等二进制curl -sfL https://rancher-mirror.rancher.cn/rke2/install.sh | INSTALL_RKE2_MIRROR=cn sh -# 启动第一台 server 节点systemctl enable rke2-server --now# 方便后续运维的配置mkdir -pv ~/.kubeln -s /etc/rancher/rke2/rke2.yaml ~/.kube/configecho &quot;export CONTAINER_RUNTIME_ENDPOINT=\\&quot;unix:///run/k3s/containerd/containerd.sock\\&quot;&quot; &gt;&gt; ~/.bashrcecho &quot;export CONTAINERD_ADDRESS=\\&quot;/run/k3s/containerd/containerd.sock\\&quot;&quot; &gt;&gt; ~/.bashrcecho &quot;export CONTAINERD_NAMESPACE=\\&quot;k8s.io\\&quot;&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=$PATH:/var/lib/rancher/rke2/bin&quot; &gt;&gt; ~/.bashrcecho &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrccurl https://rancher-mirror.rancher.cn/helm/get-helm-3.sh | INSTALL_HELM_MIRROR=cn bash -s -- --version v3.17.1echo &quot;source &lt;(helm completion bash)&quot; &gt;&gt; ~/.bashrcexport NERDCTL_VERSION=1.7.6wget &quot;https://files.m.daocloud.io/github.com/containerd/nerdctl/releases/download/v$NERDCTL_VERSION/nerdctl-$NERDCTL_VERSION-linux-amd64.tar.gz&quot;tar Czvxf /usr/local/bin nerdctl-$NERDCTL_VERSION-linux-amd64.tar.gz &amp;&amp; rm -rf nerdctl-$NERDCTL_VERSION-linux-amd64.tar.gz 安装 Rancher123456789101112131415161718192021222324# 通过 helm chart 方式安装helm repo add rancher-stable https://releases.rancher.com/server-charts/stable# 安装 cert-managerhelm repo add jetstack https://charts.jetstack.iohelm repo updatehelm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.15.3 \\ --set installCRDs=true# 安装 rancherhelm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --create-namespace \\ --set hostname=xxx.com \\ --set replicas=1 \\ --set bootstrapPassword=xxx \\ --set rancherImage=registry.cn-hangzhou.aliyuncs.com/rancher/rancher \\ --set systemDefaultRegistry=registry.cn-hangzhou.aliyuncs.com RKE2 默认会安装 Nginx Ingress Controller，监听节点的 80/443 端口，而安装 Rancher 的时候设置好 hostname 的话会创建一个 Ingress，所以可以通过该 Ingress 进行访问 创建集群在 UI 创建集群后，会提供注册命令，在节点上执行该命令进行注册 1234567891011121314151617181920212223root@rke2-test-controller-0:~# curl --insecure -fL https://xxx.com/system-agent-install.sh | sudo sh -s - --server https://xxx.com --label 'cattle.io/os=linux' --token xxx --ca-checksum xxx --etcd --controlplane --worker --node-name rke2-test-controller-0[INFO] Label: cattle.io/os=linux[INFO] Role requested: etcd[INFO] Role requested: controlplane[INFO] Role requested: worker[INFO] Using default agent configuration directory /etc/rancher/agent[INFO] Using default agent var directory /var/lib/rancher/agent[INFO] Determined CA is necessary to connect to Rancher[INFO] Successfully downloaded CA certificate[INFO] Value from https://xxx.com/cacerts is an x509 certificate[INFO] Successfully tested Rancher connection[INFO] Downloading rancher-system-agent binary from https://xxx.com/assets/rancher-system-agent-amd64[INFO] Successfully downloaded the rancher-system-agent binary.[INFO] Downloading rancher-system-agent-uninstall.sh script from https://xxx.com/assets/system-agent-uninstall.sh[INFO] Successfully downloaded the rancher-system-agent-uninstall.sh script.[INFO] Generating Cattle ID[INFO] Successfully downloaded Rancher connection information[INFO] systemd: Creating service file[INFO] Creating environment file /etc/systemd/system/rancher-system-agent.env[INFO] Enabling rancher-system-agent.serviceCreated symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.[INFO] Starting/restarting rancher-system-agent.serviceroot@rke2-test-controller-0:~# 注册后发现 cattle-cluster-agent 一直在崩溃重启 123456root@rke2-test-controller-0:~# kubectl -n cattle-system get podNAME READY STATUS RESTARTS AGEcattle-cluster-agent-767b67b66f-bcl2s 0/1 CrashLoopBackOff 5 (79s ago) 10mroot@rke2-test-controller-0:~# kubectl -n cattle-system logs cattle-cluster-agent-767b67b66f-bcl2s -p...ERROR: https://xxx.com/ping is not accessible (Could not resolve host: xxx.com) 这是由于该域名没有 DNS 去做解析，可以通过 CoreDNS 实现暂时的映射，然后重启即可 12345678910111213141516171819202122.:53 { errors health { lameduck 5s } ready kubernetes cluster.local cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus 0.0.0.0:9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance hosts { 172.16.170.200 xxx.com fallthrough }} 集群 Ready Monitoring通过 UI 选择 monitoring helm chart 即可完成安装，会有一些基本的组件（e.g. prometheus/alertmanager…） WebHook 配置告警可以对接多种形式，WebHook 则是通过 AlertmanagerConfig 的 CR 完成配置 123456789101112131415161718192021cat &lt;&lt;EOF | kubectl apply -f -apiVersion: monitoring.coreos.com/v1alpha1kind: AlertmanagerConfigmetadata: name: test-webhook namespace: defaultspec: receivers: - name: test-webhook webhookConfigs: - httpConfig: tlsConfig: {} sendResolved: false url: https://webhook.site/xxx route: groupBy: [] groupInterval: 5m groupWait: 30s matchers: [] repeatInterval: 4hEOF Logging通过 UI 选择 logging helm chart 即可完成安装，主要是使用了 Logging Operator 配置日志流水线 Logging Operator 会部署一个 FluentBit DaemonSet 用于收集日志，然后将数据传输到 Fluentd，再由 Fluentd 传到不同的 output 主要的 CR 有: Flow: 是一个命名空间自定义资源，它使用过滤器和选择器将日志消息路由到对应的 Output 或者 ClusterOutput ClusterFlow: 用于路由集群级别的日志消息 Output: 用于路由命名空间级别的日志消息 ClusterOutput: Flow 和 ClusterFlow 都可与其对接 部署 ES 和 Kibana基于 ECK Operator 的能力，部署 ES 和 Kibana，后续可通过配置 OutPut 输出到 ES 中 123# Install ECK Operatorkubectl create -f https://download.elastic.co/downloads/eck/2.14.0/crds.yamlkubectl apply -f https://download.elastic.co/downloads/eck/2.14.0/operator.yaml 安装 ES 和 Kibana，存储暂时用本地存储吧 - - 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849cat &lt;&lt;EOF | kubectl apply -f -apiVersion: elasticsearch.k8s.elastic.co/v1kind: Elasticsearchmetadata: name: logging namespace: cattle-logging-systemspec: version: 7.15.2 nodeSets: - name: logging count: 1 config: node.store.allow_mmap: falseEOFcat &lt;&lt;EOF | kubectl apply -f -apiVersion: kibana.k8s.elastic.co/v1kind: Kibanametadata: name: logging namespace: cattle-logging-systemspec: version: 7.15.2 count: 1 elasticsearchRef: name: loggingEOFcat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: HTTPS name: logging-kb namespace: cattle-logging-systemspec: ingressClassName: nginx rules: - host: kibana.warnerchen.io http: paths: - backend: service: name: logging-kb-http port: number: 5601 path: / pathType: Prefix 创建 Flow 和 Output需要先创建 Output 1234567891011121314151617181920212223242526272829303132cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1data: elastic: xxxkind: Secretmetadata: name: logging-es-elastic-user namespace: defaulttype: Opaque---apiVersion: logging.banzaicloud.io/v1beta1kind: Outputmetadata: name: output-to-es namespace: defaultspec: elasticsearch: host: logging-es-http.cattle-logging-system.svc.cluster.local index_name: ns-default password: valueFrom: secretKeyRef: key: elastic name: logging-es-elastic-user port: 9200 scheme: https ssl_verify: false ssl_version: TLSv1_2 suppress_type_name: false user: elasticEOF 创建 Flow，收集标签为 app=nginx 的 Pod 日志 1234567891011121314cat &lt;&lt;EOF | kubectl apply -f -apiVersion: logging.banzaicloud.io/v1beta1kind: Flowmetadata: name: flow-for-default namespace: defaultspec: localOutputRefs: - output-to-es match: - select: labels: app: nginxEOF 查看是否有对应的索引 创建 Pattern 查看日志 NeuVector通过 UI 选择 NeuVector helm chart 即可完成安装 LongHorn安装 LongHorn 之前，需要在所有节点上安装依赖 123apt updateapt -y install open-iscsi nfs-commonsystemctl enable iscsid --now 然后通过 UI 安装 LongHorn 数据卷的快照和恢复LongHorn 支持 SnapShot，可以直接创建快照和恢复 在 UI 中，创建一个快照 然后删除数据文件 1kubectl exec -it nginx-7f6d5dcf8c-tvxcw -- rm -rf /data/test.txt 停止服务 1kubectl scale deployment nginx --replicas=0 通过维护模式重新 Attach 挂载后进入该 Volume，然后选择快照进行恢复 恢复后，Detach 该 Volume，启动服务后，即可看到数据的恢复 数据卷的备份和灾难恢复测试通过 LongHorn 能力实现跨集群的数据备份和恢复，可备份至集群外的 S3 or NFS MinIO 部署，使用了 operator 的能力 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1data: accesskey: bWluaW8= secretkey: VGpCcFkwVTNZVGcyU3c9PQ==kind: Secretmetadata: name: backup-minio-secret namespace: defaulttype: Opaque---apiVersion: v1data: config.env: ZXhwb3J0IE1JTklPX0JST1dTRVI9Im9uIgpleHBvcnQgTUlOSU9fUk9PVF9VU0VSPSJtaW5pbyIKZXhwb3J0IE1JTklPX1JPT1RfUEFTU1dPUkQ9IlRqQnBZMFUzWVRnMlN3PT0iCg==kind: Secretmetadata: name: backup-minio-env-configuration namespace: defaulttype: Opaque---apiVersion: minio.min.io/v2kind: Tenantmetadata: name: backup-minio namespace: defaultspec: buckets: - name: longhorn configuration: name: backup-minio-env-configuration # credsSecret: # name: backup-minio-secret env: - name: MINIO_PROMETHEUS_AUTH_TYPE value: public - name: MINIO_SERVER_URL value: http://minio-hl.warnerchen.io image: quay.m.daocloud.io/minio/minio:RELEASE.2023-10-07T15-07-38Z initContainers: - command: - sh - -c - chown -R 1000:1000 /export/* || true image: quay.m.daocloud.io/minio/minio:RELEASE.2023-10-07T15-07-38Z name: change-permission securityContext: capabilities: add: - CHOWN volumeMounts: - mountPath: /export name: &quot;0&quot; pools: - name: pool-0 resources: limits: cpu: 500m memory: 500Mi requests: cpu: 50m memory: 100Mi servers: 1 volumeClaimTemplate: metadata: {} spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi volumesPerServer: 1 requestAutoCert: false serviceMetadata: minioServiceLabels: mcamel/exporter-type: minio---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: minio namespace: defaultspec: rules: - host: minio.warnerchen.io http: paths: - backend: service: name: minio port: number: 443 path: / pathType: Prefix---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: minio-hl namespace: defaultspec: rules: - host: minio-hl.warnerchen.io http: paths: - backend: service: name: backup-minio-hl port: number: 9000 path: / pathType: PrefixEOF 准备一个 Bucket 在两个集群的 longhorn-system 下创建 Secret，主要有这几个内容 AWS_ACCESS_KEY_ID: Access Key AWS_SECRET_ACCESS_KEY: Secret Key AWS_ENDPOINTS: S3 URL AWS_CERT: 如果使用了自签证书则需要配置 创建好 Secret 后，需要在 LongHorn UI 配置 Backup Target 在任意集群做操作，创建一个 PVC 1234567891011121314cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nginx-pvc namespace: defaultspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: longhornEOF 给 Nginx 挂载后，随意写入一些数据 在该集群的 LongHorn 创建一个备份 在 MinIO 就可以看到这个备份 至此，两个集群的 LongHorn 都是可以看到这个备份的，这是因为使用了同一个 Backup Target 在另一个集群，通过此备份创建一个 Volume 创建好后即可看到该 Volume，此时如果有更多数据写入 Nginx，Volume 也会自动进行同步 当一侧集群宕机，或者服务不可用时，可以使用该 Volume 进行恢复 首先需要激活这个 Volume 激活后使用这个 Volume 创建 PV/PVC 在集群中就可以看到，然后通过这个 PV/PVC 重新创建 Nginx，可以看到原本的数据 Istio在 UI 中可以直接选择 Istio 进行安装 部署两个版本的 Nginx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1data: index.html.v1: | &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;title&gt;Welcome to nginx V1!&lt;/title&gt; &lt;/html&gt; index.html.v2: | &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;title&gt;Welcome to nginx V2!&lt;/title&gt; &lt;/html&gt;kind: ConfigMapmetadata: name: nginx-conf namespace: default---apiVersion: v1kind: Servicemetadata: name: nginx namespace: defaultspec: ports: - name: port-80 port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: ClusterIP---apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx version: v1 name: nginx-v1 namespace: defaultspec: selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 sidecar.istio.io/inject: 'true' spec: containers: - image: docker.io/library/nginx:mainline imagePullPolicy: IfNotPresent name: nginx-v1 volumeMounts: - mountPath: /usr/share/nginx/html/index.html name: nginx-conf subPath: index.html.v1 volumes: - configMap: defaultMode: 420 name: nginx-conf name: nginx-conf---apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx version: v2 name: nginx-v2 namespace: defaultspec: selector: matchLabels: app: nginx version: v2 template: metadata: labels: app: nginx version: v2 sidecar.istio.io/inject: 'true' spec: containers: - image: docker.io/library/nginx:mainline imagePullPolicy: IfNotPresent name: nginx-v2 volumeMounts: - mountPath: /usr/share/nginx/html/index.html name: nginx-conf subPath: index.html.v2 volumes: - configMap: defaultMode: 420 name: nginx-conf name: nginx-confEOF 创建 Istio Gateway 12345678910111213141516cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: nginx-gatewayspec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - &quot;*&quot;EOF 创建 Destination Rule 123456789101112131415cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: nginxspec: host: nginx subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2EOF 创建 Virtual Service，先将流量全部转发到 Nginx V1 12345678910111213141516171819202122cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginxspec: hosts: - &quot;*&quot; gateways: - nginx-gateway http: - match: - uri: prefix: / route: - destination: host: nginx port: number: 80 subset: v1 weight: 100EOF 通过 Istio Gateway 访问 Nginx，会发现返回都是 V1 版本 修改 Virtual Service，将 20% 的流量转发至 V2 12345678910111213141516171819202122232425262728cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginxspec: hosts: - &quot;*&quot; gateways: - nginx-gateway http: - match: - uri: prefix: / route: - destination: host: nginx port: number: 80 subset: v1 weight: 80 - destination: host: nginx port: number: 80 subset: v2 weight: 20EOF 可以看到会有部份流量转发至 V2 熔断也是通过 Destination Rule 实现 123456789101112131415161718cat &lt;&lt;EOF | kubectl apply -f -apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: nginx-circuit-breakerspec: host: nginx trafficPolicy: connectionPool: http: # HTTP1 最大等待请求数 http1MaxPendingRequests: 1 # 每个连接的 HTTP 最大请求数 maxRequestsPerConnection: 1 tcp: # TCP 最大连接数 maxConnections: 1EOF K3sK3s 部署 12345678mkdir -pv /etc/rancher/k3scat &gt; /etc/rancher/k3s/config.yaml &lt;&lt;EOFtoken: 12345system-default-registry: registry.cn-hangzhou.aliyuncs.comEOFcurl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -","link":"/2024/08/21/SUSE-%E7%AC%AC%E4%B8%80%E5%91%A8%E4%BA%A7%E5%93%81%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"Kafka","text":"一、kafka架构kafka 是一个分布式的基于发布和订阅模式的消息队列（message queue），主要用于大数据实时处理领域。 1.1 消息队列同步通信 异步通信 使用消息队列的好处如下： 解耦：允许独立的扩展或修改两边的处理过程 可恢复性：系统的一部分组件失效时，不会影响到整个系统 缓冲：控制和优化数据经过系统的速度 灵活性和峰值处理能力：在特殊情况下，通信中的请求量会急剧增大（如双十一等活动），但这种情况并不常见，如果投入资源来待命是很浪费的，使用消息队列就可以顶住突发的访问压力，而保护集群不会因为请求量过多而崩溃 异步通信：有时候用户不想也不需要立即处理消息，这种时候就允许用户把消息放在队列中，等待处理 消息队列分为点对点模式和发布/订阅模式 点对点模式 消息生产者生产消息发送到 queue 中，消费者再从 queue 拉取消息并消费，消息被消费之后就不再存在于 queue 中，queue 可以有多个消费者，但对于消息而言只能有一个消费者。 发布/订阅模式 消息生产者将消息发送到 topic 中，可以同时有多个消费者订阅该消息，发布到 topic 的消息可以被所有消费者订阅。 1.2 kafka基础架构kafka 有四个核心的 API： The Producer API：允许一个应用程序发布一串流式的数据到一个或者多个 topic The Consumer API：允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理 The Streams API：允许一个应用程序作为一个流处理器，消费一个或者多个 topic 产生的输入流，然后生产一个输出流到一个或多个 topic 中去，在输入输出流中进行有效的转换 The Connector API：允许构建并运行可重用的生产者或者消费者，将 topics 连接到已存在的应用程序或者数据系统 producer：消息生产者 consumer：消息消费者 topic：数据主题，是数据订阅/发布的地方，可以被多个消费者订阅 partition：分区，每个 topic 包含一个或多个分区，分区是一个有序的队列，分区中的消息都会被分配一个 offset，kafka 保证按每一个分区中的顺序将消息发送给消费者，但不保证按每一个 topic 中的顺序将消息发送给消费者；且 partition 是消费者和生产者操作的最小单元 offset（偏移量）：kafka 的存储文件都是按照 offset.kafka 来命名，方便查找数据 leader 副本：消息的订阅/发布都由 leader 来完成 follower 副本：进行消息数据的备份，当 leader 挂了之后，就成为新的 leader 来代替旧的 leader broker：一个 kafka 就是一个 broker，一个集群由多个 broker 组成 consumer group：消费者组，组中有多个消费者，组中的消费者都只能够接收一个分区的消息 zookeeper：保存 kafka 集群状态的信息，2.8.0版本开始支持 kraft 模式，可不依赖 zk 运行 kafka 集群 replicas：副本数 1.3 kafka 存储机制kafka 内部会自己创建一个 _consumer_offsets 并包含 50 个分区，这些主题用来保存消费者消费某个 topic 的偏移量。 每个 partitions 都被切割成相同大小的 segment，segment 由四个部分构成，具体如下： log：数据文件 index：索引文件，用来保存消息的索引，能够使查找数据更加高效 timeindex：具体时间日志 leader-epoch-checkpoint：保存了每一任 leader 开始写入消息时的 offset，会定时更新 1.4 生产者分区策略kafka 允许为每条消息定义消息 key，可以根据 key 来为消息选择分区。 分区策略即决定生产者将消息发送到哪个分区的算法，主要有以下几种： 轮询策略（没给定分区号和 key 值）：可以提供优秀的负载均衡能力，保证消息被平均的分配到各个分区上，但不能保证消息的有序性。 随即策略：瞎分，基本不用。 hash 策略（没给定分区号，但给了 key 值）：将 key 的 hash 值与 topic 的 partition 数进行取余得到partition值 自定义分区 1.5 生产者ISRISR 即同步副本，leader 维护了一个动态的 ISR，为 leader 保持同步的 follower 集合，当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 acks，如果 follower 长时间没有向 leader 同步数据，则该 follower 将被踢出 ISR。leader 发生故障之后就会在 ISR 中选取出新的 leader。 1.6 生产者ACKSkafka 发送确认参数 acks 的几种模式： acks = 0：意味着生产者能够通过网络把消息发送出去，broker 接收到数据还没写入就发送，容易丢数据 acks = 1：等待 leader 写入数据（不等待 follower 同步），就可以发送数据，可能会丢数据 acks = all/-1：等待 leader 和 follower（ISR 中的）写入数据，就可以发送数据，数据不容易丢，但效率底 1.7 kafka数据一致性原理 HW：所有副本中的最小 LEO LEO：每个副本的最后一个（最大的） offset 假设分区的副本为3，副本0为 leader，副本1、2为 follower，并且都在 ISR 列表中。虽然 leader 已经写入了 message3，但消费者只能读到 message1，因为所有的 ISR 都同步了 message1，只有在 HW 之上的消息才支持被消费，而 HW 取决于 LEO，原理类似于木桶原理。 这么做的原因是如果 leader 崩溃了，一个 follower 成为新的 leader 后，由于该新的 leader 没有做好数据的同步，如果允许消费者读取 HW 之下的数据的话，那么就会出现数据不一致的问题。 1.7 Exactly-once当生产者发送消息到 topic 时，很可能会出现宕机的情况，或者出现了网络故障，导致生产者消息发送失败，而生产者要如何处理这样的错误，就产生了如下几种不同的语义： At least once：至少一次，如果生产者收到了 broker 发送过来的 acks，且 acks 设置为了 all/-1，这就代表消息已经被写入 topic 且同步好了。如果生产者在时间内没受到 acks 或收到的 acks 有误，那么就会重新发送消息。如果 broker 恰好在消息已经写入 topic 时，发送 acks 前出了故障，那么就会导致生产者发送同样的消息两次，消费者消费两次。 At more once：最多一次，如果生产者在接收 acks 超时或返回有问题的时候不重新发送消息，那么消息很可能没写入 topic 中，因此消费者也不会消费该条消息，不会出现数据重复的现象，但很容易缺数据。 Exactly once：精确一次，即使生产者重新发送消息，也只会被消费者消费一次，实际上就是在 kafka 集群中做一次去重的操作。kafka 集群会给生产者分配一个 PID，生产者每次发送消息到 broker 时都会附带 PID、分区以及序列号，broker 就会对数据做一个保存，如果生产者再发送同样消息，那么 broker 就会对数据进行去重，但如果生产者宕机重启了，就会被分配一个新的 PID，所以去重无法做到精准的数据写入。要开启 exactly-once，需要在 broker 中配置 enable.idempotence = true ，这时候 acks 默认被设置为 -1。 1.8 消费者分区分配策略消费者是采用 pull 的方式在 kafka 集群中获取消息的。 push 模式很难适应消费速率不同的消费者，因为消息的发送速率是由 broker 决定的；而 pull 方式当 kafka 集群没有数据的时候，消费者可能会陷入循环之中，一直取到空数据。 一个消费者组里由多个消费者，一个 topic 里由多个分区，那么数据在消费的时候就会涉及到分配的问题，即确定那些分区由消费者组里的哪些消费者来消费。 kakfa 有三种分配策略，如下： RangeAssignor（默认）：RangeAssignor 是针对 topic 而言的，首先会对 topic 中的分区按照序号进行排列，并对消费者根据字典序排列，然后用分区个数除以消费者线程的总数来决定分区的分配，但如果除不尽，那么前面的消费者就会多分配一些分区。 RoundRobin：将消费组内所有消费者以及消费者所订阅的所有 topic 的 partition 按照字典序排序，然后通过轮询的方式逐个分配给组内的消费者。如果同时消费多个 topic，那么消费者会将这些 topic 视为一个。但如果同一个消费组内的消费者所订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能会导致分区分配的不均匀。 StickyAssignor：StickyAssignor 要实现的是分区的分配要尽可能的均匀，分配给消费者者的主题分区数最多相差一个，假设一个消费者组有三个消费者，都订阅了四个 topic，且每个 topic 中有二个分区，那么这时候分配的结果与 RoundRobin 会很相似，即消费者A三个分区、消费者B三个分区、消费者C两个分区，假设消费者C退出了消费者组，这时候 StickyAssignor 会保留之前消费者A和B分配到的分区，然后再将消费者C之前分配到的分区再分配给消费者A和B，即体现了粘性，不需要消费者将之前处理过的数据送到新的消费者再处理一次。 1.9 offset存储kafka 在0.9版本之前，offset 是存储在 zk 中，在之后的版本则是存储在 kafka 内置的一个 topic 中，即 _consumer_offsets，一个 offset 提交的信息包括以下： Fields Content Key Consumer Group、Topic、Partition Payload Offset、Metadata、Timestamp offset 的提交会根据消费者组的 key（GTP） 进行分区，对于一个给定的消费者，它所有的消息都会发送到唯一的 broker，这样消费者就不需要对多个 broker 拉取数据，但如果消费者组消费很多数量的分区，会对 broker 造成性能瓶颈。 1.10 kafka写机制 顺序写入磁盘 零拷贝 1.11 kafka-controllerkafka 集群中会有一个 broker 被选举为 controller，用来负责管理 broker 的上下线，所有 topic 的分区、副本分配和 leader 选举等。 1.12 kafka事务事务可以保证在 Exactly-Once 的基础上，生产和消费可以跨分区跨会话，即生产者在同一个事务内提交到多个分区的消息，要么同时提交成功，要么同时失败，这样就保证了生产者在运行时出现异常或宕机之后仍然成立。 生产者事务 为了实现跨分区跨会话的事务，需要引入全局唯一的 Transaction ID（由客户端给定），并将生产者的 PID 与之绑定，这样以来生产者即使宕机重启也可以与原来的 PID 进行绑定。 为了管理 Transaction ID，kafka 中引入了一个新的组件 Transaction Coordinator，生产者就是与 Transaction Coordinator 交互来获得 Transaction ID 对应的任务状态。Transaction Coordinator 还负责将事务写入 topic，这样即使服务重启，也可以得到恢复。 消费者事务 事务主要是为生产者作保证，无法保证消费者的精准消费，是因为消费者可以根据 offset 来访问消息。 二、kafka基础2.1 kafka安装通过二进制包部署 下载 kafka 12curl -O https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.8.0/kafka_2.13-2.8.0.tgztar -xf kafka_2.13-2.8.0.tgz 修改 server.properties 1234# broker的id必须为唯一的整数，设置为-1时随机生成broker.id=-1# 修改为实际zookeeper节点地址+2181端口zookeeper.connect=... 开启 zookeeper 和 kafka 123# daemon参数:不输出启动日志信息./bin/zookeeper-server-start.sh -daemon ../config/zookeeper.properties./bin/kafka-server-start.sh -daemon ./config/server.properties 通过 docker-compose 部署 docker-compose.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647version: &quot;3.8&quot;services: zookeeper: container_name: zookeeper image: docker.io/bitnami/zookeeper:3.7 ports: - &quot;12181:2181&quot; volumes: - &quot;./data/zookeeper_data:/zookeeper_data&quot; environment: - ALLOW_ANONYMOUS_LOGIN=yes kafka1: container_name: kafka1 image: docker.io/bitnami/kafka:2.8.0 ports: - &quot;19092:9092&quot; volumes: - &quot;./data/kafka1_data:/data&quot; environment: - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper kafka2: container_name: kafka2 image: docker.io/bitnami/kafka:2.8.0 ports: - &quot;19093:9092&quot; volumes: - &quot;./data/kafka2_data:/data&quot; environment: - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper kafka3: container_name: kafka3 image: docker.io/bitnami/kafka:2.8.0 ports: - &quot;19094:9092&quot; volumes: - &quot;./data/kafka3_data:/data&quot; environment: - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper 进入 zookeeper 内部查看 brokers 是否存在 123zkCli.shls /brokers/ids... kraft 模式部署 kraft/server.properties 1234# 根据节点数改node.id=1# 控制节点controller.quorum.voters=1@master:9093,2@slave1:9093,3@slave2:9093 生成集群ID 1./bin/kafka-storage.sh random-uuid 生成 /tmp/kraft-combined-logs 目录 1./bin/kafka-storage.sh format -t &lt;uuid&gt; -c ./config/kraft/server.properties 各节点启动 kafka 1./bin/kafka-server-start.sh -daemon ./config/kraft/server.properties kraft 模式 docker-compose 部署 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647version: &quot;3.8&quot;services: kafka1: container_name: kafka1 image: docker.io/bitnami/kafka:2.8.0 command: - /bin/bash - -c - | kafka-storage.sh format -t &lt;uuid&gt; -c /opt/bitnami/kafka/config/kraft/server.properties kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties ports: - &quot;19092:9092&quot; volumes: - &quot;./conf/kafka1:/opt/bitnami/kafka/config/kraft&quot; environment: - ALLOW_PLAINTEXT_LISTENER=yes kafka2: container_name: kafka2 image: docker.io/bitnami/kafka:2.8.0 command: - /bin/bash - -c - | kafka-storage.sh format -t &lt;uuid&gt; -c /opt/bitnami/kafka/config/kraft/server.properties kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties ports: - &quot;19093:9092&quot; volumes: - &quot;./conf/kafka2:/opt/bitnami/kafka/config/kraft&quot; environment: - ALLOW_PLAINTEXT_LISTENER=yes kafka3: container_name: kafka3 image: docker.io/bitnami/kafka:2.8.0 command: - /bin/bash - -c - | kafka-storage.sh format -t &lt;uuid&gt; -c /opt/bitnami/kafka/config/kraft/server.properties kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties ports: - &quot;19094:9092&quot; volumes: - &quot;./conf/kafka3:/opt/bitnami/kafka/config/kraft&quot; environment: - ALLOW_PLAINTEXT_LISTENER=yes 2.2 kafka基本使用 列出某个 zookeeper 中的 topic 1./kafka-topics.sh --list --zookeeper zookeeper_ip:2181 创建 topic 123# --replication-factor:副本数，总副本数为(分区数 * 副本数参数)，下面的例子总副本数为4# --partitions:分区数./kafka-topics.sh --create --zookeeper zookeeper_ip:2181 --replication-factor 2 --partitions 2 --topic test 删除 topic 12# delete.topic.enable为true时才会真正删除./kafka-topics.sh --delete --zookeeper zookeeper_ip:2181 --topic test 查看 topic 1./kafka-topics.sh --describe --zookeeper zookeeper_ip:2181 --topic test 启动 producer 12# --broker-list:针对生产者使用，指定集群中的一个或者多个kafka服务器./kafka-console-producer.sh --broker-list kafka_id:9092 --topic test 启动 consumer 123# --bootstrap-server:针对消费者使用，指定集群中的一个或者多个kafka服务器# --from-beginning:查看所有消息数据./kafka-console-consumer.sh --bootstrap-server kafka_id:9092 --topic test --from-beginning 2.3 单播消息如果多个消费者在同一个消费者组，只有一个消费者可以收到同一个订阅的 topic 的消息。 如果 topic 进行了分区，那么一个 partition 只能被消费者组内的一个消费者消费，但消费者可以消费多个不同的 partition，而这些 partition 也可以被不同消费者组的消费者消费。 12# --consumer-property group.id:指定该消费者从属于哪个消费者组./kafka-console-consumer.sh --bootstrap-server kafka_id:9092 --consumer-property group.id=testgroup --topic test 2.4 多播消息如果多个不同消费者组中的消费者消费同一个订阅的 topic 的消息，那么这些消费者都可以消费同样的消息。 2.5 查看消费者组信息 查看指定节点有哪些消费者组 1./kafka-consumer-groups.sh --bootstrap-server kafka_id:9092 --list 查看消费者组详细信息 1234./kafka-consumer-groups.sh --bootstrap-server kafka_id:9092 --describe --group testgroup# CURRENT-OFFSET:上次消费消息偏移量# LOG-END-OFFSET:当前topic最后消息偏移量# LAG:未消费信息的数量 2.6 kafka集群操作 消息发送 1./kafka-console-producer.sh --broker-list kafka_01:9092 kafka_02:9092 kafka_03:9092 --topic test 消息消费 1./kafka-console-consumer.sh --bootstrap-server kafka_01:9092 kafka_02:9092 kafka_03:9092 --from-beginning --topic test 消费者组消费信息 1./kafka-consumer-groups.sh --bootstrap-server kafka_01:9092 kafka_02:9092 kafka_03:9092 --from-beginning --consumer-property group.id=testgroup --topic test 三、kafka优化3.1 如何防止消息丢失 发送方：设置 ack 的值为 1 或 -1/all 消费方：设置 offset 为手动提交 3.2 如何防止消息的重复消费如果 broker 收到了消息并发送了 ack 给生产者，因为网络原因生产者在一定时间内没有收到 ack 就会进行消息的重新发送，这样 broker 就会有两条一样的消息，就可能会造成消费者重复消费。 在消费者端进行非幂等性（多次访问的结果是一样的）消费问题，就可以解决。 方法一：在数据库中创建一个联合主键（id，uuid），这样只有联合主键匹配成功才能写入数据 方法二：使用分布式锁，例如 Redission.lock（uuid） 3.3 如何做到顺序消费顺序消费的使用场景并不多，因为会牺牲较多的性能。 发送方：ack 不能设置为 0（否则可能会丢失消息），关闭重试并使用同步发送（重试可能会导致消息重复，异步发送会导致消息发送顺序不一致），消息发送成功后才会发送下一条，以保证消息的顺序发送 消费方：消息都是发送到一个 partition 中，只能有一个消费者来消费该 partition 的消息 3.4 消息积压当消费者的消费速度远远跟不上生产者的消息生产速度，那么 kafka 中就会有大量的消息没有被消费，消费者寻址的性能也会越来越差，最后导致服务雪崩。 方法一：使用多线程 方法二：创建多个消费者组和多个消费者，一起消费 方法三：创建一个消费者，该消费者新建一个 topic，并划分多个分区，多个分区再划分给多个消费者，这时候该消费者将消息拉取下来但不进行消费，而是放在新的 topic 上让多个消费者进行消费。 3.5 延时队列如果在订单创建成功后 30 分钟内没有付款，则自动取消订单，在这就可以通过延时队列来实现。 方法： kafka 创建相应 topic 消费者轮询消费该 topic 的消息 消费者判断该 topic 的创建时间与当前时间是否超过 30 分钟（前提是订单未支付） 如果是：在数据库中修改订单状态为取消 如果不是：记录当前消息的 offset，并不再继续消费 四、kafka-eagle监控平台 下载 kafka-eagle 1curl -O https://github.com/smartloli/kafka-eagle-bin/archive/v2.0.8.tar.gz 修改 system-config.properties 12345678# zk地址efak.zk.cluster.alias=cluster1cluster1.zk.list=localhost:12181# mysql地址efak.driver=com.mysql.cj.jdbc.Driverefak.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNullefak.username=rootefak.password=toortoor 修改环境变量 1234567vim /etc/profileexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar.:$JAVA_HOME/lib/dt.jar.:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/binexport KE_HOME=/root/kafka/kafka-eagle/efak-web-2.0.8export PATH=$PATH:$KE_HOME/binsource /etc/profile 访问","link":"/2024/02/18/kafka/"},{"title":"linux文件系统","text":"文件系统是一种用于组织和管理计算机存储设备上数据的系统。它将存储设备上的物理空间划分为逻辑结构，并提供对数据的访问和管理机制。 文件系统的基本功能包括： 将数据组织成文件和目录 提供对文件的读写访问 管理存储空间 提供文件安全和保护 常见的文件系统有： ext4：最常用的 Linux 文件系统之一，支持大容量存储、高性能和良好的扩展性 xfs：另一种高性能文件系统，支持大文件和高 I/O 负载 fat32：兼容 Windows 和其他操作系统的文件系统，适用于需要跨平台文件共享的场景 ntfs：Windows 的默认文件系统，支持大容量存储和一些高级功能，例如文件权限和加密 ext4 和 xfs 的区别 xfs 相比于 ext4 有更高的性能，例如在 IO 密集型的负载下 ext4 的最大文件系统大小为 1EB，而 xfs 的最大文件系统大小为 8EB ext4 的最大文件大小为 16TB，而 xfs 的最大文件大小为 16EB ext4 相比于 xfs 有着更高的兼容性，被大多数 linux 发行版都支持","link":"/2024/03/17/linux%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"title":"MySQL","text":"MySQL 是一款关系型数据库管理系统。 一、MySQL基础1.1 安装MySQLdocker-compose 12345678910111213141516version: '3.8'services: db: image: mysql:5.7.35 command: --default-authentication-plugin=mysql_native_password restart: always ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: toortoor adminer: image: adminer:latest restart: always ports: - 8080:8080 1.2 基础SQL语句 刷新权限 1flush privileges; 数据库基本操作 123456-- 创建create database [if not exists] `db_name`;-- 删除drop database [if exists] `db_name`;-- 使用use `db_name`; 表基本操作 12345678910111213141516171819202122232425-- 创建create table [if not exists] `table_name`;-- 删除drop table [if exists] `table_name`;-- 查看describe `table_name`;-- 创建一个学生表create table if not exists `students`( -- 创建id列，数据类型为4位的int类型，不允许为空，自增 `id` int(4) not null auto_increment comment '学号', -- 创建name列，数据类型为30位的varchar类型，不允许为空，默认值为匿名 `name` varchar(30) not null default '匿名' comment '姓名', `pwd` varchar(20) not null default '123456' comment '密码', `gender` varchar(2) not null default '女' comment '性别', -- 创建birthday列，datetime类型，默认为空 `brithday` datetime default null comment '出生日期', `address` varchar(100) default null comment '家庭住址', `email` varchar(50) default null comment '邮箱', -- 设置主键，一般一个表只有一个主键 primary key(`id`))-- 设置引擎engine=innodb-- 默认编码default charset=utf8; 修改表 12345678910-- 修改表名称alter table `table_name` rename as new_`table_name`;-- 增加字段alter table `table_name` add age int(10);-- 修改约束alter table `table_name` modify age varchar(10);-- 重命名字段alter table `table_name` change age new_age int(10);-- 删除字段alter table `table_name` drop age; 查看创建语句 12show create database `db_name`;show create table `table_name`; 1.3 引擎INNODB和MYISAM区别 特性 INNODB MYISAM 事务 支持 不支持 数据行锁定 支持 不支持 外键 支持 不支持 全文索引 不支持 支持 空间占用 约为MYSIAM的2倍 较小 不同引擎在文件上的区别 innodb： *.frm：表结构定义文件 ibdata1：数据文件 mysiam： *.frm：表结构定义文件 *.MYD：数据文件 *.MYI：索引文件 二、MySQL数据操作2.1 外键外键就是一个表的某一列去引用另一个表的某一列 12345678910111213141516-- 该示例为students中的grade_id列引用grade中的grade_id列create table `grade`( `grade_id` int(10) not null auto_increment comment '年级' primary key(`grade_id`))engine=innodb default charset=utf8create table `students`( `id` int(4) not null auto_increment comment '学号', `name` varchar(30) not null default '匿名' comment '姓名', `grade_id` int(10) not null comment '年级', primary key(`id`), -- 定义外键 key `FK_grade_id` (`grade_id`), -- 给外键添加约束 constraint `FK_grade_id` foreign key (`grade_id`) references `grade` (`grade_id`))engine=innodb default charset=utf8 如果要给已经存在的表添加外键 1alter table `table_name` add constraint `FK_name` foreign key (`列名称`) references `引用的表名` (`引用的列名称`) 删除外键 1alter table `table_name` drop foreign key 'FK_name' 2.2 DML数据操纵语言2.2.1 insert1234insert into `table_name`(`字段1`,`字段2`,`字段3`) values('值1'),('值2'),('值3');insert into `students`(`name`) values('cqm'),('lwt');-- 字段可以省略，但后面的值必须一一对应insert into `students`(`name`,`pwd`,`email`) values('lwt','111','lwt@qq.com'); 2.2.2 update1234567-- 如果不指定条件，那么会修改所有的值update `table_name` set `字段`='值' where 条件;-- 条件可以是 =|&lt;|&gt;|!=|between|and|or 等等update `students` set `name`='handsome_cqm' where `id`=1;update `students` set `name`='cqm' where `name`='handsome_cqm'update `students` set `name`='newlwt',`email`='new@qq.com' where `name`='lwt';update `students` set `name`='cqm' where id between 1 and 2; 2.2.3 delete1234delete from `table_name` where 条件;delete from `students` where `id`=1;-- 清空表truncate `table_name`; delete 和 truncate 区别： 都可以清空表，都不会删除表结构 truncate 可以重置自增列，且不会影响事务 delete 清空表后，如果引擎是 innodb，重启数据库自增列就会变回1，因为是存储在内存中的；如果引擎是 myisam，则不会，因为是存储在文件中的 2.3 DQL数据库查询语言2.3.1 select 查询所有数据 1select * from `table_name`; 查询某列数据 1select `name`,`gander` from `students`; 给字段结果起别名 1select `name` as '姓名',`gender` as '性别' from `students`; concat 函数 1select concat('姓名：',`name`) from `students`; 去重 1select distinct `字段` from `table_name` 批量操作数据 1select `score`+1 as `new_score` from `table_name`; 2.3.2 where 逻辑运算符运用 123select `score` from `table_name` where `score` &gt;= 95 and `score` &lt;= 100;select `score` from `table_name` where `score` between 95 and 100;select `score` from `table_name` where not `score` != 95 and `score` != 100; 模糊查询 运算符 语法 描述 is null a is null a 为空，结果为真 is not null a is not null a 不为空，结果为真 between a between b and c 若 a 在 b 和 c 之间，结果为真 like a like b 如果 a 匹配 b，结果为真 in a in ( b,c,d,e ) 如果 a 在某个集合中的值相同，结果为真 查找花名册中姓陈的名字 1234-- %:匹配一个或多个字符-- _:匹配一个字符select `name` from `table_name` where `name` like '陈%';select `name` from `table_name` where `name` like '陈_明'; 查找特定学号的信息 1select `name` from `table_name` where `id` in (1,2,3); 查找地址为空或不空的同学 12select `name` from `table_name` where `address` is null;select `name` from `table_name` where `address` is not null; 2.3.3 联表查询联表查询包括： inner（内连接）：如果表中有至少一个匹配，则返回行 left（外连接）：即使右表中没有匹配，也从左表返回所有的行 right（外连接）：即使左表中没有匹配，也从右表返回所有的行 full（外连接）：只要其中一个表中存在匹配，则返回行 inner 实际就是取两个表的交集，例如有两个表，一个表有学生的基本信息，另一个表有学生的成绩，两个表都有学生的学号列，那么就可以联合起来查询 1select `name`,`subjectno`,`score` from `students` inner join `result` where student.id = result.id left 假设学生表中有 ccc 这么一个学生，但成绩表里没有 ccc 学生的成绩，如果使用了左连接，左表是学生表，右表是成绩表，那么也会返回 cqm 学生的值，显示为空 right 相反，如果成绩表里有个 ddd 学生的成绩，但学生表里没有这个学生的信息，如果使用了右连接，左表是学生表，右表是成绩表，那么也会返回 ddd 学生的成绩，注意这时候就看不到 ccc 学生的成绩信息了，因为左表中没有 ccc 学生的成绩信息 通过联表查询就可以查出缺考的同学 查询参加考试了的同学信息 1select distinct `name` from `students` right join `result` on students.id = result.id where `score` is not null; 查询参加了考试的同学以及所对应的学科成绩 2.3.4 where和on的区别在进行联表查询的时候，数据库都会在中间生成一张临时表，在使用外连接时，on 和 where 区别如下： on 条件是在生成临时表时使用的条件，它不管 on 中的条件是否为真，都会返回左边表中的记录。 where 条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有 left join 的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。 如果是 inner join，则无区别 2.3.5 自连接自连接实际上就是一张表与自己连接，将一张表拆为两张表 原表 categoryid pid categoryname 2 1 计算机科学与技术学院 3 1 心理学院 4 1 体育学院 5 2 python 6 3 心理学 7 4 短跑 8 4 篮球 父类表 category categoryname 2 计算机科学与技术学院 3 心理学院 4 体育学院 子类表 category 所属父类 categoryname 5 计算机科学与技术学院 python 6 心理学院 心理学 7 体育学院 短跑 8 体育学院 篮球 自查询结果 2.3.6 分页和排序排序 order by desc：降序 asc：升序 12-- 语法order by desc|asc 查询语文成绩并排序 分页 limit 12-- 语法limit 起始值,页面显示多少条数据 打印第一页语文成绩 打印第二页数学成绩 分页和排序配合起来就可以查询学生的前几名成绩，例如语文成绩前三名 2.3.7 子查询本质上就是在判断语句里嵌套一个查询语句，例如要查询所有语文成绩并降序排序，可以通过联表查询和子查询两种方式实现 2.4 聚合函数2.4.1 countcount 函数用于计数，例如查询有多少学生 1select count(`name`) from students; 函数内所带的参数不同，背后运行也不同： count(字段)：会忽略空值，不计数 count(*)：不会忽略空值 count(1)：不会忽略空值 从效率上看：如果查询的列为主键，那么 count(字段) 比 count(1) 快，不为主键则反之；如果表中只有一列，则 count(*) 最快 2.4.2 sum顾名思义，用于求和，例如查询所有成绩之和 1select sum(`score`) as '总分' from result 2.4.3 avg1select avg(`score`) as '平均分' from result 2.4.4 max和min1select max(`score`) as '最高分' from result 1select min(`score`) as '最低分' from result 查询所有科目的平均分、最高分和最低分 123456select subjectname, avg(studentresult), max(studentresult), min(studentresult)from `result`inner join `subject`on `result`.subjectno = `subject`.subjectno-- 定义字段进行分组group by result.subjectno; 通过分组后的次要条件，查询平均分大于 80 分的科目 1234567select subjectname, avg(studentresult) as 平均分from `result`inner join `subject`on `result`.subjectno = `subject`.subjectnogroup by `result`.subjectno-- 分组后的次要条件having 平均分 &gt; 80; 2.5 MD5加密在数据库中，密码等敏感信息都不会以明文的形式存储的，可以通过md5 进行加密 12-- 更新密码以达成加密update students set pwd=md5(pwd); 12-- 插入的时候就进行加密insert into students values(1, 'cqm', md5('12345')) 2.6 事务事务就是一系列 SQL 语句，要么全部执行，要么全部不执行。 事务的特性（ACID）： 原子性（Atomicity）：所有操作要么全部成功，要么全部失败 一致性（Consistency）：事务的执行的前后数据的完整性保持一致 隔离性（Isolation）：一个事务执行的过程中，不受到别的事务的干扰 持久性（Durability）：事务一旦结束，就会持久到数据库 隔离所导致的一些问题： 脏读：一个事务读取到了另一个事务没提交的数据 不可重复读：一个事务的多次查询结果不同，是因为查询期间数据被另一个事务提交而修改了 虚读：一个事务A在进行修改数据的操作时，另一个事务B插入了新的一行数据，而对于事务A来看事务B添加的那行就没有做修改，就发生了虚读 事务关闭自动提交 1set autocommit = 0; 事务开启 1start transaction; 事务提交 1commit; 事务回滚 1rollback; 事务结束 1set autocommit = 1; 保存点 123456-- 添加保存点savepoint 保存点名称-- 回滚到保存点rollback to savepoint 保存点名称-- 删除保存点release savepoint 保存点名称 2.7 索引索引是帮助 MySQL 高效获取数据的数据结构。 索引的分类： 主键索引（primary key）：只有一列可以作为主键，且主键的值不可重复，一般用于用户ID之类的 唯一索引（unique key）：唯一索引可以有多个，且值唯一 常规索引（key）：例如一个表中的数据经常用到，就可以添加个常规索引 全文索引（fulltext）：快速定位数据 查看某个表的所有索引 1show index from `table_name`; 添加全文索引 1alter table `table_name` add fulltext index `index_name`(`要添加索引的字段名`); 添加常规索引 12-- 这样会给表中某个字段的数据全部都添加上索引，在数据量大的时候可以提高查询效率create index `id_table_name_字段名` on `table_name`('字段名'); 索引使用原则： 并不是索引越多越好 不要对进程变动的数据添加索引 数据量小的表不需要索引 索引一般用于常查询的字段上 2.8 权限管理在 MySQL 中，用户表为 mysql.user，而权限管理其实都是在该表上操作。 创建用户 123456-- host可以为以下的值-- %:允许所有ip连接-- localhost:只允许本地连接-- 192.168.88.%:只允许给网段连接-- 192.168.88.10:只允许该ip连接create user 'user_name'@'host' identified by 'user_password'; 修改当前用户密码 1set password = password('new_password'); 修改指定用户密码 1set password for user_name = password('new_password'); 重命名 1rename user user_name to new_user_name; 授权 12-- 权限:select、insert、delete等等，所有权限则为allgrant 权限 on `db_name`.`table_name` to 'user_name'@'host'; 授予某个用户部分权限 1grant select,insert on mysql.user to 'cqm'@'%'; 给某个用户授予全部数据库的权限 12-- 基本权限都有，但不会给grant权限grant all privileges on *.* to 'cqm'@'%'; 删除用户 1drop user 'user_name'@'host'; 取消权限 1revoke 权限 on `db_name`.`table_name` from 'user_name'@'host'; 查询用户权限 1show grants for 'user_name'@'host'; 2.9 备份备份文件都是以 .sql 为后缀的文件。 导出 12345# -d:要操作的数据库# -h:指定主机# -P:指定端口# --all-databases:操作所有数据库mysqldump -uroot -ppassword -d db1_name db2_name &gt; db_backup.sql 导入 1mysqldump -uroot -ppassword -d db_name &lt; db_backup.sql 三、MySQL配置3.1 主从同步/复制MySQL 主从同步即每当主数据库进行了数据的操作后，就会将操作写入 binlog 文件，从数据库会启动一个 IO 线程去监控主数据库的 binlog 文件，并将 binlog 文件的内容写入自己的 relaylog 文件中，同时会启动一个 SQL 线程去监控 relaylog 文件，如果发生变化就更新数据。 主从复制的类型： statement模式（sbr）：只有修改数据的 SQL 语句会记录到 binlog 中，优点是减少了日志量，节省 IO，提高性能，不足是可能会导致主从节点之间的数据有差异。 row模式（rbr）：仅记录被修改的数据，不怕无法正确复制的问题，但会产生大量的 binlog。 mixed模式（mbr）：sbr 和 rbr 的混合模式，一般复制用 sbr，sbr 无法复制的用 rbr。 主从同步实现 docker-compose.yaml 1234567891011121314151617181920212223version: '3.8'services: mysql_master: image: mysql:5.7.35 command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: toortoor ports: - 3306:3306 volumes: - ./master/my.cnf:/etc/mysql/my.cnf mysql_slave: image: mysql:5.7.35 command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: toortoor ports: - 3307:3306 volumes: - ./slave/my.cnf:/etc/mysql/my.cnf master/my.cnf 12345678910111213[mysqld]server-id = 1 #节点ID，确保唯一log-bin = mysql-bin #开启mysql的binlog日志功能sync_binlog = 1 #控制数据库的binlog刷到磁盘上去，0不控制，性能最好；1每次事物提交都会刷到日志文件中，性能最差，最安全binlog_format = mixed #binlog日志格式，mysql默认采用statement，建议使用mixedexpire_logs_days = 7 #binlog过期清理时间max_binlog_size = 100m #binlog每个日志文件大小binlog_cache_size = 4m #binlog缓存大小max_binlog_cache_size= 512m #最大binlog缓存大binlog-ignore-db=mysql #不生成日志文件的数据库，多个忽略数据库可以用逗号拼接，或者复制这句话，写多行auto-increment-offset = 1 #自增值的偏移量auto-increment-increment = 1 #自增值的自增量slave-skip-errors = all #跳过从库错误 slave/my.cnf 1234567[mysqld]server-id = 2log-bin=mysql-binrelay-log = mysql-relay-binreplicate-wild-ignore-table=mysql.%replicate-wild-ignore-table=test.%replicate-wild-ignore-table=information_schema.% 在 master 创建复制用户并授权 123create user 'repl_user'@'%' identified by 'toortoor';grant replication slave on *.* to 'repl_user'@'%' identified by 'toortoor';flush privileges; 查看 master 状态 123456show master status;+------------------+----------+...| File | Position |...+------------------+----------+...| mysql-bin.000003 | 844 |...+------------------+----------+... 在从数据库配置 123456789change master toMASTER_HOST = '172.19.0.3',MASTER_USER = 'repl_user', MASTER_PASSWORD = 'toortoor',MASTER_PORT = 3306,MASTER_LOG_FILE='mysql-bin.000003',MASTER_LOG_POS=844,MASTER_RETRY_COUNT = 60,MASTER_HEARTBEAT_PERIOD = 10000; 启动从配置 12start slave;show slave status\\G; 如果配置失败，可以执行 12stop slave;set global sql_slave_skip_counter=1; 3.2 Mycat读写分离在一般项目中，对于数据库的操作读要远大于写，而如果所有的操作都放在一个节点上，那么就很容易出现压力过大而宕机，读写分离就可以很好解决该问题，主要通过 mycat 的中间件来实现。 分库分表类型： 水平拆分：将不同等级的会员信息写到不同的表中 垂直拆分：将买家信息、卖家信息、商品信息、支付信息等不同信息写到不同的表中 Mycat的主要文件： 文件 说明 server.xml 设置 Mycat 账号、参数等 schema.xml 设置 Mycat 对应的物理数据库和表等 rule.xml 分库分表设置 server.xml 123456789101112131415161718192021222324252627282930&lt;!-- Mycat用户名 --&gt;&lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;!-- 密码 --&gt; &lt;property name=&quot;password&quot;&gt;123456&lt;/property&gt; &lt;!-- 逻辑库名 --&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;!-- 默认逻辑库 --&gt; &lt;property name=&quot;defaultSchema&quot;&gt;TESTDB&lt;/property&gt; &lt;!--No MyCAT Database selected 错误前会尝试使用该schema作为schema，不设置则为null,报错 --&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- 0为禁止，1为开启 --&gt; &lt;!-- 按顺序分别为insert、update、select、delete --&gt; &lt;!-- &lt;privileges check=&quot;false&quot;&gt; &lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt; &lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt; &lt;/user&gt; &lt;!-- 其他用户设置 --&gt; &lt;user name=&quot;user&quot;&gt; &lt;property name=&quot;password&quot;&gt;user&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt; &lt;property name=&quot;defaultSchema&quot;&gt;TESTDB&lt;/property&gt; &lt;/user&gt; schema.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;!-- name为逻辑库名称，与server.xml文件对应 checkSQLschema为true，sql为select * from table_name checkSQLschema为false，sql为select * from TESTDB.table_name sqlMaxLimit是指如果sql中没有limit，则自动添加，如果有则不添加 --&gt; &lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot; randomDataNode=&quot;dn1&quot;&gt; &lt;!-- name为逻辑表名 dataNode为数据节点名称 rule为规则 --&gt; &lt;table name=&quot;customer&quot; primaryKey=&quot;id&quot; dataNode=&quot;dn1,dn2&quot; rule=&quot;sharding-by-intfile&quot; autoIncrement=&quot;true&quot; fetchStoreNodeByJdbc=&quot;true&quot;&gt; &lt;childTable name=&quot;customer_addr&quot; primaryKey=&quot;id&quot; joinKey=&quot;customer_id&quot; parentKey=&quot;id&quot;&gt; &lt;/childTable&gt; &lt;/table&gt; &lt;/schema&gt; &lt;!-- name为数据节点名称 dataHost为数据库地址 database为mysql中的database 实际就是将TESTDB逻辑库拆成dn1和dn2，而dn1和dn2又对应db1和db2 --&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;db1&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;localhost1&quot; database=&quot;db2&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;localhost1&quot; database=&quot;db3&quot; /&gt; &lt;!-- balance: 0:不开启读写分离，所有操作都在writeHost上 1:所有读操作都随机发送到当前的writeHost对应的readHost和备用的writeHost 2:所有读操作都随机发送到所有的主机上 3:所有读操作只发送到readHost上 writeType: 0:所有写操作都在第一台writeHost上，第一台挂了再切到第二台 1:所有写操作都随机分配到writeHost switchType: 用于是否允许writeHost和readHost之间自动切换 -1:不允许 1:允许 2:基于mysql的主从同步的状态决定是否切换 --&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;!-- 用此命令来进行心跳检测 --&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- 设置读写分离 --&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;jdbc:mysql://localhost:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;hostS1&quot; url=&quot;jdbc:mysql://localhost:3306&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/readHost&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; rule.xml 12345678910111213&lt;!-- 平均分算法 --&gt;&lt;tableRule name=&quot;mod-long&quot;&gt; &lt;rule&gt; &lt;!-- 根据id值平均分 --&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;mod-long&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt;...&lt;function name=&quot;mod-long&quot; class=&quot;io.mycat.route.function.PartitionByMod&quot;&gt; &lt;!-- 切片个数 --&gt; &lt;property name=&quot;count&quot;&gt;2&lt;/property&gt;&lt;/function&gt; 在 master 节点创建数据库 123-- 与schema.xml中的database参数相同create database db1;create database db2; 在每个库里创建表 12345create table students( id int(4), name varchar(10), primary key(`id`))engine=innodb default charset=utf8; 开启 Mycat，默认开启8066服务端端口和9066管理端端口 1./mycat start 在有安装 mysql 的主机登录 Mycat，也可以通过 navicat 连接 1mysql -uroot -ptoortoor -h 192.168.88.136 -P 8066 只要在 Mycat 进行 SQL 操作，都会流到 mysql 集群中被处理，也可以看到已经实现了分库分表 3.3 使用haproxy实现Mycat高可用haproxy 可以实现 Mycat 集群的高可用和负载均衡，而 haproxy 的高可用通过 keepalived 来实现。 安装 haproxy 1yum -y install haproxy 修改日志文件 12345vim /etc/rsyslog.conf# Provides UDP syslog reception$ModLoad imudp$UDPServerRun 514systemctl restart rsyslog 配置 haproxy 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263vim /etc/haproxy/haproxy.cfg# haproxy的配置文件由两个部分构成，全局设定和代理设定# 分为五段:global、defaults、frontend、backend、listenglobal # 定义全局的syslog服务器 log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy # 设置haproxy后台守护进程形式运行 daemon stats socket /var/lib/haproxy/statsdefaults # 处理模式 # http:七层 # tcp:四层 # health:状态检查,只会返回OK mode tcp # 继承global中log的定义 log global option tcplog option dontlognull option http-server-close # option forwardfor except 127.0.0.0/8 # serverId对应的服务器挂掉后,强制定向到其他健康的服务器 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000frontend mycat # 开启本地监控端口 bind 0.0.0.0:8066 bind 0.0.0.0:9066 mode tcp log global default_backend mycatbackend mycat balance roundrobin # 监控的Mycat server mycat1 192.168.88.135:8066 check inter 5s rise 2 fall 3 server mycat2 192.168.88.135:8066 check inter 5s rise 2 fall 3 server mycatadmin1 192.168.88.136:9066 check inter 5s rise 2 fall 3 server mycatadmin2 192.168.88.136:9066 check inter 5s rise 2 fall 3 listen stats mode http # 访问haproxy的端口 bind 0.0.0.0:9999 stats enable stats hide-version # url路径 stats uri /haproxy stats realm Haproxy\\ Statistics # 用户名/密码 stats auth admin:admin stats admin if TRUE 访问 haproxy 3.4 使用keepalived实现去中心化 安装 keepalived 1yum -y install keepalived 配置 Master 节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs { # 识别节点的id router_id haproxy01}vrrp_instance VI_1 { # 设置角色，由于抢占容易出现 VIP 切换而闪断带来的风险，所以该配置为不抢占模式 state BACKUP # VIP所绑定的网卡 interface ens33 # 虚拟路由ID号，两个节点必须一样 virtual_router_id 30 # 权重 priority 100 # 开启不抢占 # nopreempt # 组播信息发送间隔，两个节点必须一样 advert_int 1 # 设置验证信息 authentication { auth_type PASS auth_pass 1111 } # VIP地址池，可以多个 virtual_ipaddress { 192.168.88.200 } # 将 track_script 块加入 instance 配置块 track_script{ chk_haproxy }}# 定义监控脚本vrrp_script chk_haproxy { script &quot;/etc/keepalived/haproxy_check.sh&quot; # 时间间隔 interval 2 # 条件成立权重+2 weight 2} 配置 Slave 节点 12345678910111213141516171819202122232425262728293031vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs { router_id haproxy02}vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 30 priority 80 # nopreempt advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.88.200 } track_script{ chk_haproxy }}vrrp_script chk_haproxy { script &quot;/etc/keepalived/haproxy_check.sh&quot; interval 2 weight 2} 编写监控脚本 1234567891011121314151617181920212223242526vim /etc/keepalived/haproxy_check.sh#!/bin/bashSTART_HAPROXY=&quot;systemctl start haproxy&quot;STOP_KEEPALIVED=&quot;systemctl stop keepalived&quot;LOG_DIR=&quot;/etc/keepalived/haproxy_check.log&quot;HAPS=`ps -C haproxy --no-header | wc -l`date &quot;+%F %H:%M:%S&quot; &gt; $LOG_DIRecho &quot;Check haproxy status&quot; &gt;&gt; $LOG_DIRif [ $HAPS -eq 0 ];then echo &quot;Haproxy is down&quot; &gt;&gt; $LOG_DIR echo &quot;Try to turn on Haproxy...&quot; &gt;&gt; $LOG_DIR echo $START_HAPROXY | sh sleep 3 if [ `ps -C haproxy --no-header | wc -l` -eq 0 ]; then echo -e &quot;Start Haproxy failed, killall keepalived\\n&quot; &gt;&gt; $LOG_DIR echo $STOP_KEEPALIVED | sh else echo -e &quot;Start Haproxy successed\\n&quot; &gt;&gt; $LOG_DIR fielse echo -e &quot;Haproxy is running\\n&quot; &gt;&gt; $LOG_DIRfi 启动 keepalived 后可以看到 VIP 被哪台服务器抢占了，通过该 VIP 就可以访问到对应的 haproxy，haproxy 就会将流量流到后面的 Mycat，再由 Mycat 来实现分表分库；haproxy 停止后 keepalived 也会通过脚本尝试去重新开启，如果开启不成功就会停止 keepalived，VIP 就由 slave 节点抢占，用户依旧可以通过 VIP 来操控数据库，且无感知。 通过 VIP 去连接 Mycat 插入数据，尝试能否实现分库分表 可以看到插入的数据都分到了 db1、db2 中 3.5 Sharding JDBC读写分离Apache ShardingSphere 是一套开源的分布式数据库解决方案组成的生态圈，它由 JDBC、Proxy 和 Sidecar（规划中）这 3 款既能够独立部署，又支持混合部署配合使用的产品组成。 Sharding JDBC 同样也可以实现分库分表、数据分片、读写分离等功能，但与 Mycat 不同的是，Mycat 是一个独立的程序，而 Sharding JDBC 是以 jar 包的形式与应用程序融合在一起运行的。","link":"/2024/02/18/mysql/"},{"title":"linux基础网络服务","text":"一、DHCP服务1.1 DHCP服务介绍DHCP服务即动态主机配置协议，被运用在局域网中，主要的作用是分配IP地址。 DHCP服务采用的是UDP协议，发送采用UDP67端口，接受则采用UDP68端口。 1.2 DHCP服务部署 安装DHCP 1yum -y install dhcp DHCP配置文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150cp /usr/share/doc/dhcp*/dhcpd.conf.example /etc/dhcp/dhcpd.confcat /etc/dhcp/dhcpd.conf# DHCP服务配置文件分为全局配置和作用域配置，很好区分：subnet的就是作用域 不在subnet里面的就是全局设置。# dhcpd.conf## Sample configuration file for ISC dhcpd## DNS全局选项，指定DNS服务器的地址，可以是IP，也可以是域名。# option definitions common to all supported networks...# DNS的域名option domain-name &quot;example.org&quot;;# 具体的DNS服务器option domain-name-servers ns1.example.org, ns2.example.org;# 租约设置，默认租约为600sdefault-lease-time 600;# 租约设置，最大租约为7200s，当客户端未请求明确的租约时间。max-lease-time 7200;# 动态DNS更新方式(none:不支持；interim:互动更新模式；ad-hoc:特殊更新模式)# Use this to enble / disable dynamic dns updates globally.# ddns-update-style none;# 如果该DHCP服务器是本地官方DHCP就将此选项打开，避免其他DHCP服务器的干扰。# 当一个客户端试图获得一个不是该DHCP服务器分配的IP信息，DHCP将发送一个拒绝消息，而不会等待请求超时。# 当请求被拒绝，客户端会重新向当前DHCP发送IP请求获得新地址。# 保证IP是自己发出去的## If this DHCP server is the official DHCP server for the local# network, the authoritative directive should be uncommented.# 开启此项表权威DHCP# authoritative;# Use this to send dhcp log messages to a different log file (you also# have to hack syslog.conf to complete the redirection).# 日志级别log-facility local7;# No service will be given on this subnet, but declaring it helps the # DHCP server to understand the network topology.#作用域相关设置指令# subnet 定义一个作用域# netmask 定义作用域的掩码# range 允许发放的IP范围# option routers 指定网关地址# option domain-name-servers 指定DNS服务器地址# option broadcast-address 广播地址### 案例:定义一个作用域 网段为10.152.187.0 掩码为255.255.255.0# 此作用域不提供任何服务subnet 10.152.187.0 netmask 255.255.255.0 {}# This is a very basic subnet declaration.# 案例:定义一个基本的作用域# 网段10.254.239.0 掩码255.255.255.224# 分发范围10.254.239.10-20# 网关为rtr-239-0-1.example.org, rtr-239-0-2.example.orgsubnet 10.254.239.0 netmask 255.255.255.224 { range 10.254.239.10 10.254.239.20; option routers rtr-239-0-1.example.org, rtr-239-0-2.example.org;}# This declaration allows BOOTP clients to get dynamic addresses,# which we don't really recommend.# 案例:允许采用bootp协议的客户端动态获得地址# bootp DHCP的前身# BOOTP用于无盘工作站的局域网中，可以让无盘工作站从一个中心服务器上获得IP地址。通过BOOTP协议可以为局域网中的无盘工作站分配动态IP地址，# 这样就不需要管理员去为每个用户去设置静态IP地址。subnet 10.254.239.32 netmask 255.255.255.224 { range dynamic-bootp 10.254.239.40 10.254.239.60; option broadcast-address 10.254.239.31; option routers rtr-239-32-1.example.org;}# 案例:一个简单的作用域案例# A slightly different configuration for an internal subnet.subnet 10.5.5.0 netmask 255.255.255.224 { range 10.5.5.26 10.5.5.30; option domain-name-servers ns1.internal.example.org; option domain-name &quot;internal.example.org&quot;; option routers 10.5.5.1; option broadcast-address 10.5.5.31; default-lease-time 600; max-lease-time 7200;}# Hosts which require special configuration options can be listed in# host statements. If no address is specified, the address will be# allocated dynamically (if possible), but the host-specific information# will still come from the host declaration.## 保留地址:可以将指定的IP分发给指定的机器，根据网卡的MAC地址来做触发# host: 启用保留。# hardware:指定客户端的mac地址# filename:指定文件名# server-name:指定下一跳服务器地址# fixed-address: 指定保留IP地址### 案例:这个案例中分发给客户端的不是IP地址信息，而是告诉客户端去找toccata.fugue.com服务器，并且下载vmunix.passacaglia文件host passacaglia { hardware ethernet 0:0:c0:5d:bd:95; filename &quot;vmunix.passacaglia&quot;; server-name &quot;toccata.fugue.com&quot;;}# Fixed IP addresses can also be specified for hosts. These addresses# should not also be listed as being available for dynamic assignment.# Hosts for which fixed IP addresses have been specified can boot using# BOOTP or DHCP. Hosts for which no fixed address is specified can only# be booted with DHCP, unless there is an address range on the subnet# to which a BOOTP client is connected which has the dynamic-bootp flag# set.# 案例:保留地址，将指定IP(fantasia.fugue.com对应的IP)分给指定客户端网卡(MAC:08:00:07:26:c0:a5)host fantasia { hardware ethernet 08:00:07:26:c0:a5; fixed-address fantasia.fugue.com;}# 超级作用域# 超级作用域是DHCP服务中的一种管理功能，使用超级作用域，可以将多个作用域组合为单个管理实体。# You can declare a class of clients and then do address allocation# based on that. The example below shows a case where all clients# in a certain class get addresses on the 10.17.224/24 subnet, and all# other clients get addresses on the 10.0.29/24 subnet.# 在局域网中，可以配置策略根据各个机器的具体信息分配IP地址和其他的网络参数，客户机的具体信息：客户机能够给dhcp服务提供的信息由两个，# 第一个就是网卡的dhcp-client-identifier（mac地址），# 第二个就是设备的vendor-class-identifier。# 管理员可以根据这两个信息给不同的机器分组。# 案例:# 按client某种类型分组DHCP,而不是按物理接口网段# 例子: SUNW 分配地址段10.17.224.0/24# 非SUNW的主机,分配地址段10.0.29.0/24# 定义一个dhcp类:foo# request广播中vendor-class-identifier字段对应的值前四个字节如果是&quot;SUNW&quot;,则视合法客户端.class &quot;foo&quot; { match if substring (option vendor-class-identifier, 0, 4) = &quot;SUNW&quot;;}# 定义一个超级作用域: 224-29shared-network 224-29 {# 定义第一个作用域 subnet 10.17.224.0 netmask 255.255.255.0 { option routers rtr-224.example.org; }# 定义第二个作用域 subnet 10.0.29.0 netmask 255.255.255.0 { option routers rtr-29.example.org; }# 关连池,如果客户端匹配foo类，将获得该池地址 pool { allow members of &quot;foo&quot;; range 10.17.224.10 10.17.224.250; }# 关连池,如果客户端配置foo类，则拒绝获得该段地址 pool { deny members of &quot;foo&quot;; range 10.0.29.10 10.0.29.230; }} 1.3 配置作用域 配置作用域 12345678subnet 192.168.88.0 netmask 255.255.255.0 { range 192.168.88.150 192.168.88.160; # 发放地址范围 option routers 192.168.88.0; # 网关 option broadcast-address 192.168.88.255; # 广播地址 option domain-name-servers 8.8.8.8, 114.114.114.114; # 设置DNS default-lease-time 7200; # 默认租约2小时 max-lease-time 10800; # 最大租约3小时} 将另一台主机网卡设置为DHCP模式 12# 重启网络服务systemctl restart network 用dhclient命令进行测试 1234# 释放IPdhclient -r ens33# 获取IPdhclient -d ens33 查看是否获取了150-160网段内的地址 1.4 保留地址当租约到期的时候，client端只能乖乖交出IP地址，下一次获取就未必是同样的地址了，但公司中往往有些机器要用固定的地址，例如打印机、文件服务器等等，所以在DHCP中可以设置保留地址为其使用。 DHCP是根据主机网卡的MAC地址来做匹配，将保留的IP地址分给相应的主机网卡MAC地址。 获取网卡MAC地址 添加保留地址配置 12345vim /etc/dhcp/dhcpd.confhost fantasia { hardware ethernet 00:0c:29:6c:6f:0d; fixed-address 192.168.88.155;} 查看是否获取相应地址 12dhclient -r ens37dhclient -d ens37 1.5 超级作用域超级作用域简单来说就是将两个或两个以上的不同网段的作用域合成一个作用域。 添加超级作用域 12345678910111213141516171819202122# 添加作用域之前DHCP必须拥有两个网段的网卡shared-network supernet { subnet 192.168.88.0 netmask 255.255.255.0 { range 192.168.88.150 192.168.88.160; option routers 192.168.88.2; option broadcast-address 192.168.88.255; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 7200; max-lease-time 10800; } subnet 192.168.99.0 netmask 255.255.255.0 { range 192.168.99.150 192.168.99.160; option routers 192.168.99.0; option broadcast-address 192.168.99.255; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 7200; max-lease-time 10800; }} 二、DNS服务2.1 DNS服务介绍DNS即域名系统，在互联网中为域名和IP地址进行相互映射的一个分布式数据库。 DNS采用UDP协议，使用UDP53端口进行传输。 DNS记录类型： A：ipv4 记录，将域名映射到 ipv4 地址 AAAA：ipv6 记录，将域名映射到 ipv6 地址 CNAME：别名记录，将域名映射到另一个域名 MX：电邮交互记录，将域名映射到邮件服务器地址 TXT：文本记录，是任意可读的文本 DNS 记录 SRV：服务器资源记录，用来标识某个服务器使用了某个服务，创建于微软系统的目录管理 NS：名称服务器记录，支持将子域名委托给其他 DNS 服务商解析 CAA：CAA 资源记录，可以限定域名颁发证书和 CA 之间的关系 2.2 DNS服务部署 安装DNS 12yum -y install bind bind-chroot# bind-chroot是bind的一个功能,使bind可以在一个chroot的模式下运行.也就是说,bind运行时的/(根)目录,并不是系统真正的/(根)目录,只是系统中的一个子目录而已.这样做的目的是为了提高安全性.因为在chroot的模式下,bind可以访问的范围仅限于这个子目录的范围里,无法进一步提升,进入到系统的其他目录中。bind的默认启动方式就是chroot方式。 将配置文件和区域数据库文件拷贝到chroot目录下 12345cp -p /etc/named.conf /var/named/chroot/etc/cp -pr /var/named/name* /var/named/chroot/var/named/chown -R named:named /var/named/chroot/*# 配置文件 /var/named/chroot/etc/named.conf# 区域数据库文件 /var/named/chroot/var/named/ 配置文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274/* Sample named.conf BIND DNS server 'named' configuration file for the Red Hat BIND distribution. See the BIND Administrator's Reference Manual (ARM) for details about the configuration located in /usr/share/doc/bind-{version}/Bv9ARM.html*/options{ // Put files that named is allowed to write in the data/ directory: #指定区域数据库文件的路径目录 directory &quot;/var/named&quot;; // &quot;Working&quot; directory #CACHE文件路径,指定服务器在收到rndc dump命令时，转储数据到文件的路径。默认named_dump.db dump-file &quot;data/cache_dump.db&quot;; #静态文件路径,指定服务器在收到rndc stats命令时，追加统计数据的文件路径。默认named.stats statistics-file &quot;data/named_stats.txt&quot;; #内存静态文件路径,服务器在退出时，将内存统计写到文件的路径。默认named.memstats memstatistics-file &quot;data/named_mem_stats.txt&quot;; # 指定服务器在通过rndc recursing命令指定转储当前递归请求到的文件路径。默认named.recursing recursing-file &quot;data/named.recursing&quot;; #在收到rndc secroots指令后，服务器转储安全根的目的文件的路径名。默认named.secroots secroots-file &quot;data/named.secroots&quot;; /* Specify listenning interfaces. You can use list of addresses (';' is delimiter) or keywords &quot;any&quot;/&quot;none&quot; */ #IPV4监听端口为53,允许任何人连接 //listen-on port 53 { any; }; #IPv4监听端口为53，只允许本机连接 listen-on port 53 { 127.0.0.1; }; #IPV6监听端口为53,允许任何人连接 //listen-on-v6 port 53 { any; }; #IPv6监听端口为53，只允许本机连接 listen-on-v6 port 53 { ::1; }; /* 访问控制 Access restrictions 两个重要选项 There are two important options: allow-query { argument; }; - allow queries for authoritative data 允许查询来自权威数据 allow-query-cache { argument; }; - allow queries for non-authoritative data (mostly cached data) 允许查询来自非权威数据 You can use address, network address or keywords &quot;any&quot;/&quot;localhost&quot;/&quot;none&quot; as argument 大括号中可以使用IP地址、网段、或者关键字 any任何人 localhost本机 none任何人不允许 Examples: allow-query { localhost; 10.0.0.1; 192.168.1.0/8; }; allow-query-cache { ::1; fe80::5c63:a8ff:fe2f:4526; 10.0.0.1; }; */ #指定允许哪些主机可以进行普通的DNS查询,可以是关键字:any/localhost/none,也可以是IPV4,IPV6地址 allow-query { localhost; }; #指定允许哪些主机可以对缓存的访问 allow-query-cache { localhost; }; /* Enable/disable recursion - recursion yes/no; 递归查询开关 - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion. 假如你建立的是一个权威DNS你不需要开启递归 - If you are building a RECURSIVE (caching) DNS server, you need to enable recursion. 假如你建立的是一个递归DNS,你需要开启递归服务 - If your recursive DNS server has a public IP address, you MUST enable access 如果你的递归DNS是具有公网IP，你必须要设置访问控制来限制对合法用户的查询. control to limit queries to your legitimate users. Failing to do so will cause your server to become part of large scale DNS amplification 否者你的DNS会被大规模的攻击 attacks. Implementing BCP38 within your network would greatly 在您的网络中实现BCP38将非常重要减少此类攻击面 reduce such attack surface */ #开启递归 recursion yes; #Domain Name System Security Extensions (DNS安全扩展） /* DNSSEC related options. See information about keys (&quot;Trusted keys&quot;, bellow) */ /* Enable serving of DNSSEC related data - enable on both authoritative and recursive servers DNSSEC aware servers */ #开启DNSSEC在权威或者递归服务器之间信任服务 dnssec-enable yes; /* Enable DNSSEC validation on recursive servers */ #开启DNSSEC验证在递归服务器 dnssec-validation yes; /* In RHEL-7 we use /run/named instead of default /var/run/named so we have to configure paths properly. */ #PID文件路径 pid-file &quot;/run/named/named.pid&quot;; #session-keyfile文件路径 session-keyfile &quot;/run/named/session.key&quot;; #指定目录，其中保存着跟踪被管理DNSSEC密钥文件。默认为工作目录。 managed-keys-directory &quot;/var/named/dynamic&quot;;};logging {#开启DNS日志记录/* If you want to enable debugging, eg. using the 'rndc trace' command, * named will try to write the 'named.run' file in the $directory (/var/named). * By default, SELinux policy does not allow named to modify the /var/named directory, * so put the default debug log file in data/ : */ channel default_debug { file &quot;data/named.run&quot;; severity dynamic; };/*##日志分为两种 告警和访问logging { channel warning { file &quot;data/dns_warning&quot; versions 10 size 10m; severity warning; print-category yes; print-severity yes; print-time yes; }; channel general_dns { file &quot;data/dns_log&quot; versions 10 size 100m; severity info; print-category yes; print-severity yes; print-time yes; }; #默认日志 warning category default { warning; }; #访问日志级别 general_dns info category queries { general_dns; };};*/};/*通过Views指令配置智能查询DNS Views let a name server answer a DNS query differently depending on who is asking. By default, if named.conf contains no &quot;view&quot; clauses, all zones are in the &quot;default&quot; view, which matches all clients. Views are processed sequentially. The first match is used so the last view should match &quot;any&quot; - it's fallback and the most restricted view. If named.conf contains any &quot;view&quot; clause, then all zones MUST be in a view.*/#配置一个明称为localhost_resolver的智能访问视图view &quot;localhost_resolver&quot;{/* This view sets up named to be a localhost resolver ( caching only nameserver ). * If all you want is a caching-only nameserver, then you need only define this view: */ #允许使用该视图解析的客户端 localhost本机 any 任何机器 或者网段 match-clients { localhost; }; #允许递归 recursion yes; # all views must contain the root hints zone: #根域 zone &quot;.&quot; IN { #域类型为hint,还有master slave forward等类型 type hint; #区域数据库文件路径 file &quot;/var/named/named.ca&quot;; }; /* these are zones that contain definitions for all the localhost * names and addresses, as recommended in RFC1912 - these names should * not leak to the other nameservers: */ #包含子配置文件 include &quot;/etc/named.rfc1912.zones&quot;;};#定义视图internalview &quot;internal&quot;{/* This view will contain zones you want to serve only to &quot;internal&quot; clients that connect via your directly attached LAN interfaces - &quot;localnets&quot; . */ match-clients { localnets; }; recursion yes; zone &quot;.&quot; IN { type hint; file &quot;/var/named/named.ca&quot;; }; /* these are zones that contain definitions for all the localhost * names and addresses, as recommended in RFC1912 - these names should * not leak to the other nameservers: */ include &quot;/etc/named.rfc1912.zones&quot;; // These are your &quot;authoritative&quot; internal zones, and would probably // also be included in the &quot;localhost_resolver&quot; view above : /* NOTE for dynamic DNS zones and secondary zones: DO NOT USE SAME FILES IN MULTIPLE VIEWS! If you are using views and DDNS/secondary zones it is strongly recommended to read FAQ on ISC site (www.isc.org), section &quot;Configuration and Setup Questions&quot;, questions &quot;How do I share a dynamic zone between multIPle views?&quot; and &quot;How can I make a server a slave for both an internal and an external view at the same time?&quot; */ zone &quot;my.internal.zone&quot; { type master; file &quot;my.internal.zone.db&quot;; }; zone &quot;my.slave.internal.zone&quot; { type slave; file &quot;slaves/my.slave.internal.zone.db&quot;; masters { /* put master nameserver IPs here */ 127.0.0.1; } ; // put slave zones in the slaves/ directory so named can update them }; zone &quot;my.ddns.internal.zone&quot; { type master; allow-update { key ddns_key; }; file &quot;dynamic/my.ddns.internal.zone.db&quot;; // put dynamically updateable zones in the slaves/ directory so named can update them }; };#设置DDNS_key#主从复制加密使用key ddns_key{ #加密方式 hmac-md5 algorithm hmac-md5; secret &quot;use /usr/sbin/dnssec-keygen to generate TSIG keys&quot;;};view &quot;external&quot;{/* This view will contain zones you want to serve only to &quot;external&quot; clients * that have addresses that are not match any above view: */ match-clients { any; }; zone &quot;.&quot; IN { type hint; file &quot;/var/named/named.ca&quot;; }; recursion no; // you'd probably want to deny recursion to external clients, so you don't // end up providing free DNS service to all takers // These are your &quot;authoritative&quot; external zones, and would probably // contain entries for just your web and mail servers: zone &quot;my.external.zone&quot; { type master; file &quot;my.external.zone.db&quot;; };};/* Trusted keys#定义信任的dnssec密钥。 This statement contains DNSSEC keys. If you want DNSSEC aware resolver you have to configure at least one trusted key. Note that no key written below is valid. Especially root key because root zone is not signed yet.*//*trusted-keys {// Root Key&quot;.&quot; 257 3 3 &quot;BNY4wrWM1nCfJ+CXd0rVXyYmobt7sEEfK3clRbGaTwSJxrGkxJWoZu6I7PzJu/ E9gx4UC1zGAHlXKdE4zYIPRhaBKnvcC2U9mZhkdUpd1Vso/HAdjNe8LmMlnzY3 zy2Xy4klWOADTPzSv9eamj8V18PHGjBLaVtYvk/ln5ZApjYghf+6fElrmLkdaz MQ2OCnACR817DF4BBa7UR/beDHyp5iWTXWSi6XmoJLbG9Scqc7l70KDqlvXR3M /lUUVRbkeg1IPJSidmK3ZyCllh4XSKbje/45SKucHgnwU5jefMtq66gKodQj+M iA21AfUVe7u99WzTLzY3qlxDhxYQQ20FQ97S+LKUTpQcq27R7AT3/V5hRQxScI Nqwcz4jYqZD2fQdgxbcDTClU0CRBdiieyLMNzXG3&quot;;// Key for forward zoneexample.com. 257 3 5 &quot;AwEAAaxPMcR2x0HbQV4WeZB6oEDX+r0QM65KbhTjrW1ZaARmPhEZZe 3Y9ifgEuq7vZ/zGZUdEGNWy+JZzus0lUptwgjGwhUS1558Hb4JKUbb OTcM8pwXlj0EiX3oDFVmjHO444gLkBO UKUf/mC7HvfwYH/Be22GnC lrinKJp1Og4ywzO9WglMk7jbfW33gUKvirTHr25GL7STQUzBb5Usxt 8lgnyTUHs1t3JwCY5hKZ6CqFxmAVZP20igTixin/1LcrgX/KMEGd/b iuvF4qJCyduieHukuY3H4XMAcR+xia2 nIUPvm/oyWR8BW/hWdzOvn SCThlHf3xiYleDbt/o1OTQ09A0=&quot;;// Key for reverse zone.2.0.192.IN-ADDRPA.NET. 257 3 5 &quot;AQOnS4xn/IgOUpBPJ3bogzwcxOdNax071L18QqZnQQQA VVr+iLhGTnNGp3HoWQLUIzKrJVZ3zggy3WwNT6kZo6c0 tszYqbtvchmgQC8CzKojM/W16i6MG/ea fGU3siaOdS0 yOI6BgPsw+YZdzlYMaIJGf4M4dyoKIhzdZyQ2bYQrjyQ 4LB0lC7aOnsMyYKHHYeRv PxjIQXmdqgOJGq+vsevG06 zW+1xgYJh9rCIfnm1GX/KMgxLPG2vXTD/RnLX+D3T3UL 7HJYHJhAZD5L59VvjSPsZJHeDCUyWYrvPZesZDIRvhDD 52SKvbheeTJUm6EhkzytNN2SN96QRk8j/iI8ib&quot;;};*/ 配置主配文件 12345678910111213141516171819202122232425262728293031vim /var/named/chroot/etc/named.confoptions { listen-on port 53 { 192.168.88.132; }; #listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query { any; }; recursion yes; dnssec-enable yes; dnssec-validation yes; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;};zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;;};include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;;systemctl start named-chroot 区域数据库文件详解 123456789101112131415161718192021222324252627282930# 正向解析 named.localhost;缓存时间$TTL 1D;@表示相应的域名@ IN SOA @ rname.invalid. (;解析的域名 类型 授权域 授权域名服务器 管理员邮箱 0 ; serial 序列号,每次更新该文件系列号都应该变大 1D ; refresh 刷新时间,即规定从域名服务器多长时间查询一个主服务器，以保证从服务器的数据是最新的 1H ; retry 重试时间,即当从服务试图在主服务器上查询更时，而连接失败了，则这个值规定了从服务多长时间后再试 1W ; expire 过期时间,从服务器在向主服务更新失败后多长时间后清除对应的记录 3H ) ; minimum 这个数据用来规定缓冲服务器不能与主服务联系上后多长时间清除相应的记录 NS @ ;NS 名称服务器，表示这个主机为域名服务器 A 127.0.0.1;主机头 A记录 IP AAAA ::1; AAAA 解析为IPV6地址# 反向解析 named.loopback$TTL 1D@ IN SOA @ rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS @ PTR localhost;IP 反向指针 域名;PTR 反向指针 反解 2.3 正向解析 修改主配文件 1234zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone&quot;;}; 创建区域数据库文件 12345678910111213141516171819202122232425cp /var/named/chroot/var/named/named.localhost /var/named/chroot/var/named/cqm.com.zonechgrp named cqm.com.zonevim /var/named/chroot/var/named/cqm.com.zone$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum# A:IPv4解析为域名# PTR:域名解析为IP# MX# CNAME:设置别名 NS dns.cqm.com.# 解析dns为192.168.88.132dns A 192.168.88.132# 解析www为192.168.88.132www A 192.168.88.132# 用news访问也解析为wwwnews CNAME www# 检测文件是否有误named-checkzone cqm.com cqm.com.zone 在客户端上配置DNS 12vim /etc/resolve.confnameserver 192.168.88.132 通过host命令进行测试 1234567yum -y install bind-utilshost www.cqm.comwww.cqm.com has address 192.168.88.132host news.cqm.comnews.cqm.com is an alias for www.cqm.com.www.cqm.com has address 192.168.88.132 2.4 反向解析 修改主配文件 1234zone &quot;88.168.192.in-addr.arpa&quot; IN { type master; file &quot;192.168.88.arpa&quot;;}; 创建区域数据库文件 1234567891011121314cp /var/named/chroot/var/named/named.loopback /var/named/chroot/var/named/192.168.88.arpavim /var/named/chroot/var/named/192.168.88.arpa$TTL 1D88.168.192.in-addr.arpa. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.132 PTR www.cqm.com.named-checkzone 88.168.192.in-addr.arpa 192.168.88.arpa 测试 12host 192.168.88.132132.88.168.192.in-addr.arpa domain name pointer www.cqm.com. 2.5 主从同步即配置两台DNS服务器，由于上边我们以及配置过主DNS了，接下来再配置一台辅DNS服务器即可。 主DNS服务器IP：192.168.88.132 辅DNS服务器IP：192.168.88.135 安装DNS 1yum -y install bind bind-chroot 配置主配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445scp root@192.168.88.132:/var/named/chroot/etc/named.conf /var/named/chroot/etc/named.confchgrp named /var/named/chroot/etc/named.confvim /var/named/chroot/etc/named.confoptions { listen-on port 53 { 192.168.88.135; }; #listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query { any; }; # 从主DNS服务器拷过来的数据不进行加密 masterfile-format text; recursion yes; dnssec-enable yes; dnssec-validation yes; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;};zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;;};zone &quot;cqm.com&quot; IN { type slave; file &quot;cqm.com.zone&quot;; masters { 192.168.88.132; };};zone &quot;88.168.192.in-addr.arpa&quot; IN { type slave; file &quot;192.168.88.arpa&quot;; masters { 192.168.88.132; };};include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;; 配置和主DNS服务器相同的区域数据库文件 1234vim cqm.com.zone...vim 192.168.88.arpa... 将DNS服务器设为自己后进行测试 1234vim /etc/reslove.comfnameserver 192.168.88.135host www.cqm.com... 2.6 智能解析在DNS中植入全世界的IP库以及IP对应的地域，当用户发来请求时，会根据用户属于哪个地区来找那个地区的区域数据库文件来进行解析，从而使得不同地域的用户解析不同。 例子： 部署一台智能解析DNS服务器，对cqm.com进行解析 深圳用户解析为1.1.1.1 广州用户解析为2.2.2.2 佛山用户解析为3.3.3.3 修改主配文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071options { listen-on port 53 { 192.168.88.132; }; #listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query { any; }; recursion yes; dnssec-enable yes; dnssec-validation yes; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;};acl sz { # 假设该网段是深圳的IP地址段 192.168.77.0/24;};acl gz { 192.168.88.0/24;};acl fs { 192.168.99.0/24;};view shenzhen {match-clients { sz; }; zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;; }; zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone.SZ&quot;; };};view guangzhou {match-clients { gz; }; zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;; }; zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone.GZ&quot;; };};view foshan {match-clients { fs; }; zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;; }; zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone.FS&quot;; };}; 添加区域数据库文件 12345678910111213141516171819202122232425262728293031323334353637383940414243cp cqm.com.zone cqm.com.zone.SZcp cqm.com.zone cqm.com.zone.GZcp cqm.com.zone cqm.com.zone.FSchgrp named cqm.com.zone.*# 深圳$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.dns A 192.168.88.132www A 1.1.1.1news CNAME www# 广州$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.dns A 192.168.88.132www A 2.2.2.2news CNAME www# 佛山$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.dns A 192.168.88.132www A 3.3.3.3news CNAME www 测试 123# 测试主机的地址段为192.168.88.0/24网段的，所以属于广州区域，即匹配解析到2.2.2.2host www.cqm.comwww.cqm.com has address 2.2.2.2 三、FTP文件传输服务3.1 FTP服务介绍FTP即文件传输协议，是TCP/IP协议组的协议之一。 FTP默认采用TCP20和21端口，20用于传输数据，21用于控制传输信息。 FTP分别有主动传输方式和被动传输方式两种，当FTP为主动传输方式时运用20和21端口，而当FTP为被动传输方式时则会随即打开一个大于1024的端口来进行数据的传输。 3.2 FTP服务部署 安装vsftpd 1yum -y install vsftpd 主配文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167# Example config file /etc/vsftpd/vsftpd.conf## The default compiled in settings are fairly paranoid. This sample file# loosens things up a bit, to make the ftp daemon more usable.# Please see vsftpd.conf.5 for all compiled in defaults.## READ THIS: This example file is NOT an exhaustive list of vsftpd options.# Please read the vsftpd.conf.5 manual page to get a full idea of vsftpd's# capabilities.##匿名用户访问,YES是允许，NO是拒绝# Allow anonymous FTP? (Beware - allowed by default if you comment this out).anonymous_enable=YES## Uncomment this to allow local users to log in.# 本地用户登录,YES是允许，NO是拒绝.默认访问的是本地用户家目录，如果你开启了selinux# 请设置开启布尔值ftp_home_dir为ON# When SELinux is enforcing check for SE bool ftp_home_dirlocal_enable=YES##允许本地用户上传# Uncomment this to enable any form of FTP write command.write_enable=YES## Default umask for local users is 077. You may wish to change this to 022,# 上传的权限是022，使用的是umask权限。对应的目录是755，文件是644# if your users expect that (022 is used by most other ftpd's)local_umask=022## Uncomment this to allow the anonymous FTP user to upload files. This only# has an effect if the above global write enable is activated. Also, you will# obviously need to create a directory writable by the FTP user.# When SELinux is enforcing check for SE bool allow_ftpd_anon_write, allow_ftpd_full_access# 开启匿名用户上传功能，默认是拒绝的#anon_upload_enable=YES## Uncomment this if you want the anonymous FTP user to be able to create# new directories.# 开启匿名用户创建文件或文件夹权限#anon_mkdir_write_enable=YES## Activate directory messages - messages given to remote users when they# go into a certain directory.# 开启目录欢迎消息，一般对命令行登陆有效dirmessage_enable=YES## Activate logging of uploads/downloads.# 开启上传和下载日志记录功能xferlog_enable=YES##使用标准模式# Make sure PORT transfer connections originate from port 20 (ftp-data).connect_from_port_20=YES## If you want, you can arrange for uploaded anonymous files to be owned by# a different user. Note! Using &quot;root&quot; for uploaded files is not# recommended!# 声明匿名用户上传文件的所有者# 允许更改匿名用户上传文件的所有者#chown_uploads=YES#所有者为whoever#chown_username=whoever## You may override where the log file goes if you like. The default is shown# below.# 日志文件路径#xferlog_file=/var/log/xferlog## If you want, you can have your log file in standard ftpd xferlog format.# Note that the default log file location is /var/log/xferlog in this case.# 日志文件采用标准格斯xferlog_std_format=YES## You may change the default value for timing out an idle session.# 会话超时时间#idle_session_timeout=600## You may change the default value for timing out a data connection.# 数据传输超时时间#data_connection_timeout=120## It is recommended that you define on your system a unique user which the# ftp server can use as a totally isolated and unprivileged user.# FTP子进程管理用户#nopriv_user=ftpsecure## Enable this and the server will recognise asynchronous ABOR requests. Not# recommended for security (the code is non-trivial). Not enabling it,# however, may confuse older FTP clients.# 是否允许客户端发起“async ABOR”请求，该操作是不安全的默认禁止。#async_abor_enable=YES## By default the server will pretend to allow ASCII mode but in fact ignore# the request. Turn on the below options to have the server actually do ASCII# mangling on files when in ASCII mode. The vsftpd.conf(5) man page explains# the behaviour when these options are disabled.# Beware that on some FTP servers, ASCII support allows a denial of service# attack (DoS) via the command &quot;SIZE /big/file&quot; in ASCII mode. vsftpd# predicted this attack and has always been safe, reporting the size of the# raw file.# ASCII mangling is a horrible feature of the protocol.# 该选项用于指定是否允许上传时以ASCII模式传输数据#ascii_upload_enable=YES#该选项用于指定是否允许下载时以ASCII模式传输数据#ascii_download_enable=YES## You may fully customise the login banner string:# FTP文本界面登陆欢迎词#ftpd_banner=Welcome to blah FTP service.## You may specify a file of disallowed anonymous e-mail addresses. Apparently# useful for combatting certain DoS attacks.# 是否开启拒绝的Email功能#deny_email_enable=YES# (default follows)# 指定保存被拒接的Email地址的文件#banned_email_file=/etc/vsftpd/banned_emails## You may specify an explicit list of local users to chroot() to their home# directory. If chroot_local_user is YES, then this list becomes a list of# users to NOT chroot().# (Warning! chroot'ing can be very dangerous. If using chroot, make sure that# the user does not have write access to the top level directory within the# chroot)# 是否开启对本地用户chroot的限制，YES为默认所有用户都不能切出家目录，NO代表默认用户都可以切出家目录# 设置方法类似于：YES拒绝所有允许个别；NO允许所有拒绝个别#chroot_local_user=YES# 开启特例列表#chroot_list_enable=YES# (default follows)# 如果chroot_local_user的值是YES则该文件中的用户是可以切出家目录，如果是NO，该文件中的用户则不能切出家目录# 一行一个用户。#chroot_list_file=/etc/vsftpd/chroot_list## You may activate the &quot;-R&quot; option to the builtin ls. This is disabled by# default to avoid remote users being able to cause excessive I/O on large# sites. However, some broken FTP clients such as &quot;ncftp&quot; and &quot;mirror&quot; assume# the presence of the &quot;-R&quot; option, so there is a strong case for enabling it.# 是否开启ls 递归查询功能 ls -R#ls_recurse_enable=YES## When &quot;listen&quot; directive is enabled, vsftpd runs in standalone mode and# listens on IPv4 sockets. This directive cannot be used in conjunction# with the listen_ipv6 directive.# 是否开启ftp独立模式在IPV4listen=NO## This directive enables listening on IPv6 sockets. By default, listening# on the IPv6 &quot;any&quot; address (::) will accept connections from both IPv6# and IPv4 clients. It is not necessary to listen on *both* IPv4 and IPv6# sockets. If you want that (perhaps because you want to listen on specific# addresses) then you must run two copies of vsftpd with two configuration# files.# Make sure, that one of the listen options is commented !!# 是否开启ftp独立模式在ipv6listen_ipv6=YES#启用pam模块验证pam_service_name=vsftpd#是否开启userlist功能.定义对列表中的用户做定义userlist_deny=NO#NO拒绝所有人访问，对应列表中的用户可以访问，YES允许所有人访问，列表中的用户无法访问。#只有userlist_file=/etc/vsftpd/user_list定义的用户才可以访问或拒绝访问userlist_enable=YES#是否开启tcp_wrappers管理，TCP_Wrappers是一个工作在第四层（传输层）的的安全工具，#对有状态连接的特定服务进行安全检测并实现访问控制tcp_wrappers=YES 匿名用户和本地用户 需要注意的是，匿名用户访问的是/var/ftp，而本地用户访问的话是家目录。 关于权限，在主配文件中设置的权限是反码，文件实际权限 = 666 - 反码。 假如主配文件中设置为022，那么文件实际权限 = 666 - 022 = 644，文件夹实际权限 = 777 - 022 = 755。 在linux端访问FTP服务器时，无论FTP服务器是否开启了匿名用户访问，客户访问时都要输入用户名和密码，匿名用户用户名ftp，密码随意，但是需要为带有@的email地址。 开启chroot 12345678910# 是否开启对本地用户chroot的限制，YES为默认所有用户都不能切出家目录，NO代表默认用户都可以切出家目录# 设置方法类似于：YES拒绝所有允许个别；NO允许所有拒绝个别chroot_local_user=YESchroot_list_enable=YES# 特例列表chroot_list_file=/etc/vsftpd/chroot_list# 如果用户家目录有写权限的话，则该用户登陆不上# 如果想在有写权限的家目录登录的话，需在配置文件加上allow_writeable_chroot=YES 3.3 FTP命令登录到FTP服务器后，基本命令如下 123456789help # 打印命令菜单!+linux命令 # 执行linux命令lcd 目录路径 # 切换linux当前路径put mput # 上传 批量上传get mget # 下载 批量下载ls dir # 列出目录内容mkdir cd delete rmdir # 创建目录 进入目录 删除文件 删除目录pwd # 现实FTP当前路径open close bye # 开启/关闭/退出FTP 3.4 虚拟用户由于FTP是采用本地用户来进行登录的，所以会将本地用户暴露在互联网中，如果没有相关安全设置，就会造成FTP不安全。 因此FTP可以设置虚拟用户来解决该问题。 在主配文件中开启虚拟用户 123456guest_enable=YESguest_username=cqm# 虚拟用户不用本地用户的权限virtual_use_local_privs=NO# 用户文件存放地址user_config_dir=/etc/vsftpd/vconf.d 3.5 基于虚拟用户配置的安全FTP案例要求： 公司公共文件可以通过匿名下载 公式A部门、B部门、C部门分别由自己的文件夹，并相互隔离 部门之间只有主管拥有上传权限，部门员工只有下载权限 禁止用户查看家目录以外的数据 确保FTP账号安全 创建虚拟用户映射本地账号 1useradd -s /sbin/nologin -d /var/tmp/vuser_ftp cqm 创建目录，所有操作都只能在此目录进行 1234chmod 500 /var/tmp/vuser_ftpmkdir /var/tmp/vuser_ftp/{A,B,C}chmod 700 /var/tmp/vuser_ftp/*chown -R cqm:cqm /var/tmp/vuser_ftp 创建虚拟用户账号密码文件，并生成db文件 123456789101112131415vim /etc/vsftpd/vuserA_01123B_01123C_01123A_02123B_02123C_02123db_load -T -t hash -f /etc/vsftpd/vuser /etc/vsftpd/vuser.dbchmod 600 /etc/vsftpd/vuser.db 设置虚拟用户pam认证 123vim /etc/pam.d/vsftpdauth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/vuseraccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser 要求不能切出家目录，所以要设置chroot_list 1234567vim /etc/vsftpd/chroot_listA_01B_01C_01A_02B_02C_02 创建子配置文件 1234567891011121314151617181920mkdir /etc/vsftpd/vconf.d# A主管文件vim /etc/vsftpd/vconf.d/A_01# 指定家目录local_root=/var/tmp/vuser_ftp/A# 指定权限anon_umask=077# 下载权限anon_world_readable_only=NO# 上传权限anon_upload_enable=YES# 创建目录权限anon_mkdir_write_enable=YES# 删除和重命名目录权限anon_other_write_enable=YES# A员工文件vim /etc/vsftpd/vconf.d/A_02local_root=/var/tmp/vuser_ftp/Aanon_world_readable_only=NO 配置主配文件 123456789101112131415161718192021vim /etc/vsftpd/vsftpd.confanonymous_enable=YESlocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_std_format=YESchroot_local_user=YESchroot_list_enable=YESchroot_list_file=/etc/vsftpd/chroot_listlisten=YESlisten_ipv6=NOpam_service_name=vsftpduserlist_enable=YEStcp_wrappers=YESguest_enable=YESguest_username=cqmvirtual_use_local_privs=NOuser_config_dir=/etc/vsftpd/vconf.d 测试主管用户和员工用户的权限 1234ftp 192.168.88.132A_01123230 Login successful 四、Samba服务4.1 Samba服务介绍Samba是可以实现不同计算机系统之间文件共享的服务，即是在Linux和UNIX系统上实现SMB协议的一个免费软件。 SMB（Server Messages Block，信息服务块）是一种在局域网上共享文件和打印机的一种通信协议，它为局域网内的不同计算机之间提供文件及打印机等资源的共享服务。 Samba采用到的端口有： UDP 137：NetBIOS 名字服务 UDP 138：NetBIOS 数据报服务 UDP 139：SMB TCP 389：用于 LDAP (Active Directory Mode) TCP 445：NetBIOS服务在windos 2000及以后版本使用此端口, (Common Internet File System，CIFS，它是SMB协议扩展到Internet后，实现Internet文件共享) TCP 901：用于 SWAT，用于网页管理Samba 4.2 Samba服务部署 安装samba 12yum -y install samba samba-clientsystemctl start smb nmb 主配文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351cat /etc/samba/smb.conf.example# This is the main Samba configuration file. For detailed information about the# options listed here, refer to the smb.conf(5) manual page. Samba has a huge# number of configurable options, most of which are not shown in this example.## The Samba Wiki contains a lot of step-by-step guides installing, configuring,# and using Samba:# https://wiki.samba.org/index.php/User_Documentation## In this file, lines starting with a semicolon (;) or a hash (#) are# comments and are ignored. This file uses hashes to denote commentary and# semicolons for parts of the file you may wish to configure.## NOTE: Run the &quot;testparm&quot; command after modifying this file to check for basic# syntax errors.##---------------##SAMBA selinux相关设置，如果你开启了selinux，请注意下面的说明###Security-Enhanced Linux (SELinux) Notes:## Turn the samba_domain_controller Boolean on to allow a Samba PDC to use the# useradd and groupadd family of binaries. Run the following command as the# root user to turn this Boolean on:# 如果你在域环境中使用samba那么请设置下面的bool值# setsebool -P samba_domain_controller on## Turn the samba_enable_home_dirs Boolean on if you want to share home# directories via Samba. Run the following command as the root user to turn this# Boolean on:## 假如希望通过samba共享用户家目录请设置下面的bool值# setsebool -P samba_enable_home_dirs on## If you create a new directory, such as a new top-level directory, label it# with samba_share_t so that SELinux allows Samba to read and write to it. Do# not label system directories, such as /etc/ and /home/, with samba_share_t, as# such directories should already have an SELinux label.##加入你想将目录通过samba共享，请确认其目录标签为sambe_share_t# Run the &quot;ls -ldZ /path/to/directory&quot; command to view the current SELinux# label for a given directory.## Set SELinux labels only on files and directories you have created. Use the# chcon command to temporarily change a label:# 标签设置方法# chcon -t samba_share_t /path/to/directory## Changes made via chcon are lost when the file system is relabeled or commands# such as restorecon are run.## Use the samba_export_all_ro or samba_export_all_rw Boolean to share system# directories. To share such directories and only allow read-only permissions:# 对共享目录的权限的bool设置，只读或读写# setsebool -P samba_export_all_ro on# To share such directories and allow read and write permissions:# setsebool -P samba_export_all_rw on## To run scripts (preexec/root prexec/print command/...), copy them to the# /var/lib/samba/scripts/ directory so that SELinux will allow smbd to run them.# Note that if you move the scripts to /var/lib/samba/scripts/, they retain# their existing SELinux labels, which may be labels that SELinux does not allow# smbd to run. Copying the scripts will result in the correct SELinux labels.# Run the &quot;restorecon -R -v /var/lib/samba/scripts&quot; command as the root user to# apply the correct SELinux labels to these files.##--------------##======================= Global Settings =====================================#全局设置，对整个服务都生效[global]#网络设置# ----------------------- Network-Related Options -------------------------## workgroup = the Windows NT domain name or workgroup name, for example, MYGROUP.## server string = the equivalent of the Windows NT Description field.## netbios name = used to specify a server name that is not tied to the hostname,# maximum is 15 characters.## interfaces = used to configure Samba to listen on multiple network interfaces.# If you have multiple interfaces, you can use the &quot;interfaces =&quot; option to# configure which of those interfaces Samba listens on. Never omit the localhost# interface (lo).## hosts allow = the hosts allowed to connect. This option can also be used on a# per-share basis.## hosts deny = the hosts not allowed to connect. This option can also be used on# a per-share basis.##定义计算机的工作组,如果希望和windows共享，可以设置为workgroup，这样就可以在windows的网上邻居中找到linux计算机 workgroup = MYGROUP#对samba服务器的描述信息 server string = Samba Server Version %v#设置netbios计算机名称; netbios name = MYSERVER#samba使用本机的那块网卡; interfaces = lo eth0 192.168.12.2/24 192.168.13.2/24#允许那个网段访问samba服务器共享; hosts allow = 127. 192.168.12. 192.168.13.##日志选项# --------------------------- Logging Options -----------------------------## log file = specify where log files are written to and how they are split.## max log size = specify the maximum size log files are allowed to reach. Log# files are rotated when they reach the size specified with &quot;max log size&quot;.# #samba日志文件路径 # log files split per-machine: log file = /var/log/samba/log.%m #日志文件大小，0为不限制，注意不建议这样设置 # maximum size of 50KB per log file, then rotate: max log size = 50#独立服务选项# ----------------------- Standalone Server Options ------------------------## security = the mode Samba runs in. This can be set to user, share# (deprecated), or server (deprecated).## passdb backend = the backend used to store user information in. New# installations should use either tdbsam or ldapsam. No additional configuration# is required for tdbsam. The &quot;smbpasswd&quot; utility is available for backwards# compatibility.##samba安全级别#share: 不需要账号密码，公开共享#user: 需要提供sam账号密码才能访问共享，私密共享#server：依靠其他Windows NT/2000或Samba Server来验证用户的账号和密码,是一种代理验证。此种安全模式下,系统管理员可以把所有的Windows用户和口令集中到一个NT系统上,&gt;使用Windows NT进行Samba认证, 远程服务器可以自动认证全部用户和口令,如果认证失败,Samba将使用用户级安全模式作为替代的方式。#domain：域安全级别,使用主域控制器(PDC)来完成认证。##一般情况下我们使用share和user的比较多，除非公司有完整的域环境 security = user#该方式则是使用一个数据库文件来建立用户数据库。数据库文件叫passdb.tdb，默认在/etc/samba目录下。passdb.tdb 用户数据库可以使用smbpasswd –a来建立Samba用户，不过要建立的Samba用户必须先是系统用户。我们也可以使用pdbedit命令来建立Samba账户并由其pdbedit管理。 passdb backend = tdbsam#域成员选项# ----------------------- Domain Members Options ------------------------## security = must be set to domain or ads.## passdb backend = the backend used to store user information in. New# installations should use either tdbsam or ldapsam. No additional configuration# is required for tdbsam. The &quot;smbpasswd&quot; utility is available for backwards# compatibility.## realm = only use the realm option when the &quot;security = ads&quot; option is set.# The realm option specifies the Active Directory realm the host is a part of.## password server = only use this option when the &quot;security = server&quot;# option is set, or if you cannot use DNS to locate a Domain Controller. The# argument list can include My_PDC_Name, [My_BDC_Name], and [My_Next_BDC_Name]:## password server = My_PDC_Name [My_BDC_Name] [My_Next_BDC_Name]## Use &quot;password server = *&quot; to automatically locate Domain Controllers.#设置域共享; security = domain; passdb backend = tdbsam#定义域名称; realm = MY_REALM#域验证服务器; password server = #域控选项# ----------------------- Domain Controller Options ------------------------## security = must be set to user for domain controllers.## passdb backend = the backend used to store user information in. New# installations should use either tdbsam or ldapsam. No additional configuration# is required for tdbsam. The &quot;smbpasswd&quot; utility is available for backwards# compatibility.## domain master = specifies Samba to be the Domain Master Browser, allowing# Samba to collate browse lists between subnets. Do not use the &quot;domain master&quot;# option if you already have a Windows NT domain controller performing this task.## domain logons = allows Samba to provide a network logon service for Windows# workstations.## logon script = specifies a script to run at login time on the client. These# scripts must be provided in a share named NETLOGON.## logon path = specifies (with a UNC path) where user profiles are stored.##; security = user; passdb backend = tdbsam; domain master = yes; domain logons = yes # the following login script name is determined by the machine name # (%m):; logon script = %m.bat # the following login script name is determined by the UNIX user used:; logon script = %u.bat; logon path = \\\\%L\\Profiles\\%u # use an empty path to disable profile support:; logon path = # various scripts can be used on a domain controller or a stand-alone # machine to add or delete corresponding UNIX accounts:; add user script = /usr/sbin/useradd &quot;%u&quot; -n -g users; add group script = /usr/sbin/groupadd &quot;%g&quot;; add machine script = /usr/sbin/useradd -n -c &quot;Workstation (%u)&quot; -M -d /nohome -s /bin/false &quot;%u&quot;; delete user script = /usr/sbin/userdel &quot;%u&quot;; delete user from group script = /usr/sbin/userdel &quot;%u&quot; &quot;%g&quot;; delete group script = /usr/sbin/groupdel &quot;%g&quot;#这些设置选项主要用于SMB网络中进行浏览时，设置samba服务器的行为。缺省情况不让 samba服务器参加broswser的推举过程，为了使得samba服务器能成为browser，就需要设定local master =yes。然后samba服务就可以根据os level设置的权重进行推举，缺省的os level为0，这个权重不会赢得推举。但可以取消注释，将os level设置为33，这将在与所有Windows计算机（包括Windows NT）的推举竞赛中获得胜利，因为NT server的权重为32。设置比33更高的权重，只是在不同的samba 服务器之间进行选择时才有意义。## preferred master 可以设置自己优先成为浏览服务器候选人## ----------------------- Browser Control Options ----------------------------## local master = when set to no, Samba does not become the master browser on# your network. When set to yes, normal election rules apply.## os level = determines the precedence the server has in master browser# elections. The default value should be reasonable.## preferred master = when set to yes, Samba forces a local browser election at# start up (and gives itself a slightly higher chance of winning the election).#; local master = no; os level = 33; preferred master = yes###wins服务，如果网络中配置了wins服务器可以在此设置wins相关项#----------------------------- Name Resolution -------------------------------## This section details the support for the Windows Internet Name Service (WINS).## Note: Samba can be either a WINS server or a WINS client, but not both.## wins support = when set to yes, the NMBD component of Samba enables its WINS# server.## wins server = tells the NMBD component of Samba to be a WINS client.## wins proxy = when set to yes, Samba answers name resolution queries on behalf# of a non WINS capable client. For this to work, there must be at least one# WINS server on the network. The default is no.## dns proxy = when set to yes, Samba attempts to resolve NetBIOS names via DNS# nslookups.#设置nmb进程支持wins服务; wins support = yes#设置wins服务器ip; wins server = w.x.y.z#设置wins代理IP; wins proxy = yes#设置Samba服务器是否在无法联系WINS服务器时通过DNS去解析主机的NetBIOS名; dns proxy = yes#该部分包括Samba服务器打印机相关设置# --------------------------- Printing Options -----------------------------## The options in this section allow you to configure a non-default printing# system.## load printers = when set you yes, the list of printers is automatically# loaded, rather than setting them up individually.## cups options = allows you to pass options to the CUPS library. Setting this# option to raw, for example, allows you to use drivers on your Windows clients.## printcap name = used to specify an alternative printcap file.##是否启用共享打印机 load printers = yes cups options = raw#打印机配置文件; printcap name = /etc/printcap # obtain a list of printers automatically on UNIX System V systems:; printcap name = lpstat#打印机的系统类型,现在支持的打印系统有：bsd, sysv, plp, lprng, aix, hpux, qnx,cups; printing = cups#该部分包括Samba服务器如何保留从Windows客户端复制或移动到Samba服务器共享目录文件的Windows文件属性的相关配置.# --------------------------- File System Options ---------------------------## The options in this section can be un-commented if the file system supports# extended attributes, and those attributes are enabled (usually via the# &quot;user_xattr&quot; mount option). These options allow the administrator to specify# that DOS attributes are stored in extended attributes and also make sure that# Samba does not change the permission bits.## Note: These options can be used on a per-share basis. Setting them globally# (in the [global] section) makes them the default for all shares.#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的存档属性。默认no。; map archive = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的隐藏属性。默认no。; map hidden = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的只读属性。默认为no。; map read only = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的系统文件属性。默认为no。; map system = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的相关属性（只读、系统、隐藏、存档属性）。默认为yes。; store dos attributes = yes#共享设置#============================ Share Definitions ==============================#用户家目录共享#共享名称[homes]#描述 comment = Home Directories#是否支持浏览 browseable = no#是否允许写入 writable = yes#允许访问该共享资源的smb用户，@组; valid users = %S; valid users = MYDOMAIN\\%S#打印机共享[printers]#描述 comment = All Printers#路径 path = /var/spool/samba#是否可浏览，no类似隐藏共享 browseable = no#是否支持guest访问，和public指令类似 guest ok = no#是否可写 writable = no#是否允许打印 printable = yes# Un-comment the following and create the netlogon directory for Domain Logons:; [netlogon]; comment = Network Logon Service; path = /var/lib/samba/netlogon; guest ok = yes; writable = no; share modes = no# Un-comment the following to provide a specific roaming profile share.# The default is to use the user's home directory:; [Profiles]; path = /var/lib/samba/profiles; browseable = no; guest ok = yes# A publicly accessible directory that is read only, except for users in the# &quot;staff&quot; group (which have write permissions):; [public]; comment = Public Stuff; path = /home/samba; public = yes; writable = no; printable = no#定义允许哪些smb用户写入; write list = +staff 4.3 Samba共享案例：在Windows上访问Samba服务器，共享目录为/common，指定用cqm1、cqm2用户才能访问，且只有cqm2有写权限。 创建Samba用户 12345678910# smbpasswd用户命令# -a 添加用户 smbpasswd -a cqm# -x 删除用户 smbpasswd -x cqm# -d 禁用帐号 smbpasswd -d cqm# -e 取消禁用 smbpasswd -e cqm# -n 清除密码 smbpasswd -a cqmuseradd -s /sbin/nologin cqm1useradd -s /sbin/nologin cqm2smbpasswd -a cqm1smbpasswd -a cqm2 创建共享目录 123mkdir /common# 设置757是为了让cqm2有写权限chmod 757 /common 修改主配文件 1234567891011[global] workgroup = WORKGROUP ...[common] comment = samba share directory path = /common browseable = YES hosts allow = 10.0.0.0/8,192.168.88.0/24 valid users = cqm1,cqm2 writable = No write list = cqm2 在Windows中输入 \\\\192.168.88.132 访问 首先是cqm1用户 可以看到cqm1用户没有写入权限 cqm2用户 可以看到cqm2用户有创建文件夹的权限 4.4 Linux挂载 在客户端上安装samba-client 1yum -y install samba-client 通过smbclient命令访问 1smbclient //192.168.88.132/common -U cqm2%toortoor 通过mount命令挂载 1234mkdir /commonmount -o username=cqm2,password=toortoor -t cifs //192.168.88.132/common /commonmount//192.168.88.132/common on /common type cifs (rw,relatime,vers=default,cache=strict,username=cqm2,domain=LOCALHOST,uid=0,noforceuid,gid=0,noforcegid,addr=192.168.88.132,file_mode=0755,dir_mode=0755,soft,nounix,serverino,mapposix,rsize=1048576,wsize=1048576,echo_interval=60,actimeo=1) 五、NFS文件服务5.1 NFS服务介绍NFS即网络文件系统，它允许网络中的计算机之间通过TCP/IP网络共享资源。在NFS的应用中，本地NFS的客户端应用可以透明地读写位于远端NFS服务器上的文件，就像访问本地文件一样。 NFS应用场景： 共享存储服务器：图片服务器、视频服务器 家目录漫游：域用户家目录服务器 文件服务器：文件存储服务器 5.2 NFS实现共享 安装NFS 123yum -y install nfs-utilssystemctl start rpcbindsystemctl start nfs /etc/exports共享文件 12345678910111213141516171819202122# 共享格式# 共享目录绝对路径 IP地址或网段地址(权限1,权限2)/test 192.168.88.132(rw,sync)# 权限说明ro 只读访问 rw 读写访问 sync 所有数据在请求时写入共享 async NFS在写入数据前可以相应请求 secure NFS通过1024以下的安全TCP/IP端口发送 insecure NFS通过1024以上的端口发送 wdelay 如果多个用户要写入NFS目录，则归组写入（默认） no_wdelay 如果多个用户要写入NFS目录，则立即写入，当使用async时，无需此设置。 hide 在NFS共享目录中不共享其子目录 no_hide 共享NFS目录的子目录 subtree_check 如果共享/usr/bin之类的子目录时，强制NFS检查父目录的权限（默认） no_subtree_check 和上面相对，不检查父目录权限 all_squash 共享文件的UID和GID映射匿名用户anonymous，适合公用目录。 no_all_squash 保留共享文件的UID和GID（默认） root_squash root用户的所有请求映射成如anonymous用户一样的权限（默认） no_root_squash root用户具有根目录的完全管理访问权限 anonuid=xxx 指定NFS服务器/etc/passwd文件中匿名用户的UID anongid=xxx 指定NFS服务器/etc/passwd文件中匿名用户的GID exportfs共享管理命令 12345678910exportfs命令：-a 打开或取消所有目录共享。-o options,... 指定一列共享选项，与 exports(5) 中讲到的类似。-i 忽略 /etc/exports 文件，从而只使用默认的和命令行指定的选项。-r 重新共享所有目录。它使/var/lib/nfs/xtab和/etc/exports同步。它将/etc/exports中已删除的条目从 /var/lib/nfs/xtab中删除，将内核共享表中任何不再有效的条目移除。-u 取消一个或多个目录的共享。-f 在“新”模式下，刷新内核共享表之外的任何东西。 任何活动的客户程序将在它们的下次请求中得到mountd 添加的新的共享条目。-v 输出详细信息。当共享或者取消共享时，显示在做什么。 显示当前共享列表的时候，同时显示共享的选项。 设置共享 12345678910# 创建被共享目录mkdir /test# 设置exportsvim /etc/exports/test 192.168.88.0/24(rw,sync)# 共享exportfs -r# 查看是否共享exportfs -vshowmount -e 192.168.88.132 客户端挂载 客户端挂载是使用nfsnobody用户进行的，如果是root创建的共享目录，且客户端挂载后要进行读写的话，得给目录757的权限。 1mount -t nfs 192.168.88.132:/test /test 六、iSCSI服务6.1 iSCSI介绍iSCSI即网络小型计算机系统接口，又被称为IPSAN。实际就是通过网络来共享设备。 数据存储技术： DSA（Direct Attached Storage 直接附加存储）：IDE SATA SAS SCSI（本地磁盘） NSA（Network Attached Storage 网络附加存储）：Samba NFS（共享文件夹） SAN（Storage Attached Network 网络附加存储）：iSCSI（共享设备） 6.2 iSCSI服务部署 准备好要被挂载的磁盘，这里共享sdb1 安装iSCSI 12yum -y install targetclisysetmctl start target 通过targetcli命令添加设备共享 123456789101112131415161718192021222324# 进入命令行targetclils...# backstores 代表后端存储,iscsi通过使用文件、逻辑卷或任何类型的磁盘作为底层存储来仿真呈现为目标的scsi设备# block 后端存储是个块设备# fileio 后端存储是一个文件# pscsi 物理scsi设备# ramdisk 后端存储是内存上的空间，在内存上创建一个指定大小的ramdisk设备可以通过help命令来打印可用命令# 将要共享的设备添加到backstores存储库中cd backstores/block/ creat block /dev/sdb1# 设置IQN标识# 格式：iqn.年-月.二级域名倒写:共享名cd ..iscsi/ create iqn.2021-04.com.cqm:storage# 设置TPG组中对应的三个问题 谁 从哪里 访问什么设备cd iscsi/iqn.2021-04.com.cqm:storage/tpg1/acls/ create iqn.2021-04.com.cqm:clientluns/ create /backstores/block/blockexit 6.3 iSCSI客户端挂载 安装iSCSI客户端 1yum -y install iscsi-initiator-utils 设置客户端名称 123vim /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2021-04.com.cqm:clientsystemctl start iscsi 发现共享设备 1iscsiadm --mode discoverydb --type sendtargets --portal 192.168.88.132:3260 --discover 连接远程设备 1234iscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.88.132:3260 --loginlsblk...sdb 分区格式化 1234fdisk /dev/sdb...mkfs.ext4 /dev/sdb1... 挂载共享磁盘 1234mkdir /root/sdb1vim /etc/fstab/dev/sdb1 /root/sdb1 ext4 _netdev 0 0mount -a 6.4 iSCSI取消挂载 客户端 123iscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.88.132:3260 --logoutrm -rf /var/lib/iscsi/nodes/iqn.2021-04.com.cqm\\:storage/rm -rf /var/lib/iscsi/send_targets/192.168.88.132,3260/ 服务端 1234567# 倒着删targetcliiscsi/iqn.2021-04.com.cqm:storage/tpg1/portals/ delete 0.0.0.0 3260iscsi/iqn.2021-04.com.cqm:storage/tpg1/luns/ delete lun=0iscsi/iqn.2021-04.com.cqm:storage/tpg1/acls/ delete iqn.2021-04.com.cqm:clientiscsi/ delete iqn.2021-04.com.cqm:storagebackstores/block/ delete block 七、IPSAN多链路部署在上边的环境中的共享设备是通过单链路共享的，如果这条链路出现了故障，那么就会出现连接不上共享设备的问题，所以在生产环境中都会配置多链路进行部署。 iSCSI服务端和客户端分别拥有两张不同网段的网卡，就可以配置多链路部署。 7.1 部署多链路 在服务端上设置共享设备，并用两个IP进行共享 在客户端发现共享设备 123456789vim /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2021-04.com.cqm:clientsystemctl start iscsiiscsiadm --mode discoverydb --type sendtargets --portal 192.168.88.132:3260 --discover 192.168.88.132:3260,1 iqn.2021-04.com.cqm:storage 192.168.99.130:3260,1 iqn.2021-04.com.cqm:storageiscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.88.132:3260 --loginiscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.99.130:3260 --login 这时候通过lsblk命令可以看到多了两块设备，但其实是同一个设备不同名 安装多路径软件 123yum -y install device-mapper-multipathcp /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf /etc/systemctl start multipathd 再用lsblk命令查看可发现 配置多路径运行模式 123456789101112131415161718192021222324252627282930multipath -ll# wwid:36001405ac25fe1abdfd4eecb1d0624b2mpatha (36001405ac25fe1abdfd4eecb1d0624b2) dm-2 LIO-ORG ,block...vim /etc/multipath.confmultipaths { multipath { wwid 36001405ac25fe1abdfd4eecb1d0624b2 # wwid alias cqm # 起名 path_grouping_policy multibus # 多路径组策略 path_selector &quot;round-robin 0&quot; # 负载均衡模式 failback manual rr_weight priorities # 按优先级轮询 no_path_retry 5 # 重试时间5s } multipath { wwid 1DEC_____321816758474 alias red }}systemctl restart multipathdsystemctl restart iscsimultipath -llcqm (36001405ac25fe1abdfd4eecb1d0624b2) dm-2 LIO-ORG ,blocksize=1023M features='1 queue_if_no_path' hwhandler='0' wp=rw`-+- policy='round-robin 0' prio=1 status=active |- 5:0:0:0 sdb 8:16 active ready running `- 6:0:0:0 sdc 8:32 active ready running 挂载 12mkdir /root/testmount /dev/mapper/cqm1 /root/test 7.2 测试将一块网卡断掉，看是否还能使用iSCSI设备 断开网卡ens33 1ifdown ens33 依旧可以在iSCSI设备上写入数据","link":"/2024/02/18/linux%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1/"},{"title":"Prometheus","text":"Prometheus是一个开源的云原生监控系统和时间序列数据库。 一、Prometheus概述Prometheus 作为新一代的云原生监控系统，目前已经有超过 650+位贡献者参与到 Prometheus 的研发工作上，并且超过 120+项的第三方集成。 1.1 Prometheus的优点 提供多维度数据模型和灵活的查询方式，通过将监控指标关联多个 tag，来将监控数据进行任意维度的组合，并且提供简单的 PromQL 查询方式，还提供 HTTP 查询接口，可以很方便地结合 Grafana 等 GUI 组件展示数据。 在不依赖外部存储的情况下，支持服务器节点的本地存储，通过 Prometheus 自带的时序数据库，可以完成每秒千万级的数据存储；不仅如此，在保存大量历史数据的场景中，Prometheus 可以对接第三方时序数据库和 OpenTSDB 等。 定义了开放指标数据标准，以基于 HTTP 的 Pull 方式采集时序数据，只有实现了 Prometheus 监控数据才可以被 Prometheus 采集、汇总、并支持 Push 方式向中间网关推送时序列数据，能更加灵活地应对多种监控场景。 支持通过静态文件配置和动态发现机制发现监控对象，自动完成数据采集。 Prometheus 目前已经支持 Kubernetes、etcd、Consul 等多种服务发现机制。易于维护，可以通过二进制文件直接启动，并且提供了容器化部署镜像。 支持数据的分区采样和联邦部署，支持大规模集群监控。 1.2 Prometheus基本组件 Prometheus Server：是 Prometheus 组件中的核心部分，负责实现对监控数据的获取，存储以及查询。收集到的数据统称为metrics。 Push Gateway：当网络需求无法直接满足时，就可以利用 Push Gateway 来进行中转。可以通过 Push Gateway 将内部网络的监控数据主动 Push 到 Gateway 当中。而 Prometheus Server 则可以采用同样 Pull 的方式从 Push Gateway 中获取到监控数据。 Exporter：主要用来采集数据，并通过 HTTP 服务的形式暴露给 Prometheus Server，Prometheus Server 通过访问该 Exporter 提供的接口，即可获取到需要采集的监控数据。 Alert manager：管理告警，主要是负责实现报警功能。现在grafana也能实现报警功能，所以也慢慢被取代。 1.3 Prometheus数据类型 Counter（计数器类型）：Counter类型的指标的工作方式和计数器一样，只增不减（除非系统发生了重置）。 Gauge（仪表盘类型）：Gauge是可增可减的指标类，可以用于反应当前应用的状态。 Histogram（直方图类型）：主要用于表示一段时间范围内对数据进行采样（通常是请求持续时间或响应大小），并能够对其指定区间以及总数进行统计，通常它采集的数据展示为直方图。 Summary（摘要类型）：主要用于表示一段时间内数据采样结果（通常是请求持续时间或响应大小）。 二、Prometheus安装2.1 Prometheus server安装 Prometheus安装较为简单，下载解压即可 123wget https://github.com/prometheus/prometheus/releases/download/v2.26.0-rc.0/prometheus-2.26.0-rc.0.linux-amd64.tar.gztar -xf prometheus-2.26.0-rc.0.linux-amd64.tar.gzmv prometheus-2.26.0-rc.0.linux-amd64 prometheus prometheus.yml配置文件 12345678910111213141516171819202122232425# 全局配置global: scrape_interval: 15s # 设置抓取间隔，默认为1分钟 evaluation_interval: 15s # 估算规则的默认周期，每15秒计算一次规则，默认1分钟 # scrape_timeout # 默认抓取超时，默认为10s# 报警配置alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# 规则文件列表，使用'evaluation_interval' 参数去抓取rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot;# 抓取配置列表scrape_configs: # 任务名称 - job_name: 'prometheus' # 要被监控的客户端 static_configs: - targets: ['localhost:9090'] 创建Prometheus的用户及数据存储目录 1234groupadd prometheususeradd -g prometheus -s /sbin/nologin prometheusmkdir /root/prometheus/datachown -R prometheus:prometheus /root/prometheus Prometheus的启动很简单，只需要直接启动解压目录的二进制文件Prometheus即可，但是为了更加方便对Prometheus进行管理，这里编写脚本或者使用screen工具来进行启动 123456789vim /root/prometheus/start.sh#!/bin/bashprometheus_dir=/root/prometheus${prometheus_dir}/prometheus --config.file=${prometheus_dir}/prometheus.yml --storage.tsdb.path=${prometheus_dir}/data --storage.tsdb.retention.time=24h --web.enable-lifecycle --storage.tsdb.no-lockfile# --config.file:指定配置文件路径# --storage.tsdb.path:指定tsdb路径# --storage.tsdb.retention.time:指定数据存储时间# --web.enable-lifecycle:类似nginx的reload功能# --storage.tsdb.no-lockfile:如果用k8s的deployment管理需加此项 启动Prometheus后访问 12# nohup英文全称no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。nohup sh start.sh 2&gt;&amp;1 &gt; prometheus.log 用screen工具进行启动 123456789101112yum -y install screen# 进入后台screen# 运行脚本/root/prometheus/prometheus --config.file=/root/prometheus/prometheus.yml --storage.tsdb.path=/root/prometheus/data --storage.tsdb.retention.time=24h --web.enable-lifecycle --storage.tsdb.no-lockfile# 输入CTRL + A + D撤回前台# 查看后台运行的脚本screen -ls# 返回后台screen -r 后台id# 删除后台screen -S 后台id -X quit 2.2 node exporter安装在Prometheus架构中，exporter是负责收集数据并将信息汇报给Prometheus Server的组件。官方提供了node_exporter内置了对主机系统的基础监控。 下载node exporter 123wget https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gztar -xf node_exporter-1.1.2.linux-amd64.tar.gzmv node_exporter-1.1.2.linux-amd64.tar.gz node_exporter 在prometheus.yml中添加被监控主机 12static_configs: - targets: ['localhost:9090','localhost:9100'] 后台启动exporter和重启prometheus 12screen./root/node_exporter/node_exporter 通过curl命令获取收集到的数据key 12curl http://localhost:9100/metrics... 用其中的一个key在Prometheus测试是否被监控 三、Prometheus命令行的使用3.1 计算cpu使用率 通过上图可以知道，linux的cpu使用是分为很多种状态的，例如用户态user，空闲态idle。 要计算cpu的使用率有两种粗略的公式： 除去idle状态的所有cpu状态时间之和 / cpu时间总和 100% - （idle状态 / cpu时间总和） 但这两种方式都存在两个问题： 如何计算某一时间段的cpu使用率？例如精确到每一分钟。 实际工作中cpu大多数都是多核的，node exporter截取到的数据精确到了每个核，如何监控所有核加起来的数据？ Prometheus提供了许多的函数，其中 increase 和 sum 就很好的解决了以上两个问题。 提取cpu的key，即node_cpu_seconds_total 把idle空闲时间和总时间过滤出来，在Prometheus中使用{}进行过滤 1node_cpu_seconds_total{mode='idle'} 使用increase函数取一分钟内的增量 1increase(node_cpu_seconds_total{mode='idle'}[1m]) 使用sum函数将每个核的数整合起来 1sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) 到这里又出现一个问题，sum函数会将所有数据整合起来，不光将一台机器的所有cpu加到一起，也将所有机器的cpu都加到了一起，最终显示的是集群cpu的总平均值，by(instance)可以解决这个问题。 1sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) by(instance) 这样就得到了空闲时cpu的数据了，用上边第一个公式即可得到单台主机cpu在一分钟内的使用率。 1(1 - ((sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) by(instance)) / (sum(increase(node_cpu_seconds_total[1m])) by(instance)))) * 100 3.2 计算内存使用率内存使用率公式为 = (available / total) * 100 1(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 3.3 rate函数rate函数是专门搭配counter类型数据使用的函数，功能是按照设置的一个时间段，取counter在这个时间段中平均每秒的增量。 举个栗子，假设我们要取ens33这个网卡在一分钟内字节的接受数量，假如一分钟内接收到的是1000bytes，那么平均每秒接收到就是1000bytes / 1m * 60s ≈ 16bytes/s。 1rate(node_network_receive_bytes_total{device='ens33'}[1m]) 如果是五分钟的话即为5000bytes / 5m * 60s ≈ 16bytes/s，结果是一样的，但曲线图就不一样了，上图为一分钟，下图为五分钟，因为五分钟的密度要更底，所以可以看到五分钟的曲线图更加平缓。 rate和increase的概念有些类似，但rate取的是一段时间增量的平均每秒数量，increase取的是一段时间增量的总量，即： rate(1m)：总量 / 60s increase(1m)：总量 3.4 sum函数sum函数就是将收到的数据全部进行整合。 假如一个集群里有20台服务器，分别为5台web服务器，10台db服务器，还有5台其他服务的服务器，这时候sum就可以分为三条曲线来代表不同功能服务器的总和数据。 3.5 topk函数topk函数的作用就是取前几位的最高值。 3.6 count函数count函数的作用是把符合条件的数值进行整合。 假如我们要查看集群中cpu使用率超过80%的主机数量的话 1count((1 - ((sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) by(instance)) / (sum(increase(node_cpu_seconds_total[1m])) by(instance)))) * 100 &gt; 80) 四、Push gatewayPush gateway实际上就是一种被动推送数据的方式，与exporter主动获取不同。 4.1 Push gateway安装 下载安装Push gateway 123wget https://github.com/prometheus/pushgateway/releases/download/v1.4.0/pushgateway-1.4.0.linux-amd64.tar.gztar -xf pushgateway-1.4.0.linux-amd64.tar.gzmv pushgateway-1.4.0.linux-amd64 pushgateway 后台运行Push gateway 12screen./root/pushgateway/pushgateway 在prometheus.yml中加上 123- job_name: 'pushgateway' static_configs: - targets: ['localhost:9091'] 4.2 自定义编写脚本由于Push gateway自己本身是没有任何抓取数据的功能的，所以用户需要自行编写脚本来抓取数据。 举个例子：编写脚本抓取 TCP waiting_connection 的数量 编写自定义脚本 123456789101112131415161718192021222324252627#!/bin/bash# 获取监控主机名instance_name=`hostname -f | cut -d'.' -f1`# 如果主机名为localhost，则退出if [ $instance_name == &quot;localhost&quot; ]then echo &quot;不能监控主机名为localhost的主机&quot; exit 1fi#---# 获取TCP CONNECTED数量# 抓取TCP CONNECTED数量，定义为一个新keylable_tcp_connected=&quot;count_netstat_connected_connections&quot;count_netstat_connected_connections=`netstat -an | grep 'CONNECTED' | wc -l`# 上传至pushgatewayecho &quot;$lable_tcp_connected $count_netstat_connected_connections&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name#---# 该脚本是通过post的方式将key推送给pushgateway# http://localhost:9091 即推送给哪台pushgateway主机# job/pushgateway 即推送给prometheus.yml中定义的job为pushgateway的主机# instance/$instance_name 推送后显示的主机名 因为脚本都是运行一次后就结束了，可以配合crontab反复运行 12345crontab -e# 每分钟执行一次脚本* * * * * sh /root/pushgateway/node_exporter_shell.sh# 每10s执行一次脚本* * * * * sh /root/pushgateway/node_exporter_shell.sh 4.3 编写抓取ping丢包和延迟时间数据在node_exporter_shell.sh中加入 123456789101112131415161718192021#---# 获取ping某网站丢包率和延迟时间site_address=&quot;www.baidu.com&quot;# 获取丢包率和延迟时间，定义为两个新keylable_ping_packet_loss=&quot;ping_packet_loss&quot;ping_packet_loss_test=`ping -c3 $site_address | awk 'NR==7{print $6}'`# 字符串截取，%?为去除最后一个字符ping_packet_loss=`echo ${ping_packet_loss_test%?}`lable_ping_time=&quot;ping_time&quot;ping_time_test=`ping -c3 $site_address | awk 'NR==7{print $10}'`# 字符串截取，%??为去除最后两个字符ping_time=`echo ${ping_time_test%??}`# 上传至push_ping_timegatewayecho &quot;$lable_ping_packet_loss $ping_packet_loss&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_nameecho &quot;$lable_ping_time $ping_time&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name#--- 五、Grafana的使用Grafana是一款用Go语言开发的开源数据可视化工具，可以做数据监控和数据统计，带有告警功能。 5.1 Grafana安装1234wget https://dl.grafana.com/oss/release/grafana-7.5.1-1.x86_64.rpmyum -y install grafana-7.5.1-1.x86_64.rpmsystemctl start grafana-serversystemctl enable grafana-server 5.2 设置数据源 Grafana -&gt; Configuration -&gt; Date Sources -&gt; Prometheus New dashboard 添加一个监控和CPU内存使用率的仪表盘 5.3 json备份和还原 备份：dashboard -&gt; Settings -&gt; JSON Model，将里面内容保存为json文件 恢复：Create -&gt; import 5.4 Grafana实现报警功能 配置Grafana文件 123456789101112131415# 安装依赖和图形显示插件yum -y install libatk-bridge* libXss* libgtk*grafana-cli plugins install grafana-image-renderer# 修改配置vim /etc/grafana/grafana.inienabled = truehost = smtp.163.com:25# 发送报警邮件的邮箱user = chenqiming13@163.com# 授权码password = QXQALYMTRYRWIOOSskip_verify = truefrom_address = chenqiming13@163.comfrom_name = Grafanasystemctl restart grafana-server 创建报警规则 针对具体监控项，设置发送邮件阈值等，这里设置为发现超过阈值起5分钟后触发报警 ![Grafana实现报警功能七](Grafana实现报警功能七.png 六、Prometheus + Grafana实际案例6.1 predict_linear函数实现硬盘监控硬盘使用率公式为：（（总容量 - 剩余容量）/ 总容量）* 100，在Prometheus中表示为 1((node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes) * 100 通过df -m可以看出计算出来的值是正确的 Prometheus提供了一个predict_linear函数可以预计多长时间磁盘爆满，例如当前这1个小时的磁盘可用率急剧下降，这种情况可能导致磁盘很快被写满，这时可以使用该函数，用当前1小时的数据去预测未来几个小时的状态，实现提前报警。 12# 该式子表示用当前1小时的值来预测未来4小时后如果根目录下容量小于0则触发报警predict_linear(node_filesystem_free_bytes {mountpoint =&quot;/&quot;}[1h], 4*3600) &lt; 0 在Grafana添加监控硬盘使用率和预测硬盘使用率的仪表盘 6.2 监控硬盘IO公式为：（读取时间 / 写入时间）/ 1024 / 1024，用rate函数取一分钟内读和写的字节增长率来计算，用Prometheus表示为 1((rate(node_disk_read_bytes_total[1m]) + rate(node_disk_written_bytes_total[1m])) / 1024 / 1024) &gt; 0 6.3 监控TCP_WAIT状态的数量在被监控主机上编写监控脚本 12345678910111213141516171819202122#!/bin/bash# 获取监控主机名instance_name=`hostname -f | cut -d'.' -f1`# 如果主机名为localhost，则退出if [ $instance_name == &quot;localhost&quot; ]then echo &quot;不能监控主机名为localhost的主机&quot; exit 1fi#---# 获取TCP WAIT数量# 抓取TCP WAIT数量，定义为一个新keylable_tcp_wait=&quot;count_netstat_wait_connections&quot;count_netstat_wait_connections=`netstat -an | grep 'WAIT' | wc -l`# 上传至pushgatewayecho &quot;$lable_tcp_wait $count_netstat_wait_connections&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name#--- 6.4 监控文件描述符使用率在linux中，每当进程打开一个文件时，系统就会为其分配一个唯一的整型文件描述符，用来标识这个文件，每个进程默认打开的文件描述符有三个，分别为标准输入、标准输出、标准错误，即stdin、stout、steer，用文件描述符来表示为0、1、2。 用命令可以查看目前系统的最大文件描述符限制，一般默认设置是1024。 1ulimit -n 文件描述符使用率公式为：（已分配的文件描述符数量 / 最大文件描述符数量）* 100，在Prometheus中则表示为 1(node_filefd_allocated / node_filefd_maximum) * 100 6.5 网络延迟和丢包率监控前面我们采用的都是简单的ping + ip地址来进行测试，实际上这样测试发出去的icmp数据包是非常小的，只适合用来测试网络是否连通，因此用以下命令来进行优化： 12345ping -q ip地址 -s 500 -W 1000 -c 100-q:不显示指令执行过程，开头和结尾的相关信息除外。-s:设置数据包的大小。-W:在等待 timeout 秒后开始执行。-c:设置完成要求回应的次数。 6.6 使用Pageduty实现报警Pagerduty是一套付费监控报警系统，经常作为SRE/运维人员的监控报警工具，可以和市面上常见的监控工具直接整合。 创建新service 在Grafana新建报警渠道，并在仪表盘中设置为Pageduty报警 设置报警信息 查看是否收到报警 当问题解决可以点击已解决","link":"/2024/02/18/prometheus/"},{"title":"Web","text":"一、Apache1.1 Apache介绍Apache HTTP Server（简称Apache）是Apache软件基金会的一个开放源码的网页服务器，是世界使用排名第一的Web服务器软件。它可以运行在几乎所有广泛使用的计算机平台上，由于其跨平台和安全性被广泛使用，是最流行的Web服务器端软件之一。它快速、可靠并且可通过简单的API扩充，将Perl/Python等解释器编译到服务器中。 Apache HTTP服务器是一个模块化的服务器，源于NCSAhttpd服务器，经过多次修改，成为世界使用排名第一的Web服务器软件。 Apache官方文档：http://httpd.apache.org/docs/ 1.2 通过脚本源码安装Apache 编写安装脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189useradd -s /sbin/nologin -r wwwvim apache-install.sh#!/bin/bashapr_version=1.7.0apr_iconv_version=1.2.2apr_util_version=1.6.1apache_version=2.4.46#检查function check(){ #检查是否为root用户 if [ $USER != 'root' ] then echo -e &quot;\\e[1;31m error:need to be root so that \\e[0m&quot; exit 1 fi #检查是否安装了wget if [ `rpm -qa | grep wget | wc -l` -lt 1 ] then echo -e &quot;\\e[1;31m error:not found wget \\e[0m&quot; exit 1 fi}#安装前准备function install_pre(){ #安装依赖 if [ ! `yum -y install zlib-devel pcre-devel libxml2 expat-devel &amp;&gt; /dev/null` ] then echo -e &quot;\\e[1;31m error:yum install dependency package failed \\e[0m&quot; exit 1 fi #下载apr cd /usr/local if [ ! `wget https://downloads.apache.org/apr/apr-${apr_version}.tar.bz2 &amp;&gt; /dev/null` ] then tar -xf apr-${apr_version}.tar.bz2 if [ ! -d apr-${apr_version} ] then echo -e &quot;\\e[1;31m error:not found apr-${apr_version} \\e[0m&quot; exit 1 else cd apr-${apr_version} fi else echo -e &quot;\\e[1;31m error:Failed to download apr-${apr_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装apr echo &quot;apr configure...&quot; ./configure --prefix=/usr/local/apr &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;apr make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m apr installed successfully \\e[0m&quot; else echo -e &quot;\\e[1;31m apr installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:apr configure failed \\e[0m&quot; exit 1 fi #下载apr-iconv cd /usr/local if [ ! `wget https://www.apache.org/dist/apr/apr-iconv-${apr_iconv_version}.tar.bz2 &amp;&gt; /dev/null` ] then tar -xf apr-iconv-${apr_iconv_version}.tar.bz2 if [ ! -d apr-iconv-${apr_iconv_version} ] then echo -e &quot;\\e[1;31m error:not found apr-iconv-${apr_iconv_version} \\e[0m&quot; exit 1 else cd apr-iconv-${apr_iconv_version} fi else echo -e &quot;\\e[1;31m error:Failed to download apr-iconv-${apr_iconv_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装apr-iconv echo &quot;apr-iconv configure...&quot; ./configure --prefix=/usr/local/apr-iconv --with-apr=/usr/local/apr &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;apr-iconv make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m apr-iconv installed successfully \\e[0m&quot; else echo -e &quot;\\e[1;31m apr-iconv installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:apr-iconv configure failed \\e[0m&quot; exit 1 fi #下载apr-util cd /usr/local if [ ! `wget https://www.apache.org/dist/apr/apr-util-${apr_util_version}.tar.bz2 &amp;&gt; /dev/null` ] then tar -xf apr-util-${apr_util_version}.tar.bz2 if [ ! -d apr-util-${apr_util_version} ] then echo -e &quot;\\e[1;31m error:not found apr-util-${apr_util_version} \\e[0m&quot; exit 1 else cd apr-util-${apr_util_version} fi else echo -e &quot;\\e[1;31m error:Failed to download apr-util-${apr_util_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装apr-util echo &quot;apr-util configure...&quot; ./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr/ &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;apr-util make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m apr-util installed successfully \\e[0m&quot; else echo -e &quot;\\e[1;31m apr-util installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:apr-util configure failed \\e[0m&quot; exit 1 fi}#下载安装Apachefunction apache_install(){ #下载Apache cd /usr/local if [ ! `wget https://downloads.apache.org/httpd/httpd-${apache_version}.tar.gz &amp;&gt; /dev/null` ] then tar -xf httpd-${apache_version}.tar.gz if [ ! -d httpd-${apache_version} ] then echo -e &quot;\\e[1;31m error:not found httpd-${apache_version} \\e[0m&quot; exit 1 else cd httpd-${apache_version} fi else echo -e &quot;\\e[1;31m error:Failed to download httpd-${apache_version} \\e[0m&quot; exit 1 fi #安装Apache echo &quot;Apache configure...&quot; ./configure --prefix=/usr/local/apache --enable-mpms-shared=all --with-mpm=event --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr-util --enable-so --enable-remoteip --enable-proxy --enable-proxy-fcgi --enable-proxy-uwsgi --enable-deflate=shared --enable-expires=shared --enable-rewrite=shared --enable-cache --enable-file-cache --enable-mem-cache --enable-disk-cache --enable-static-support --enable-static-ab --disable-userdir --enable-nonportable-atomics --disable-ipv6 --with-sendfile &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;Apache make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m Apache installed sucessfully \\e[0m&quot; else echo -e &quot;\\e[1;31m Apache installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m Apache configure failed \\e[0m&quot; exit 1 fi}checkinstall_preapache_install 编写启动脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/bin/bashapache_doc=/usr/local/apache/binapache_pid=/usr/local/apache/logs/httpd.pidfunction apache_start(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -gt 1 ] &amp;&amp; [ -f $apache_pid ] then echo -e &quot;Apache [\\e[1;32m running \\e[0m]&quot; exit 1 elif [ $apache_num -eq 1 ] &amp;&amp; [ -f $apache_pid ] then killall httpd fi cd /usr/local/apache/bin;./apachectl echo -e &quot;start Apache [\\e[1;32m OK \\e[0m]&quot;}function apache_stop(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -eq 1 ] then echo -e &quot;Apache [\\e[1;31m stopping \\e[0m]&quot; else killall httpd echo -e &quot;stop Apache [\\e[1;32m OK \\e[0m]&quot; fi}function apache_restart(){ cd /usr/local/apache/bin;./apachectl restart echo -e &quot;restart Apache [\\e[1;32m OK \\e[0m]&quot;}function apache_status(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then echo -e &quot;Apache [\\e[1;32m running \\e[0m]&quot; else echo -e &quot;Apache [\\e[1;31m stopping \\e[0m]&quot; fi}function apache_reload(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then cd /usr/local/apache/bin;./apachectl graceful echo -e &quot;reload Apache [\\e[1;32m OK \\e[0m]&quot; else echo -e &quot;Apache [\\e[1;31m stopping \\e[0m]&quot; fi}case $1 instart) apache_start;;stop) apache_stop;;restart) apache_restart;;status) apache_status;;reload) apache_reloadesac 1.3 多处理模块MPMApache HTTP 服务器被设计为一个功能强大，并且灵活的 web 服务器， 可以在很多平台与环境中工作。不同平台和不同的环境往往需要不同 的特性，或可能以不同的方式实现相同的特性最有效率。Apache 通过模块化的设计来适应各种环境。这种设计允许网站管理员通过在 编译时或运行时，选择哪些模块将会加载在服务器中，来选择服务器特性。 实际上就是用来接受请求和处理请求的。 Apache的三种工作方式： Prefork MPM：使用多个进程，每个进程只有一个线程，每个进程再某个确定的时间只能维持一个连接，有点是稳定，缺点是内存消耗过高。 ![Prefork MPM](Prefork MPM.png) Worker MPM：使用多个进程，每个进程有多个线程，每个线程在某个确定的时间只能维持一个连接，内存占用比较小，是个大并发、高流量的场景，缺点是一个线程崩溃，整个进程就会连同其任何线程一起挂掉。 ![Worker MPM](Worker MPM.png) Event MPM：使用多进程多线程+epoll的模式。 ![Event MPM](Event MPM.png) 1.4 虚拟主机默认情况下，一个web服务器只能发布一个默认网站，也就是只能发布一个web站点，对于大网站来说还好，但对于访问量较少的小网站那就显得有点浪费了。 而虚拟主机就可以实现在一个web服务器上发布多个站点，分为基于IP地址、域名和端口三种。 Apache的虚拟主机和默认网站不能够同时存在，如果设置了虚拟主机那么默认网站也就失效了，需要在用虚拟主机发布默认站点才可解决。 基于IP：基于IP的虚拟主机需要耗费大量的IP地址，只适合IP地址充足的环境。 基于端口：需要耗费较多的端口，适合私网环境。 基于域名：需要耗费较多的域名，适合公网环境。 1.4.1 基于IP的虚拟主机 在主配文件中调用虚拟主机文件 123vim /usr/local/apache/conf/httpd.conf# Virtual hostsInclude conf/extra/httpd-vhosts.conf 修改虚拟主机文件 123456789101112131415161718192021# 添加一个逻辑网卡，重启即失效ifconfig eth0:1 192.168.88.100/24 upvim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost 49.232.160.75:80&gt; # 管理员邮箱 # ServerAdmin webmaster@dummy-host.example.com # web目录 DocumentRoot &quot;/usr/local/apache/htdocs/web1&quot; # 域名 # ServerName dummy-host.example.com # 给域名起别名，起到重定向作用 # ServerAlias www.dummy-host.example.com # 错误日子 # ErrorLog &quot;logs/dummy-host.example.com-error_log&quot; # 访问日志 # CustomLog &quot;logs/dummy-host.example.com-access_log&quot; common&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.88.100:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web2&quot;&lt;/VirtualHost&gt; 创建站点目录和文件 1234mkdir /usr/local/apache/htdocs/web{1..2}echo 'this is web1' &gt; /usr/local/apache/htdocs/web1/index.htmlecho 'this is web2' &gt; /usr/local/apache/htdocs/web2/index.html./apache start 测试 1.4.2 基于端口的虚拟主机 修改虚拟主机文件 12345678&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web1&quot;&lt;/VirtualHost&gt;Listen 81&lt;VirtualHost *:81&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web2&quot;&lt;/VirtualHost&gt; 测试 1.4.3 基于域名的虚拟主机 修改虚拟主机文件 123456789&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web1&quot; ServerName www.cqm1.com&lt;/VirtualHost&gt;&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web2&quot; ServerName www.cqm2.com&lt;/VirtualHost&gt; 测试 1.5 LAMPLAMP：Linux + Apache + Mysql + PHP 作用就是构建一个PHP业务环境，用来发布PHP网站。 1.5.1 Mysql通过脚本源码安装 编写安装脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208#!/bin/bashmysql_version=5.7.35install_dir=/optdata_dir=/datawget_url=&quot;https://mirrors.tuna.tsinghua.edu.cn/mysql/downloads/MySQL-5.7/mysql-${mysql_version}-linux-glibc2.12-x86_64.tar.gz&quot;function loginfo(){ if [[ $? -eq 0 ]];then echo -e &quot;\\033[32m[INFO][$(date +&quot;%F %T&quot;)] $1 succeed! \\033[0m&quot; else echo -e &quot;\\033[31m[ERROR][$(date +&quot;%F %T&quot;)] $1 failed! \\033[0m&quot; fi}function mysql_install(){ echo -e &quot;\\033[32mBegin install mysql V${mysql_version} ...\\033[0m&quot; # 安装依赖 sudo yum -y install libaio &gt;/dev/null 2&gt;&amp;1 loginfo &quot;libaio install&quot; # 下载mysql echo -e &quot;\\033[32mBegin download mysql V${mysql_version} ...\\033[0m&quot; curl -O $wget_url &gt;/dev/null 2&gt;&amp;1 mv ./mysql-${mysql_version}-linux-glibc2.12-x86_64.tar.gz $install_dir loginfo &quot;mysql software download&quot; # 解压缩mysql sudo tar -xf $install_dir/mysql-${mysql_version}-linux-glibc2.12-x86_64.tar.gz -C $install_dir loginfo &quot;mysql software decompression&quot; # 创建配置文件目录和数据目录 if [[ -d $install_dir/mysql ]];then rm -rf $install_dir/mysql fi sudo ln -s $install_dir/mysql-${mysql_version}-linux-glibc2.12-x86_64 $install_dir/mysql loginfo &quot;create mysql config dir soft link&quot; if [[ -d $data_dir/mysql ]];then rm -rf $data_dir/mysql fi sudo mkdir -p $data_dir/mysql loginfo &quot;create mysql data dir&quot; # 修改启动脚本 sudo sed -i &quot;46s#basedir=#basedir=${install_dir}/mysql#&quot; ${install_dir}/mysql/support-files/mysql.server sudo sed -i &quot;47s#datadir=#datadir=${data_dir}/mysql#&quot; ${install_dir}/mysql/support-files/mysql.server sudo cp ${install_dir}/mysql/support-files/mysql.server /etc/init.d/mysqld sudo chmod 755 /etc/init.d/mysqld # 创建用户组及用户 if ! grep -q '^mysql:' /etc/group then sudo groupadd mysql loginfo &quot;create user mysql&quot; fi if ! grep -q '^mysql:' /etc/passwd then sudo useradd -r -g mysql -s /bin/false mysql loginfo &quot;create group mysql&quot; fi # 授权 sudo chown -R mysql:mysql $install_dir/mysql sudo chown -R mysql:mysql $data_dir/mysql # 为二进制文件创建软连接 if [ ! -f /usr/bin/mysql ] then sudo ln -s /opt/mysql/bin/mysql /usr/bin/ fi # 创建配置文件 if [ -f /etc/my.cnf ] then sudo rm -f /etc/my.cnf fi sudo bash -c &quot;cat &gt;&gt; /etc/my.cnf&quot; &lt;&lt;EOF[mysqld]datadir = /data/mysqlbasedir = /opt/mysql#tmpdir = /data/mysql/tmp_mysqlport = 3306socket = /data/mysql/mysql.sockpid-file = /data/mysql/mysql.pidmax_connections = 8000max_connect_errors = 100000max_user_connections = 3000check_proxy_users = onmysql_native_password_proxy_users = onlocal_infile = OFFsymbolic-links = FALSEgroup_concat_max_len = 4294967295max_join_size = 18446744073709551615max_execution_time = 20000lock_wait_timeout = 60autocommit = 1lower_case_table_names = 1thread_cache_size = 64disabled_storage_engines = &quot;MyISAM,FEDERATED&quot;character_set_server = utf8mb4character-set-client-handshake = FALSEcollation_server = utf8mb4_general_ciinit_connect = 'SET NAMES utf8mb4'transaction-isolation = &quot;READ-COMMITTED&quot;skip_name_resolve = ONexplicit_defaults_for_timestamp = ONlog_timestamps = SYSTEMlocal_infile = OFFevent_scheduler = OFFquery_cache_type = OFFquery_cache_size = 0sql_mode = NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZEROlog_error = /data/mysql/mysql.errslow_query_log = ONslow_query_log_file = /data/mysql/slow.loglong_query_time = 1general_log = OFFgeneral_log_file = /data/mysql/general.logexpire_logs_days = 99log-bin = /data/mysql/mysql-binlog-bin-index = /data/mysql/mysql-bin.indexmax_binlog_size = 500Mbinlog_format = mixedbinlog_rows_query_log_events = ONbinlog_cache_size = 128kbinlog_stmt_cache_size = 128klog-bin-trust-function-creators = 1max_binlog_cache_size = 2Gmax_binlog_stmt_cache_size = 2Grelay_log = /data/mysql/relayrelay_log_index = /data/mysql/relay.indexmax_relay_log_size = 500Mrelay_log_purge = ONrelay_log_recovery = ONserver_id = 1read_buffer_size = 1Mread_rnd_buffer_size = 2Msort_buffer_size = 64Mjoin_buffer_size = 64Mtmp_table_size = 64Mmax_allowed_packet = 128Mmax_heap_table_size = 64Mconnect_timeout = 43200wait_timeout = 43200back_log = 512interactive_timeout = 300net_read_timeout = 30net_write_timeout = 30skip_external_locking = ONkey_buffer_size = 16Mbulk_insert_buffer_size = 16Mconcurrent_insert = ALWAYSopen_files_limit = 65000table_open_cache = 16000table_definition_cache = 16000default_storage_engine = InnoDBdefault_tmp_storage_engine = InnoDBinternal_tmp_disk_storage_engine = InnoDB[client]socket = /data/mysql/mysql.sockdefault_character_set = utf8mb4[mysql]default_character_set = utf8mb4[ndatad default]TransactionDeadLockDetectionTimeOut = 20000EOF sudo chown -R mysql:mysql /etc/my.cnf loginfo &quot;configure my.cnf&quot; # 创建SSL证书 # sudo mkdir -p ${install_dir}/mysql/ca-pem/ # sudo ${install_dir}/mysql/bin/mysql_ssl_rsa_setup -d ${install_dir}/mysql/ca-pem/ --uid=mysql # sudo chown -R mysql:mysql ${install_dir}/mysql/ca-pem/ # sudo bash -c &quot;cat &gt;&gt; ${data_dir}/mysql/init_file.sql&quot; &lt;&lt;EOF# set global sql_safe_updates=0;# set global sql_select_limit=50000;# EOF # sudo chown -R mysql:mysql ${data_dir}/mysql/init_file.sql # sudo chown -R mysql:mysql /etc/init.d/mysqld # 初始化 ${install_dir}/mysql/bin/mysqld --initialize --user=mysql --basedir=${DEPLOY_PATH}/mysql --datadir=/data/mysql loginfo &quot;initialize mysql&quot; # 客户端环境变量 echo &quot;export PATH=\\$PATH:${install_dir}/mysql/bin&quot; | sudo tee /etc/profile.d/mysql.sh source /etc/profile.d/mysql.sh loginfo &quot;configure envirement&quot; # 获取初始密码 mysql_init_passwd=$(grep 'A temporary password is generated' ${data_dir}/mysql/mysql.err | awk '{print $NF}') # 启动服务 chkconfig --add mysqld sudo systemctl start mysqld loginfo &quot;start mysqld&quot; # 修改密码 mysql --connect-expired-password -uroot -p${mysql_init_passwd} -e 'alter user user() identified by &quot;toortoor&quot;;' &gt;/dev/null 2&gt;&amp;1 loginfo &quot;edit mysql root password&quot;}mysql_install 1.5.2 PHP通过脚本源码安装 编写安装脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176#!/bin/bashcmake_version=3.22.0libzip_version=1.8.0php_version=7.4.16#检查function check(){ #检查是否为root用户 if [ $USER != &quot;root&quot; ] then echo -e &quot;\\e[1;31m error:need to be root so that \\e[0m&quot; exit 1 fi #检查是否安装了wget if [ `rpm -qa | grep wget | wc -l` -lt 1 ] then echo -e &quot;\\e[1;31m error:not found wget \\e[0m&quot; exit 1 fi}#安装前准备function pre(){ #安装依赖包 if [ ! `yum -y install gcc-c++ libxml2 libxml2-devel openssl openssl-devel bzip2 bzip2-devel libcurl libcurl-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel gmp gmp-devel libmcrypt libmcrypt-devel readline readline-devel libxslt libxslt-devel gd net-snmp-* sqlite-devel oniguruma-devel &amp;&gt; /dev/null` ] then echo -e &quot;\\e[1;31m error:yum install dependency package failed \\e[0m&quot; exit 1 fi #下载最新版cmake cd /usr/local if [ ! `wget https://github.com/Kitware/CMake/releases/download/v${cmake_version}/cmake-${cmake_version}.tar.gz &amp;&gt; /dev/null` ] then tar -xf cmake-${cmake_version}.tar.gz if [ ! -d cmake-${cmake_version} ] then echo -e &quot;\\e[1;31m error:no found cmake-${cmake_version} \\e[0m&quot; exit 1 else cd cmake-${cmake_version} fi else echo -e &quot;\\e[1;31m error:Failed to download cmake-${cmake_version} \\e[0m&quot; exit 1 fi #安装cmake echo &quot;cmake configure...&quot; ./configure &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;cmake make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m cmake installed sucessfully \\e[0m&quot; else echo -e &quot;\\e[1;31m cmake installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:cmake configure failed \\e[0m&quot; exit 1 fi #下载libzip1.1以上版本 cd /usr/local if [ ! `wget --no-check-certificate https://libzip.org/download/libzip-${libzip_version}.tar.gz &amp;&gt; /dev/null` ] then echo &quot;tar libzip...&quot; tar -xf libzip-${libzip_version}.tar.gz if [ ! -d libzip-${libzip_version} ] then echo -e &quot;\\e[1;31m error:not found libzip-${libzip_version} \\e[0m&quot; exit 1 else cd libzip-${libzip_version} fi else echo -e &quot;\\e[1;31m error:Failed to download libzip-${libzip_version}.tar.gz \\e[0m&quot; exit 1 fi #安装libzip mkdir build;cd build echo &quot;cmake libzip...&quot; cmake .. &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;make &amp;&amp; make install libzip...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m libzip install sucessfully \\e[0m&quot; echo -e '/usr/local/lib64\\n/usr/local/lib\\n/usr/lib\\n/usr/lib64'&gt;&gt; /etc/ld.so.conf ldconfig -v &amp;&gt; /dev/null else echo -e &quot;\\e[1;31m error:libzip install failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:lipzip cmake failed \\e[0m&quot; exit 1 fi}function php_install(){ #下载php cd /usr/local if [ ! `wget https://www.php.net/distributions/php-${php_version}.tar.bz2 &amp;&gt; /dev/null` ] then echo &quot;tar php...&quot; tar -xf php-${php_version}.tar.bz2 if [ ! -d php-${php_version} ] then echo -e &quot;\\e[1;31m error:not found php-${php_version} \\e[0m&quot; exit 1 else cd php-${php_version} fi else echo -e &quot;\\e[1;31m error:Failed to download php-${php_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装php echo &quot;configure php...&quot; #要php以apache模块运行需加上--with-apxs2=/usr/localapache/bin/apxs参数 ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-mysqli=mysqlnd --enable-pdo --with-pdo-mysql=mysqlnd --with-iconv-dir=/usr/local/ --enable-fpm --with-fpm-user=www --with-fpm-group=www --with-pcre-regex --with-zlib --with-bz2 --enable-calendar --disable-phar --with-curl --enable-dba --with-libxml-dir --enable-ftp --with-gd --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --enable-gd-jis-conv --with-mhash --enable-mbstring --enable-opcache=yes --enable-pcntl --enable-xml --disable-rpath --enable-shmop --enable-sockets --enable-zip --enable-bcmath --with-snmp --disable-ipv6 --with-gettext --disable-rpath --disable-debug --enable-embedded-mysqli --with-mysql-sock=/var/lib/mysql/ &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;make &amp;&amp; make install php...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m php install sucessfully \\e[0m&quot; else echo -e &quot;\\e[1;31m php install failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m configure php failed \\e[0m&quot; exit 1 fi}function php_set(){ if [ ! -f /usr/local/php-${php_version}/sapi/fpm/php-fpm.service ] then echo -e &quot;\\e[1;31m No found php-fpm.service \\e[0m&quot; exit 1 else cp /usr/local/php-${php_version}/sapi/fpm/php-fpm.service /etc/systemd/system if [ `echo $?` -ne 0 ] then echo -e &quot;\\e[1;31m Copy php-fpm.service failed \\e[0m&quot; exit 1 else sed -i '/PrivateTmp=true/a\\ProtectSystem=false' /etc/systemd/system/php-fpm.service systemctl daemon-reload echo -e &quot;\\e[1;32m php set sucessfully \\e[0m&quot; fi fi}checkprephp_installphp_set 配置PHP 123456789101112131415161718192021222324cd /usr/local/php/etccp php-fpm.conf.default php-fpm.confcp php-fpm.d/www.conf.default php-fpm.d/www.confegrep -v '^;|^$' php-fpm.conf[global]pid = run/php-fpm.piderror_log = log/php-fpm.logdaemonize = yesinclude=/usr/local/php/etc/php-fpm.d/*.confegrep -v '^;|^$' php-fpm.d/www.conf[www]user = wwwgroup = wwwlisten = 127.0.0.1:9000listen.owner = wwwlisten.group = wwwlisten.mode = 0660pm = dynamicpm.max_children = 5pm.start_servers = 2pm.min_spare_servers = 1pm.max_spare_servers = 3 启动 1systemctl start php-fpm 1.5.3 PHP作为Apache模块运行 在apache主配置文件中调用子配置文件 12vim /usr/local/apache/conf/httpd.confinclude conf/extra/php.conf 配置子配置文件 123vim /usr/local/apache/conf/extra/php.conLoadModule php7_module modules/libphp7.soAddType application/x-httpd-php .php 配置虚拟主机 1234vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt; 写web目录 12345echo 'this is cqm web' &gt; /usr/local/apache/htdocs/web/index.htmlvim /usr/local/apache/htdocs/web/phpinfo.php&lt;?phpphpinfo()?&gt; 测试 1.5.4 PHP作为独立服务运行PHP作为独立服务运行有两种模式： TCP socket模式 UNIX socket模式 TCP socket模式 修改www.conf文件 12vim /usr/local/php/etc/php-fpm.d/www.conflisten = 127.0.0.1:9000 配置虚拟主机文件 123456789101112131415161718vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;&lt;Directory &quot;/usr/local/apache/htdocs/web&quot;&gt; Options Indexes FollowSymLinks AllowOverride None Require all granted&lt;/Directory&gt;&lt;IfModule dir_module&gt; DirectoryIndex index.php index.html&lt;/IfModule&gt;&lt;FilesMatch \\.php$&gt; SetHandler &quot;proxy:fcgi://127.0.0.1:9000&quot;&lt;/FilesMatch&gt; 在apache主配文件添加关联 12vim /usr/local/apache/conf/httpd.confinclude conf/extra/php-fpm.conf 配置子配文件 1234vim /usr/local/apache/conf/php-fpm.conf# 载入需要的模块LoadModule proxy_module modules/mod_proxy.soLoadModule proxy_fcgi_module modules/mod_proxy_fcgi.so UNIX socket模式 修改www.conf文件 12vim /usr/local/php/etc/php-fpm.d/www.conflisten = /usr/local/php/etc/php-fpm.socket 配置虚拟主机 123&lt;FilesMatch \\.php$&gt; SetHandler &quot;proxy:unix:/usr/local/php/etc/php-fpm.socket|fcgi://localhost/&quot;&lt;/FilesMatch&gt; 1.6 Apache常用模块1.6.1 长连接HTTP采用TCP进行传输，是面向连接的协议，每完成一次请求就要经历以下过程： 三次握手 发起请求 响应请求 四次挥手 那么N个请求就要建立N次连接，如果希望用户能够更快的拿到数据，服务器的压力降到最低，那么靠长连接就可以解决。 长连接实际上就是优化了TCP连接。 Apache默认开启了长连接，持续时间为5秒，在httpd-default.conf中可以定义。 1234567vim /usr/local/apache/conf/extra/httpd-default.conf# 开启长连接KeepAlive On# 限制每个连接允许的请求数MaxKeepAliveRequests 500# 长连接时间KeepAliveTimeout 5 1.6.2 静态缓存用户每次访问网站都会将页面中的所有元素都请求一遍，全部下载后通过浏览器渲染，展示到浏览器中。但是，网站中的某些元素我们一般都是固定不变的，比如logo、框架文件等。用户每次访问都需要加载这些元素。这样做好处是保证了数据的新鲜，可是这些数据不是常变化的，很久才变化一次。每次都请求、下载浪费了用户时间和公司带宽。 所以我们通过静态缓存的方式，将这些不常变化的数据缓存到用户本地磁盘，用户以后再访问这些请求，直接从本地磁盘打开加载，这样的好处是加载速度快，且节约公司带宽及成本。 在apache主配文件中加载缓存模块 12vim /usr/local/apache/conf/httpd.confLoadModule expires_module modules/mod_expires.so 修改虚拟主机文件调用模块 1234567891011121314151617181920vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; &lt;IfMoudle expires_module&gt; #开启缓存 ExpiresActive on #针对不同类型元素设置缓存时间 ExpiresByType image/gif &quot;access plus 1 days&quot; ExpiresByType image/jpeg &quot;access plus 24 hours&quot; ExpiresByType image/png &quot;access plus 24 hours&quot; #now 相当于 access ExpiresByType text/css &quot;now plus 2 hour&quot; ExpiresByType application/x-javascript &quot;now plus 2 hours&quot; ExpiresByType application/x-shockwave-flash &quot;now plus 2 hours” #其他数据不缓存 ExpiresDefault &quot;now plus 0 min&quot; &lt;/IfModule&gt; &lt;/VirtualHost&gt; 1.6.3 数据压缩数据从服务器传输到客户端，需要传输时间，文件越大传输时间就越长，为了减少传输时间，我们一般把数据压缩后在传给客户端。 apache支持两种模式的压缩： default gzip 两者的区别： mod_deflate 压缩速度快。 mod_gzip 的压缩比略高。 一般情况下，mod_gzip 会比 mod_deflate 多出 4%~6％ 的压缩量。 mod_gzip 对服务器CPU的占用要高一些，所以 mod_deflate 是专门为确保服务器的性能而使用的一个压缩模块，只需较少的资源来进行压缩。 在apache主配文件中加载压缩模块 12vim /usr/local/apache/conf/httpd.confLoadModule deflate_module modules/mod_deflate.so 修改虚拟主机文件调用模块 12345678910111213141516171819vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; &lt;IfMoudle deflate_module&gt; #压缩等级1-9，数字越大压缩能力越好，相应地也越耗CPU性能 DeflateCompressionLevel 4 #压缩类型，html、xml、php、css、js AddOutputFilterByType DEFLATE text/html text/plain text/xml application/x-javascript application/x-httpd-php AddOutputFilter DEFLATE js css #浏览器匹配为IE1-6的不压缩 BrowserMatch \\bMSIE\\s[1-6] dont-vary #设置不压缩的文件 SetEnvIfNoCase Request_URI .(?:gif|jpe?g|png)$ no-gzip dont-vary SetEnvIfNoCase Request_URI .(?:exe|t?gz|zip|bz2|sit|rar)$ no-gzip dont-vary SetEnvIfNoCase Request_URI .(?:pdf|doc)$ no-gzip dont-vary &lt;/IfModule&gt; &lt;/VirtualHost&gt; 1.6.4 限速网站除了能共享页面给用户外，还能作为下载服务器存在。但是作为下载服务器时，我们应该考虑服务器的带宽和IO的性能，防止部分邪恶分子会通过大量下载的方式来攻击你的带宽和服务器IO性能。 问题： 假如你的服务器被邪恶分子通过下载的方式把带宽占满了，那么你或其他用户在访问的时候就会造成访问慢或者根本无法访问。 假如你的服务器被邪恶分子通过下载的方式把服务器IO占满了，那么你的服务器将会无法处理用户请求或宕机。 以上问题可以通过限速来解决，apache自带了基于宽带限速的模块： ratelimit_module：只能对连接下载速度做限制，且是单线程的下载，迅雷等下载工具使用的是多线程下载。 mod_limitipconn：限制每 IP 的连接数，需要额外安装该模块。 ratelimit_module模块 在apache主配文件中加载压缩模块 12vim /usr/local/apache/conf/httpd.confLoadModule ratelimit_module modules/mod_ratelimit.so 修改虚拟主机文件调用模块 123456789101112vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;# Location相对路径：/usr/local/apache/htdocs/...# Directory绝对路径：...&lt;Location /download&gt; SetOutputFiler RATE_LIMIT #限速100k SetEnv rate-limit 100&lt;/Location&gt; mod_limitipconn模块 下载安装模块 123456wget http://dominia.org/djao/limit/mod_limitipconn-0.24.tar.bz2tar -xf mod_limitipconn-0.24.tar.bz2cd mod_limitipconn-0.24vim Makefile apxs = &quot;/usr/local/apache/bin/apxs&quot;make &amp;&amp; make install 在apache主配文件启用模块 12vim /usr/local/apache/conf/httpd.confLoadModule limitipconn_module modules/mod_limitipconn.so 修改虚拟主机文件调用模块 123456789&lt;Location /download&gt; SetOutputFiler RATE_LIMIT #限速100k SetEnv rate-limit 100 #限制线程数 MaxConnPerIP 3 #对index.html文件不作限制 NoIPLimit index.html&lt;/Location&gt; 1.6.5 访问控制在生产环境中，网站分为公站和私站，公站允许所有人访问，但私站就只允许内部人员访问，Require就可以实现访问控制的功能。 容器： RequireAny：一个符合即可通过 RequireAll：所有符合才可通过 Requirenone：所有都不符合才可通过 普通的访问控制 修改虚拟主机文件 1234567891011121314vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;&lt;Directory &quot;/usr/local/apache/htdocs/web/test&quot;&gt; AllowOverride None # 拒绝所有人访问 Require all denied # 允许该地址段的用户访问 Require ip 192.168.88 # 允许该主机访问 Require host www.cqm.com&lt;/Directory&gt; 用户登录验证访问控制 修改虚拟主机文件 123456789101112131415161718192021vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;&lt;Directory &quot;/usr/local/apache/htdocs/web/test&quot;&gt; # 定义提示信息，用户访问时提示信息会出现在认证的对话框中 AuthName &quot;Private&quot; # 定义认证类型，在HTTP1.0中，只有一种认证类型：basic。在HTTP1.1中有几种认证类型，如：MD5 AuthType Basic # 定义包含用户名和密码的文本文件，每行一对 AuthUserFile &quot;/usr/local/apache/user.dbm&quot; # 配合容器使用，只有条件全部符合才能通过 &lt;RequireAll&gt; Require not ip 192.168.88 # require user user1 user2 (只有用户user1和user2可以访问) # requires groups group1 (只有group1中的成员可以访问) # require valid-user (在AuthUserFile指定的文件中的所有用户都可以访问) Require valid-user &lt;/RequireAll&gt;&lt;/Directory&gt; 生成用户文件 123# 生成cqm用户/usr/local/apache/bin/htpasswd -cm /usr/local/apache/user.dbm cqm... 1.6.6 URL重写Apache通过mod_rewrite模块可以实现URL重写的功能，URL重写其实就是改写用户浏览器中的URL地址。 在主配文件开启模块 12vim /usr/local/apache/conf/httpd.confLoadModule rewrite_module modules/mod_rewrite.so 修改虚拟主机文件 123456789101112131415161718192021222324vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; # 开启URL重写功能 RewriteEngine on # 重写规则，跳转到百度 RewriteRule &quot;^/$&quot; &quot;http://www.baidu.com&quot; [NC,L] # 匹配条件，根据请求头进行匹配 RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;chrome&quot; [NC,OR] RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;curl&quot; # 重写规则，和匹配到的条件配合使用，请求头匹配到chrome或curl则返回403状态码 RewriteRule &quot;^/$&quot; - [F]&lt;/VirtualHost&gt;RewreteRule [flag] 部分标记规则R:强制外部重定向F:禁用URL，返回403HTTP状态码G:强制URL为GONE，返回410HTTP状态码P:强制使用代理转发L:表明当前规则是最后一条规则，停止分析以后规则的重写N:重新从第一条规则开始运行重写过程C:与下一条规则关联NS:只用于不是内部子请求NC:不区分大小写 通过URL重写实现分流功能 123456789vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; RewriteEngine on RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;(chrome|curl)&quot; [NC,OR] RewriteRule &quot;^/$&quot; &quot;http://pc.cqm.com&quot; [NC] RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;(iPhone|Blackberry|Android|ipad)&quot; [NC] RewriteRule &quot;^/$&quot; &quot;http://phone.cqm.com&quot; [NC]&lt;/VirtualHost&gt; 1.6.7 压力测试Apache压力测试使用ab命令 123456789101112131415161718ab-A:指定连接服务器的基本的认证凭据-c:指定一次向服务器发出请求数-C:添加cookie-g:将测试结果输出为“gnuolot”文件-h:显示帮助信息-H:为请求追加一个额外的头-i:使用“head”请求方式-k:激活HTTP中的“keepAlive”特性-n:指定测试会话使用的请求数-p:指定包含数据的文件-q:不显示进度百分比-T:使用POST数据时，设置内容类型头-v:设置详细模式等级-w:以HTML表格方式打印结果-x:以表格方式输出时，设置表格的属性-X:使用指定的代理服务器发送请求-y:以表格方式输出时，设置表格属性 123/usr/local/apache/bin/ab -n 10000 -c 200 http:...# 并发数per second... 二、Nginx2.1 Nginx介绍Nginx是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理服务器，也是一个 IMAP/POP3/SMTP 代理服务器。和apache一样，都是web服务器软件，因为其性能优异，所以被广大运维喜欢。又因为nginx是一个轻量级的web服务器，相比apache来说资源消耗更低。 Nginx中文文档：https://www.nginx.cn/doc/index.html 2.2 通过脚本源码安装Nginx 编写安装脚本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576vim nginx-install.sh#!/bin/bashnginx_version=1.21.3#检测function check(){ #检测是否为root if [ $USER != &quot;root&quot; ] then echo -e &quot;\\e[1;31m error:need to be root so that \\e[0m&quot; exit 1 fi #检测wget是否安装 if [ ! -e /usr/bin/wget ] then echo -e &quot;\\e[1;31m error:not found command /usr/bin/wget \\e[0m&quot; exit 1 fi}#安装前准备function install_pre(){ # 安装依赖 #0:stdin标准输入 1:stdout标准输出 2:stderr错误输出 if [ ! `yum -y install gcc-* pcre-devel zlib-devel &amp;&gt; /dev/null` ] then echo -e &quot;\\e[1;31m error:yum install dependency package failed \\e[0m&quot; exit 1 fi #下载源码包 cd /usr/local/ if [ ! `wget http://nginx.org/download/nginx-${nginx_version}.tar.gz &amp;&gt; /dev/null` ] then tar -xf nginx-${nginx_version}.tar.gz if [ ! -d nginx-${nginx_version} ] then echo -e &quot;\\e[1;31m error:not found nginx-${nginx_version} \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:wget file nginx-${nginx_version}.tar.gz failed \\e[0m&quot; exit 1 fi}#安装function install_nginx(){ cd /usr/local/nginx-${nginx_version} echo &quot;nginx configure...&quot; ./configure --prefix=/usr/local/nginx &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;nginx make...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m nginx install success \\e[0m&quot; else echo -e &quot;\\e[1;31m error:nginx install fail \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:nginx configure fail \\e[0m&quot; exit 1 fi}checkinstall_preinstall_nginx 编写启动脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#!/bin/bash#Source function libiaryif [ -f /etc/init.d/functions ]then . /etc/init.d/functionselse echo &quot;Not found file /etc/init.d/functions&quot; exitfinginxd=/usr/local/nginx/sbin/nginxnginx_pid=/usr/local/nginx/logs/nginx.pidfunction nginx_start(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then echo -e &quot;nginx [\\e[1;32m running \\e[0m]&quot; exit 1 elif [ $nginx_num -eq 1 ] &amp;&amp; [ -f $nginx_pid ] then killall nginx fi $nginxd}function nginx_stop(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -eq 1 ] then echo -e &quot;nginx [\\e[1;31m stopping \\e[0m]&quot; exit 1 elif [ $nginx_num -gt 1 ] then killall nginx fi}function nginx_status(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then echo -e &quot;nginx [\\e[1;32m running \\e[0m]&quot; else echo -e &quot;nginx [\\e[1;31m stopping \\e[0m]&quot; fi}function nginx_restart(){ nginx_stop nginx_start}function nginx_reload(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then $nginxd -s reload else echo -e &quot;nginx [\\e[1;31m stopping \\e[0m]&quot; fi}case $1 instart) nginx_start echo -e &quot;nginx start [\\e[1;32m OK \\e[0m]&quot;;;stop) nginx_stop echo -e &quot;nginx stop [\\e[1;32m OK \\e[0m]&quot;;;status) nginx_status;;restart) nginx_restart echo -e &quot;nginx restart [\\e[1;32m OK \\e[0m]&quot;;;reload) nginx_reload echo -e &quot;nginx reload [\\e[1;32m OK \\e[0m]&quot;esac 2.3 Nginx的Server块当Nginx配置文件只有一个Server块时，那么该Server块就被Nginx认为是默认网站，所有发给Nginx的请求都会传给该Server块。 123456789101112131415161718192021222324252627server { # 监听80端口 listen 80; # 域名 server_name localhost; # 字符集 charset koi8-r; # 访问日志路径 access_log logs/host.access.log main; # web根路径 # /代表相对路劲，这里代表/usr/local/nginx location / { # 根目录路径，这里代表/usr/local/nginx/html root html; # 索引页 index index.html index.htm; } # 404状态码 error_page 404 /404.html; location = /404.html{ root html; } # 50x状态码 error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } 2.4 Nginx的访问控制 编写主配文件 12345678910111213location / { root html; index index.html index.htm; # 允许192.168.88.0/24的用户访问 allow 192.168.88.0/24; # 拒绝所有 deny all; # 基于客户端IP做过滤，符合条件的允许访问，不符合的返回404 # 这里为不是192.168.88的就返回404 if ( $remote_addr !~ &quot;192.168.88&quot; ){ return 404; }} 2.5 Nginx的用户验证 编写主配文件 12345678location / { root html; index index.html index.htm; # 欢迎词 auth_basic &quot;welcome to cqm's web&quot;; # 存放用户文件 auth_basic_user_file /usr/local/nginx/htpasswd;} 生成用户文件 1/usr/local/apache/bin/htpasswd -cm /usr/local/nginx/htpasswd cqm 2.6 Nginx参数1234567891011# nginx中的log_format可以用来自定义日志格式# log_format变量：$remote_addr:记录访问网站的客户端地址$remote_user:远程客户端用户名$time_local:记录访问时间与时区$request:用户的http请求起始行信息$status:http状态码，记录请求返回的状态码，例如：200、301、404等$body_bytes_sent:服务器发送给客户端的响应body字节数$http_referer:记录此次请求是从哪个连接访问过来的，可以根据该参数进行防盗链设置。$http_user_agent:记录客户端访问信息，例如：浏览器、手机客户端等$http_x_forwarded_for:当前端有代理服务器时，设置web节点记录客户端地址的配置，此参数生效的前提是代理服务器也要进行相关的x_forwarded_for设置 2.7 Nginx防盗链盗链用大白话讲就是抓取别人网站的资源，加以利用，以至于被抓取资源的网站消耗了带宽，而收益的是抓取资源的人。 而反盗链就可以防止别人抓取自身网站的资源。 编写主配文件 1234567location / { # 除了www.cqm.com之外，都返回403 valid_referers none blocked www.cqm.com; if ($invalid_referer){ return 403; }} 2.8 Nginx虚拟主机Nginx的虚拟主机是通过server块来实现的。 2.8.1 基于IP的虚拟主机 修改主配文件 12vim /usr/local/nginx/conf/nginx.confinclude /usr/local/nginx/conf/conf.d/nginx_vhosts.conf; 修改虚拟主机文件 12345678910111213141516vim /usr/local/nginx/conf/conf.d/nginx_vhosts.confserver { listen 192.168.88.100; location / { root html/web1; index index.html index.htm index.php; }}server { listen 192.168.88.101; location / { root html/web2; index index.html index.htm index.php; }} 其它配置 12345# 添加一个逻辑网卡，重启即失效ifconfig eth0:1 192.168.88.100/24 upmkdir /usr/local/nginx/html/web{1..2}echo 'this is web1' &gt; /usr/local/nginx/html/web1/index.htmlecho 'this is web2' &gt; /usr/local/nginx/html/web2/index.html 测试 1234curl http://192.168.88.100/this is web1curl http://192.168.88.101/this is web2 2.8.2 基于端口的虚拟主机 修改虚拟主机文件 12345678910111213141516vim /usr/local/nginx/conf/conf.d/nginx_vhosts.confserver { listen 80; location / { root html/web1; index index.html index.htm index.php; }}server { listen 81; location / { root html/web2; index index.html index.htm index.php; }} 2.8.3 基于域名的虚拟主机123456789101112131415161718vim /usr/local/nginx/conf/conf.d/nginx_vhosts.confserver { listen 80; server_name www.cqm1.com; location / { root html/web1; index index.html index.htm index.php; }}server { listen 80; server_name www.cqm2.com; location / { root html/web2; index index.html index.htm index.php; }} 2.9 Nginx反向代理代理最常见的使用方式就是翻墙，能够实现让国内的用户访问国外的网站。 原理： 用户讲请求发给代理服务器 代理服务器代替用户去获取数据 代理服务器将数据发送给用户 正常没有代理的上网 使用代理服务器的上网 = 代理服务器又分为两种：正向代理、反向代理 正向代理：代理用户向服务器获取资源 反向代理：代理服务器去管理网络资源，用户有请求找反向代理就可以了 编写反向代理服务器主配文件 123456vim /usr/local/nginx/conf/nginx.conflocation / { index index.html index.htm index.php; # 访问代理服务器就会跳转到http://192.168.88.100 proxy_pass http://192.168.88.100;} 反向代理其它配置 1234567891011121314151617181920212223proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;client_max_body_size 10m; #允许客户端请求的最大单文件字节数client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数，proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时)proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时)proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时)proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2）proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 2.10 Nginx下载限速限速方法主要分为： 下载速度限制 单位时间内请求数限制 基于客户端的并发数限制 Nginx官方提供的限制IP连接和并发的模块有两个： limit_req_zone：用来限制单位时间内的请求数，即速率限制，采用的漏桶算法 limit_req_conn：来限制同一时间连接数，即并发限制 单位时间内请求数限制 修改主配文件 12345678910111213# 在http快下调用模块# $binary_remote_addr:基于ip地址做限制# zone:创建缓存域和缓存大小# rate:设置访问频数limit_req_zone $binary_remote_addr zone=cqm:10m rate=1r/s;# server块location /test { ... # 调用模块 # 当请求数超过5次时，就拒绝访问，并返回503状态码 limit_req zone=cqm burst=5 nodelay;} 限制并发连接数 修改主配文件 1234567891011# 在http快下调用模块# $binary_remote_addr:基于ip地址做限制# zone:创建缓存域和缓存大小limit_req_conn $binary_remote_addr zone=cqm:10m;# server块location /test { ... # 限制同一时间内下载数为1个 limit_conn cqm 1;} 限制下载速度 修改主配文件 12345location /test { ... # 限制下载速度为1k limit_rate 1k;} 2.11 Nginx的URL重写rewrite的主要功能是实现URL地址的重定向。Nginx的rewrite功能需要PCRE软件的支持，即通过perl兼容正则表达式语句进行规则匹配的。默认参数编译nginx就会支持rewrite的模块，但是也必须要PCRE的支持。 URL模板语块： set：设置变量 if：判断 return：返回值或URL break：终止 rewrite：重定向URL 例一：根据不同域名跳转到主域名的不同目录下 创建测试目录 1234mkdir /usr/local/nginx/html/{cn,jp,us}echo 'this is China' &gt; /usr/local/nginx/html/cn/index.htmlecho 'this is Japan' &gt; /usr/local/nginx/html/jp/index.htmlecho 'this is America' &gt; /usr/local/nginx/html/us/index.html 修改hosts文件，以便解析 12345vim /etc/hosts192.168.88.100 www.cqm.com192.168.88.100 www.cqm.com.cn192.168.88.100 www.cqm.com.jp192.168.88.100 www.cqm.com.us 修改主配文件 1include /usr/local/nginx/conf/conf.d/rewrite.conf 修改重定向文件 1234567891011121314151617181920212223242526272829vim /usr/local/nginx/conf/conf.d/rewrite.conf# 设置重定向serverserver { listen 80; server_name www.cqm.com.cn www.cqm.com.jp www.cqm.com.us; location / { # 模糊匹配到cn的话，就跳转到http://www.cqm.com/cn下 if ($http_host ~ (cn)$){ set $nation cn; rewrite ^/$ http://www.cqm.com/$nation; } if ($http_host ~ (jp)$){ set $nation jp; rewrite ^/$ http://www.cqm.com/$nation; } if ($http_host ~ (us)$){ set $nation us; rewrite ^/$ http://www.cqm.com/$nation; } }}server { listen 80; server_name www.cqm.com; location / { root html; index index.html; }} 测试 1234567curl -L http://www.cqm.com.cnthis is Chinacurl -L http://www.cqm.com.jpthis is Japancurl -L http://www.cqm.com.usthis is America-L:自动获取重定向 例二：retuen以及break的简单实用 修改重定向文件 12345678910111213141516171819vim /usr/local/nginx/conf/conf.d/rewrite.confserver { listen 80; server_name www.cqm.com; location / { root html; index index.html; # 模糊匹配 ~ # 精确匹配 = # 不匹配 !~ # 如果匹配请求头不是chrome的话，就返回403 if ($http_user_agent !~ 'chrome'){ return 403; # break放在return上面的话就不会执行return操作 # break; # return http://www.baidu.com; } }} flag flag是放在rewrite重定向的URL后边的，格式为：rewrite URL flag flag的选项有： last：本条规则匹配完成后继续执行到最后。 break：本条规则匹配完成即终止。 redirect：返回302临时重定向。 permanent：返回301永久重定向。 redirect和permanent的区别：设置permanent的话，新网址就会完全继承旧网址，旧网址的排名等完全清零，如果不是暂时迁移的情况下都建议使用permanent；设置redirect的话，新网址对旧网址没有影响，且新网址也不会有排名。 2.12 Nginx优化2.12.1 大并发Nginx的工作模式：主进程 + 工作进程 假如Nginx服务器有4个CPU 设置主配文件来实现高并发 12345678vim /usr/local/nginx/conf/nginx.confworker_processes 4;# 指定运行的核的编号，采用掩码的方式设置编号worker_cpu_affinity 0001 0010 0100 1000;events { # 单个工作进程维护的请求队列长度，根据实际情况调整 worker_connections 1024;} 2.12.2 长连接 修改主配文件 1234567vim /usr/local/nginx/conf/nginx.conf# keepalive_timeout用来设置长连接，0代表关闭keepalive_timeout 0;# 设置长连接时间100s#keepalive_timeout 100;# 设置每秒可以接受的请求数#keepalive_requests 8192; 2.12.3 压缩Nginx是采用gzip进行压缩。 修改主配文件 1234567891011121314151617181920212223242526272829303132vim /usr/local/nginx/conf/nginx.conf# 开启缓存gzip on;# Nginx做为反向代理的时候启用# off:关闭所有的代理结果数据压缩# expired:如果header中包含”Expires”头信息，启用压缩# no-cache:如果header中包含”Cache-Control:no-cache”头信息，启用压缩# no-store:如果header中包含”Cache-Control:no-store”头信息，启用压缩# private:如果header中包含”Cache-Control:private”头信息，启用压缩# no_last_modified:启用压缩，如果header中包含”Last_Modified”头信息，启用压缩# no_etag:启用压缩，如果header中包含“ETag”头信息，启用压缩# auth:启用压缩，如果header中包含“Authorization”头信息，启用压缩# any:无条件压缩所有结果数据gzip_proxied any;# 启用gzip压缩的最小文件，小于设置值的文件将不会压缩gzip_min_length 1k;# 设置压缩所需要的缓冲区大小# 32 4K表示按照内存页（one memory page）大小以4K为单位（即一个系统中内存页为4K），申请32倍的内存空间# 建议此项不设置，使用默认值gzip_buffers 32 4k;# 设置gzip压缩级别，级别越底压缩速度越快文件压缩比越小，反之速度越慢文件压缩比越大gzip_comp_level 1;# 用于识别http协议的版本，早期的浏览器不支持gzip压缩，用户会看到乱码，所以为了支持前期版本加了此选项。默认在http/1.0的协议下不开启gzip压缩gzip_http_version 1.1;# 设置需要压缩的MIME类型,如果不在设置类型范围内的请求不进行压缩gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png application/vnd.ms-fontobject font/ttf font/opentype font/x-woff image/svg+xml; 2.12.4 静态缓存将部分数据缓存在用户本地磁盘，用户加载时，如果本地和服务器的数据一致，则从本地加载。提升用户访问速度，提升体验度。节省公司带宽成本。 修改主配文件 12345# 模糊匹配以png或gif结尾的文件location ~* \\.(png|gif)$ { # 缓存时间为1小时 expires 1h;} 三、Tomcat3.1 Tomcat介绍Tomcat 服务器是一个免费的开放源代码的 Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试 JSP 程序的首选。 实际上 Tomcat 是 Apache 服务器的扩展，但运行时它是独立运行的，所以当你运行 tomcat 时，它实际上作为一个与 Apache 独立的进程单独运行的。 Tomcat 官方文档：https://tomcat.apache.org/ 3.2 Tomcat安装 安装jdk和tomcat 123456yum -y install java-1.8.0-openjdk*wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gztar -xf apache-tomcat-9.0.43.tar.gzmkdir /root/tomcatmv apache-tomcat-9.0.43.tar.gz/* /root/tomcat./root/tomcat/bin/startup.sh","link":"/2024/02/18/web/"},{"title":"使用 Istio 实现应用迁移","text":"背景：当前环境中有 Cluster A 和 Cluster B，各自部署了相同的应用，且该应用有两个域名用于处理不同的请求。目前，这两个域名的请求都由 Cluster A 中的应用处理。为进行应用迁移，现希望将域名 A 的请求继续由 Cluster A 处理，而域名 B 的请求则切换至由 Cluster B 中的应用来处理。 由于目前的流量处理都是由 Istio 负责，所以可以通过 ServiceEntry 来实现此需求。 创建一个 ServiceEntry： 将 b.nginx.com 的流量转发到 Cluster B 的 Istio Ingress Gateway，然后再通过 Cluster B 的 VirtualService 将流量转发到 Cluster B 中的应用。 1234567891011121314151617181920apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-nginx namespace: defaultspec: addresses: {} endpoints: # 此处为 Cluster B 的 Istio Ingress Gateway 的地址和端口 - address: 172.16.0.104 ports: http: 80 hosts: - b.nginx.com location: MESH_EXTERNAL ports: - name: http number: 80 protocol: HTTP resolution: DNS 修改原有的 VirtualService 中处理域名 B 流量的部份： 12345678910111213141516171819202122232425262728293031apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: nginx-virtualservice namespace: defaultspec: gateways: - http-gateway.istio-system.svc.cluster.local hosts: - a.nginx.com - b.nginx.com http: - match: - headers: host: exact: a.nginx.com route: - destination: host: nginx.default.svc.cluster.local port: number: 80 - match: - headers: host: exact: b.nginx.com route: - destination: # 此处原本为 nginx.default.svc.cluster.local，如果要做迁移将其修改为 b.nginx.com 即可 host: b.nginx.com port: number: 80 尝试请求查看效果：","link":"/2024/11/11/%E4%BD%BF%E7%94%A8-Istio-%E5%AE%9E%E7%8E%B0%E5%BA%94%E7%94%A8%E8%BF%81%E7%A7%BB/"},{"title":"使用代理获取镜像","text":"由于国内网络受限，去到 Docker Hub 等获取镜像都会非常缓慢（甚至失败），如果有梯子，可以通过代理获取，此处使用 Clash 作为代理组件。 Docker 配置代理在节点上准备 Clash 配置文件： 12# 将梯子的 .yaml 配置文件放在此处mkdir /root/clash 启动 Clash 容器： 1docker run -d --restart=unless-stopped --name clash -v /root/clash/xxx.yaml:/root/.config/clash/config.yaml:ro -p 9090:9090 -p 7890:7890 -p 7891:7891 harbor.warnerchen.com/dreamacro/clash:v1.18.0 验证是否可用： 1curl -x http://localhost:7890 https://www.google.com -I 创建 systemd 配置文件： 123456cat &lt;&lt;EOF &gt; /etc/systemd/system/docker.service.d/http-proxy.conf[Service]Environment=&quot;HTTP_PROXY=http://127.0.0.1:7890&quot;Environment=&quot;HTTPS_PROXY=http://127.0.0.1:7890&quot;Environment=&quot;NO_PROXY=localhost,127.0.0.0/8,harbor.warnerchen.com,registry.rancher.com,registry.rancher.cn,registry.cn-hangzhou.aliyuncs.com&quot;EOF 重启 Docker： 123systemctl daemon-reloadsystemctl show --property Environment dockersystemctl restart docker 验证是否可用： 1docker pull nginx:mainline","link":"/2025/02/24/%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E8%8E%B7%E5%8F%96%E9%95%9C%E5%83%8F/"},{"title":"SUSE Observability 使用随记","text":"SUSE Observability（前身为 StackState）可用于观察 Kubernetes 集群及其工作负载。 SUSE Observability 主要分为 Server 和 Agent 两个部分，Server 负责存储和展示数据，Agent 负责采集数据并发送给 Server。 Server 的组件有： Topology (StackGraph) Metrics (VictoriaMetrics) Traces (ClickHouse) Logs (ElasticSearch) 部署 SUSE Observability基于 Rancher Prime 的 SUSE Observability 部署文档 helm template 的命令会生成两个 values 文件，baseConfig_values.yaml 配置 license 等信息，sizing_values.yaml 配置集群规模等信息： 1234567891011helm repo add suse-observability https://charts.rancher.com/server-charts/prime/suse-observabilityhelm repo updatekubectl create namespace suse-observabilityexport VALUES_DIR=.helm template \\ --set license='xxx' \\ --set baseUrl='https://suse-observability.warnerchen.com' \\ --set sizing.profile='trial' \\ suse-observability-values \\ suse-observability/suse-observability-values --output-dir $VALUES_DIR 安装前，创建 ingress_values.yaml、ingress_otel_values.yaml 中添加 Ingress 配置： 12345678910111213cat &lt;&lt;EOF &gt; $VALUES_DIR/suse-observability-values/templates/ingress_values.yamlingress: enabled: true annotations: nginx.ingress.kubernetes.io/proxy-body-size: &quot;50m&quot; hosts: - host: suse-observability.warnerchen.com tls: # 此处证书只能使用权威证书 - secretName: tls-secret hosts: - suse-observability.warnerchen.comEOF 12345678910111213141516171819202122232425262728293031323334cat &lt;&lt;EOF &gt; $VALUES_DIR/suse-observability-values/templates/ingress_otel_values.yamlopentelemetry-collector: ingress: enabled: true annotations: nginx.ingress.kubernetes.io/proxy-body-size: &quot;50m&quot; nginx.ingress.kubernetes.io/backend-protocol: GRPC cert-manager.io/cluster-issuer: suse-observability-otlp-selfsigned-cluster-issuer hosts: - host: suse-observability-otlp.warnerchen.com paths: - path: / pathType: Prefix port: 4317 tls: - hosts: - suse-observability-otlp.warnerchen.com secretName: suse-observability-otlp-tls-secret additionalIngresses: - name: otlp-http annotations: nginx.ingress.kubernetes.io/proxy-body-size: &quot;50m&quot; cert-manager.io/cluster-issuer: suse-observability-otlp-selfsigned-cluster-issuer hosts: - host: suse-observability-otlp-http.warnerchen.com paths: - path: / pathType: Prefix port: 4318 tls: - hosts: - suse-observability-otlp-http.warnerchen.com secretName: suse-observability-otlp-tls-secretEOF 通过 Cert Manager 创建自签名证书给 Otlp Ingress 使用： 1234567891011121314151617181920212223242526272829cat &lt;&lt;EOF | kubectl apply -f -apiVersion: cert-manager.io/v1kind: ClusterIssuermetadata: name: suse-observability-otlp-selfsigned-cluster-issuerspec: selfSigned: {}---apiVersion: cert-manager.io/v1kind: Certificatemetadata: name: suse-observability-otlp-tls namespace: suse-observabilityspec: secretName: suse-observability-otlp-tls-secret issuerRef: name: suse-observability-otlp-selfsigned-cluster-issuer kind: ClusterIssuer commonName: suse-observability-otlp.warnerchen.com dnsNames: - suse-observability-otlp.warnerchen.com - suse-observability-otlp-http.warnerchen.com duration: 8760h renewBefore: 720h privateKey: algorithm: RSA size: 2048EOF 执行安装： 12345678helm upgrade \\ --install \\ --namespace suse-observability \\ --values $VALUES_DIR/suse-observability-values/templates/baseConfig_values.yaml \\ --values $VALUES_DIR/suse-observability-values/templates/sizing_values.yaml \\ --values $VALUES_DIR/suse-observability-values/templates/ingress_values.yaml \\ --values $VALUES_DIR/suse-observability-values/templates/ingress_otel_values.yaml \\ suse-observability suse-observability/suse-observability 等待所有 Pod 运行完毕： 通过 Service suse-observability-router / Ingress 访问 SUSE Observability UI： 部署 SUSE Observability Agent被监控集群需要部署 Agent 才能够进行监控。 在 StackPacks 选择 Kubernetes，然后填入集群名称： 点击 Install 后，会提供安装命令： 在被监控集群执行安装： 123456789helm upgrade --install \\--namespace suse-observability \\--create-namespace \\--set-string 'stackstate.apiKey'='xxx' \\--set-string 'stackstate.cluster.name'='test' \\--set-string 'stackstate.url'='https://suse-observability.warnerchen.com/receiver/stsAgent' \\--set-string 'global.skipSslValidation'='true' \\--set-string 'global.imageRegistry'='harbor.warnerchen.com' \\suse-observability-agent suse-observability/suse-observability-agent 等待所有 Pod 正常运行： 收集 Logs/Metrics/Events 数据部署 SUSE Observability Agent 后即可查看相关数据。 Pod 的指标： Pod 的详细监控信息： Pod 的事件： Pod 的日志： 收集 Traces 数据通过 Helm Chart 部署 OpenTelemetry Collector收集被监控集群的 Traces 数据，还需要部署 OpenTelemetry Collector。 准备 values.yaml： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110cat &lt;&lt;EOF &gt; otel-collector.yamlextraEnvsFrom: - secretRef: name: open-telemetry-collectormode: deploymentimage: repository: &quot;harbor.warnerchen.com/otel/opentelemetry-collector-k8s&quot;ports: metrics: enabled: truepresets: kubernetesAttributes: enabled: true extractAllPodLabels: trueconfig: extensions: bearertokenauth: scheme: SUSEObservability # 修改为实际的 API Key token: &quot;xxx&quot; exporters: otlp/stackstate: auth: authenticator: bearertokenauth # 修改为实际的 SUSE O11y Server 集群的 otlp ingress host endpoint: suse-observability-otlp.warnerchen.com:443 tls: insecure_skip_verify: true otlphttp/stackstate: auth: authenticator: bearertokenauth # 修改为实际的 SUSE O11y Server 集群的 otlp http ingress host endpoint: https://suse-observability-otlp-http.warnerchen.com tls: insecure_skip_verify: true processors: tail_sampling: decision_wait: 10s policies: - name: rate-limited-composite type: composite composite: max_total_spans_per_second: 500 policy_order: [errors, slow-traces, rest] composite_sub_policy: - name: errors type: status_code status_code: status_codes: [ ERROR ] - name: slow-traces type: latency latency: threshold_ms: 1000 - name: rest type: always_sample rate_allocation: - policy: errors percent: 33 - policy: slow-traces percent: 33 - policy: rest percent: 34 resource: attributes: - key: k8s.cluster.name action: upsert # 修改为实际的集群名称 value: test - key: service.instance.id from_attribute: k8s.pod.uid action: insert filter/dropMissingK8sAttributes: error_mode: ignore traces: span: - resource.attributes[&quot;k8s.node.name&quot;] == nil - resource.attributes[&quot;k8s.pod.uid&quot;] == nil - resource.attributes[&quot;k8s.namespace.name&quot;] == nil - resource.attributes[&quot;k8s.pod.name&quot;] == nil connectors: spanmetrics: metrics_expiration: 5m namespace: otel_span routing/traces: error_mode: ignore table: - statement: route() pipelines: [traces/sampling, traces/spanmetrics] service: extensions: - health_check - bearertokenauth pipelines: traces: receivers: [otlp] processors: [filter/dropMissingK8sAttributes, memory_limiter, resource] exporters: [routing/traces] traces/spanmetrics: receivers: [routing/traces] processors: [] exporters: [spanmetrics] traces/sampling: receivers: [routing/traces] processors: [tail_sampling, batch] exporters: [debug, otlp/stackstate] metrics: receivers: [otlp, spanmetrics, prometheus] processors: [memory_limiter, resource, batch] exporters: [debug, otlp/stackstate]EOF 部署 OpenTelemetry Collector： 12345678910111213kubectl create ns open-telemetrykubectl create secret generic open-telemetry-collector \\ --namespace open-telemetry \\ --from-literal=API_KEY='xxx'helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-chartshelm repo updatehelm upgrade --install opentelemetry-collector open-telemetry/opentelemetry-collector \\ --values otel-collector.yaml \\ --namespace open-telemetry 收集 Java 应用 Traces 数据此处使用 Automatic instrumentation 的方式，为 Spring Boot 注入 OpenTelemetry Java Agent，其原理就是在 Java 启动命令中调用 OpenTelemetry 的 Java Agent，然后通过 OpenTelemetry Collector 发送数据到 SUSE Observability。 DEMO 仓库地址：https://github.com/warnerchen/otel-spring-boot-demo.git 部署后效果如下： 也会收集 Java 应用的 Metrics 数据： Rancher 对接SUSE Observability在 Rancher 对接 SUSE Observability，URL 需要使用有效证书（非自签名证书）。 在 SUSE Observability -&gt; CLI 页面，获取 CLI 工具安装命令： 通过 CLI 工具获取 Service Token，后续用于 Rancher 对接 SUSE Observability： 12curl -o- https://dl.stackstate.com/stackstate-cli/install.sh | STS_URL=&quot;https://suse-observability.warnerchen.com&quot; STS_API_TOKEN=&quot;xxx&quot; bashsts service-token create --name suse-observability-extension --roles stackstate-k8s-troubleshooter 在 Rancher Extensions 中安装 Observability：","link":"/2025/03/03/SUSE-Observability-%E4%BD%BF%E7%94%A8%E9%9A%8F%E8%AE%B0/"},{"title":"八股文随记录","text":"一、网络方面1.1 OSI七层体系模型和TCP/IP四层体系模型 OSI七层： 物理层 数据链路层 网络层 传输层 会话层 表示层 应用层 TCP/IP四层： 网络接口层 网际层IP 传输层 应用层 基本五层体系模型： 物理层：IEEE802.3 数据链路层：PPP、Ethernet 网络层：ARP、RARP、IP、ICMP、RIP、OSPF、BGP 传输层：TCP、UDP 应用层：HTTP、DNS、FTP、TELNET、SMTP、NFS、SNMP 1.2 物理层1.3 数据链路层1.3.1 PPPPPP协议即点对点协议，属于数据链路层协议，一般用于广域网。PPP协议首先会建立物理链路，当物理链路建立成功，就通过链路控制协议LCP来建立数据链路连接，接着网络控制协议NCP就会协商该链路上所传输的数据包格式和类型，从而建立不同的网络层协议。 1.3.2 CSMA/CDCSMA/CD协议即载波监听多点接入/碰撞检测协议，属于数据链路层协议。工作原理是发送数据之前会先侦听信道是否空闲，如果空闲就发送数据，如果忙碌就等待一段时间再发送，即载波监听；当上一段信息发送完后，发生了两个或两个以上的节点同时发送数据，那就判定为冲突，会立即停止发送，等待一段时间再发送数据，即碰撞检测。节点在发送过程中也会监听信道。所以CSMA/CD的工作原理可以概括为：先听后发、边发边听、冲突停发、随机延迟后重发。 1.3.2 划分vlan有何用vlan 即虚拟局域网，就是把一个大的局域网，划分为多个相互隔离的小局域网，用于划分数据链路层，可以实现隔离广播域，避免每个节点收到太多无用的广播包，减小节点性能和网络宽带的消耗，同时可以隔离常见的攻击，如 arp 攻击，受到攻击的影响范围仅限于该 vlan。 1.4 网络层1.4.1 ARP地址解析协议ARP，是根据IP地址来获取物理地址的协议，工作在数据链路层，源主机发送信息时会将目标地址主机的IP地址通过广播的形式发送到局域网内的所有主机上，目标地址主机收到广播后就会将自己的MAC地址发送给源主机，这样源主机就会将目标地址主机的IP地址和MAC地址保存在ARP缓存中，从而节约网络资源。 1.4.2 RIPRIP是采用距离向量的路由选择协议，属于内部网关协议，使用 “跳数” 来衡量到达目标地址的路由距离。距离的取值为0 - 16，16即为不可达。RIP协议仅和相邻路由器交换信息，交换的信息是自己的路由表，每30秒就会交换一次，如果超过180秒没收到相邻路由器发送过来的信息，则视为不可达。 1.4.3 OSPFOSPF即开放最短路径优先，属于内部网关协议，是采用洪泛法向自治系统内的所有路由器发送信息，每一个相邻路由器会将此消息再次发送给相邻的路由器，交换的信息是与本路由器相邻的所有路由器的链路状态，链路状态可以是费用、距离、时延、带宽等，只有当链路状态发生变化时，路由器才会向所有路由器采用洪泛法发送消息。 OSPF工作状态： 发现邻居：通过发送hello包来发现邻居 建立邻居关系：选举DR/BDR来建立关系，其它路由仅与DR和BDR建立关系，DR挂了还有BDR在所以不怕链路发生断联的状况 更新链路状态：每个路由去都有个LSDB（链路状态数据库），里面存放LSA（链路状态公告），如果网络状态没发生变化，就每隔30分钟与邻居更新链路状态信息，如果发生变化则自动更新链路状态信息 路由计算：根据LSDB中的LSA计算最优路径 OSPF的五种包类型： 发现邻居：通过发送hello包来发现邻居，以及用来选举DR/BDR 数据库描述：用在LSDB交换过程中，确立主/从关系，交换LSA包头，以及确定首个序列号 链路状态请求：请求LSDB交换过程中本路由器没有的LSA 链路状态更新：通过泛洪法更新LSA 链路状态确认：对收到的链路状态更新进行确认， 如果发送确认的路由器的状态是DR或者BDR，确认数据包发送到OSPF的组播地址224.0.0.5，如果不是，则组播地址224.0.0.6 OSPF区域： 同一区域内的路由器才会建立关系，交换LSA，收敛后，同一区域内的路由器都拥有相同的LSDB。 如果有多个区域，那么每个区域都会选择一个性能较好的路由器来作为ABR（区域边界路由器），不同区域内的路由器进行通信直接通过这个ABR转发路由。 每个区域都有一个区域ID，为32位二进制数，可以表达为一个十进制数，也可以表达为一个点分十进制数字，例如区域0等价于0.0.0.0，区域1等价于0.0.0.1 骨干区域为区域0 非骨干区域间进行通信都要通过骨干区域0进行转发 如果网络中有不同的OSPF区域，那么有个区域肯定是区域0 1.4.4 BGPBGP即边界网关协议，属于外部网关协议，是不同自治系统AS使用的协议，每个自治系统都要有一个BGP发言人，用来交换信息，交换的信息是网络可达性的信息，即要到达某一网络所经过的一系列自治系统，且只有在信息发生变化时才会进行信息交换。 1.4.5 PINGPING实际就是发送一个ICMP回送请求报文给目的主机，并等待回送的ICMP响应。实际就是利用了IP地址的唯一性，给目标主机的IP地址发送一个ICMP数据包，再要求对方返回一个同等大小的数据包来确定两台主机是否相互连通，以及时延是多少。 1.4.6 TracerouteTraceroute是通过TTL和ICMP报文来确定从一个主机到网络上其他主机的路由。首先会发送一个TTL为1的数据包到目的主机，经过一个路由器TTL就减1，此时TTL为0数据包就会被丢弃，路由器会回送一个ICMP超时报文给源主机，源主机收到超时报文后，就会将TTL的值加1，再发送数据包到目的主机，不断重复以上的过程直到数据包到达目的主机，目的主机收到数据包后就会会送一个ICMP响应报文。 1.4.7 划分子网有何用IP 是以网络号和主机号构成的，只有在同一个网络号下的主机才能够相互通信，不同网络号的主机要通信就要通过网关来实现，但只通过网络号来划分并不灵活，而子网就是将一个网络划分为更多个小的网络，这些小的网络就是子网，每一个子网都有相应的子网掩码，子网掩码就是用来判断多个 IP 是否在同一子网中的。 1.5 传输层1.5.1 TCP与UDP TCP与UDP的区别： TCP即传输控制协议，是一种面向连接的、可靠的、基于字节流的传输层协议，TCP的连接是点对点连接，所以传输的数据是无差错、不丢失、不重复、按需到达的，但首部开销较大，有20字节。 UDP即用户数据报协议，是一种面向无连接的、尽最大努力交付的、基于报文的传输层协议，UDP的连接可以是一对一、一对多、多对一和多对多的，传输的数据很可能会丢失，但首部开销小，只有8字节。 TCP的流量控制： 所谓流量控制就是让对方发送速率不要过快，让接收方来得及接收。 滑动窗口机制：滑动窗口机制的基本原理就是在任意时刻，发送方都维持了一个连续的允许发送的帧的序号，称为发送窗口；同时，接收方也维持了一个连续的允许接收的帧的序号，称为接收窗口。发送窗口和接收窗口的序号的上下界不一定要一样，甚至大小也可以不同。不同的滑动窗口协议窗口大小一般不同。 TCP的拥塞控制（加法增大乘法减小算法 AIMD）： 拥塞控制的目的是防止过多的数据注入到网络中。 慢开始：不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。 拥塞避免：让拥塞窗口值cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞控制窗口加一。 快重传：发送方只要收到三个重复确认就立即重传对方尚未收到的报文段，不必继续等待设置的重传计时器时间到期。 快恢复：快恢复和快重传是配合使用的，执行快恢复会将拥塞窗口值设为慢开始门限值的一半，然后执行拥塞避免。 流量控制和拥塞控制的区别： 流量控制是端到端的问题，就是要控制住发送端的发送速率，以便接收端来得及接收。 拥塞控制是一个全局性的过程，目的是防止过多的数据注入到网络中。 TCP的三次握手和四次挥手 三次握手：三次握手就是计算机网络中客户端和服务端通信前进行连接的一个过程，客户端会先给服务端发出一个请求SYN（seq=x），服务端收到请求且能够进行连接就会给客户端发送一个请求SYN和确认ACK（ack=x+1，seq=y），客户端收到了服务端的响应就会给服务端再次发送一个请求SYN和确认ACK（ack=y+1，seq=x+1），就完成了三次握手。 四次挥手：客户端获得了需要的资源之后，就会给服务端发送中止FIN和确认ACK（fin=1，ack=z，seq=x），服务端收到就会回复一个ACK（ack=x+1，seq=z），关闭连接接着给客户端发送一个中止FIN（fin=1，ack=x，seq=y），客户端收到后就会给服务端发送一个确认ACK（ack=y，seq=x），便完成了四次挥手。 TIME_WAIT 状态产生的原因： 为实现TCP全双工连接的可靠释放 为使旧的数据包在网络因过期而消失 大量 TIME_WAIT 状态所带来的危害： 如果系统中有很多 socket 处于 TIME_WAIT 状态，当需要创建新的 socket 连接的时候可能会受到影响，如果客户端的并发量持续很高，此时部分客户端就会显示连接不上。 为什么会采用三次握手，二次握手可以吗？第三次握手失败了怎么办？ 采用三次握手是为了防止失效的连接请求报文段再次传到服务器，因而产生错误。如果由于网络不稳定，虽然客户端以前发送的连接请求以到达服务方，但服务方的同意连接的应答未能到达客户端。则客户方要重新发送连接请求，若采用二次握手，服务方收到客服端重传的请求连接后，会以为是新的请求，就会发送同意连接报文，并新开进程提供服务，这样会造成服务方资源的无谓浪费。如果第三次握手失败，服务端不会重新发送ACK报文，而是发送RST（复位）报文，进入CLOSED状态，防止SYN泛洪攻击。 为什么断开连接要四次挥手？ 因为TCP连接是全双工的网络协议，允许同时通信的双方同时进行数据的收发，同样也允许收发两个方向的连接被独立关闭，以避免client数据发送完毕，向server发送FIN关闭连接，而server还有发送到client的数据没有发送完毕的情况。所以关闭TCP连接需要进行四次握手，每次关闭一个方向上的连接需要FIN和ACK两次握手。 TCP连接数过高怎么办？ 首先可以通过 netstat 命令查看当前的TCP连接哪种状态比较多，如果存在大量的 TIME_WAIT，则可以通过修改内核参数解决，修改 /etc/sysctl.conf 文件，添加控制 TIME_WAIT 数量的参数，一般设置为30左右，然后再执行 /sbin/sysctl -p 命令让参数生效。 产生大量TIME_WAIT的原因有哪些？ TIME_WAIT状态是指TCP连接在关闭后等待一段时间的状态。这个状态的存在是为了确保网络中所有传输的数据都被正确接收和处理。产生大量TIME_WAIT状态的原因可以有以下几种： 主动关闭连接的一方在关闭连接后，可能需要在TIME_WAIT状态等待一段时间，以确保远程主机确认连接关闭。这是TCP协议的一部分，可以防止已关闭的连接的数据包在网络中被误解为新连接的数据包。如果有大量的主动关闭连接的操作，那么就会导致大量的TIME_WAIT状态的产生。 网络中存在丢包或延迟较大的情况，这会导致TCP协议无法及时接收到远程主机的连接关闭确认。在这种情况下，主动关闭连接的一方可能会在TIME_WAIT状态等待更长的时间，以确保确认收到。 连接频繁地建立和关闭。如果一个应用程序频繁地建立和关闭连接，就会导致大量的TIME_WAIT状态的产生。这可能是由于应用程序设计不当，或者存在某些错误导致连接频繁断开。 操作系统资源限制。操作系统对于TIME_WAIT状态的处理是有限制的，通常会限制同时存在的TIME_WAIT状态的数量。如果操作系统资源不足，就可能导致TIME_WAIT状态的积累。 1.6 应用层1.6.1 HTTP和HTTPS HTTP常见状态码： 1开头：代表请求成功。 200：这是最常见的状态码，表示服务端已经接受到客户端的请求，也给客户端返回了结果。 202：表示服务端已经接受了请求，但还未处理。 301：永久重定向，表示请求的资源已经被分配到新的URL。 302：临时重定向。 400：请求报文有误。 403：服务器拒绝此次访问，一般是权限问题。 404：访问失败。 500：服务器执行请求时发生错误。 503：服务器处于超负载或停机维护，无法处理请求。 HTTP的长连接和短连接： HTTP/1.0默认使用短链接，是一种非流水线工作方式，也就是说客户端向服务端每进行一次请求，就建立一次连接，收到响应就断开连接。 HTTP/1.1默认使用长连接，是一种流水线工作方式，也就是说客户端和服务端建立连接，在客户端收到响应后并不会断开连接，而是等设定的时间来断开连接。 HTTP/1.0和HTTP/1.1的区别： 长连接 节约宽带 HOST域 HTTP/1.1和HTTP/2.0的区别： 多路复用 二进制分帧 首部压缩 服务器推送 GET 和 POST 的区别： GET 获取数据，POST 传送数据 对于 GET 方式的请求，浏览器会把 http header 和 data 一并发送出去，服务器响应200；而对于POST，浏览器先发送header，服务器响应100，浏览器再发送 data，服务器响应200。 HTTP 和 HTTPS 的区别： HTTP 即超文本传输协议，通过 TCP 80 端口进行传输，而 HTTPS 是HTTP + SSL 协议构建的可进行加密传输、身份认证的网络协议，比 HTTP 更加安全，通过 TCP 443 端口进行传输。 HTTPS 需要到 CA 申请证书，一般免费证书较少，因此需要一定费用。 HTTP 信息是明文传输，HTTPS 是具有安全性的 SSL 加密传输协议。 HTTPS 加密过程： 服务器生成一对公钥A，密钥B 浏览器向服务器发起请求的话，就发送公钥A给浏览器 浏览器拿到公钥A后，随机生成密钥C，通过公钥A进行加密并传输给服务器 服务器拿到后，通过密钥B解密获得密钥C，这时候两边都持有密钥C，就可以进行对称加解密 1.6.2 DNSDNS 即域名系统，在互联网中为域名和IP地址进行相互映射的一个分布式数据库，采用UDP协议，使用UDP53端口进行传输。 DNS 中，主机向本地域名服务器的查询一般都是采用递归查询，如果主机所询问的本地域名服务器不知道被查询域名的IP地址时，那么本地域名服务器就会以DNS客户的身份向其它域名服务器发送查询请求，就是代替主机继续查询；本地域名服务器向根域名服务器的查询通常是采用迭代查询，当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地域名服务器下一步应当向哪一个域名服务器进行查询，然后让本地域名服务器进行后续的查询，查询到结果就会回送给源主机。 1.6.3 DHCPDHCP 即动态主机配置协议，是用与局域网的协议，采用UDP进行传输，服务端采用67端口，客户端采用68端口。 客户端会以广播的形式发送 DHCP 发现报文 DHCP 服务端收到之后就会发送 offer 报文，客户端只能接收一个 offer 报文，通常只接收第一个 客户端受到 offer 报文之后会发送一个请求报文，字段中包含选中的 DHCP 地址和 IP 地址 DHCP 服务端收到后会判断字段是否相同，相同就会给出一个确认报文，并携带 IP 地址和租期信息 客户端收到确认报文后就会检查是否可用，如果不可用就会发送一个报文通知 DHCP 服务端该地址不可用 二、Linux方面2.1 命令 基本命令： 查看硬件信息：ifconfig、free、fdisk 查看系统性能信息：top、ps、iostat、lsof、netstat、df、mount、umount 系统安全相关：chmod、chown、chgrp、passwd、su、sudo 三剑客：grep、sed、awk 其它：ls、ll、cd、pwd、rpm、yum、firewall-cmd、date、clear、echo、rm、touch、mkdir、mv、cp、find、uniq、sort 2.2 vim vim的三种模式： 普通模式：vim + 文件名即可进入普通模式 插入模式：普通模式下输入i、a、o即可进入插入模式 命令模式：输入：即可进入命令模式，输入Esc退出命令模式 2.3 Linux进程 Linux系统进程可以分为： 交互进程：由一个shell终端启动的进程，在执行过程中，需要与用户进行交互操作，可以运行于前台，也可以运行在后台。 批处理进程：该进程是一个进程集合，负责按顺序启动其他的进程。 守护进程：守护进程是一直运行的一种进程，经常在Linux系统启动时启动，在系统关闭时终止。 2.4 run levelLinux在完成核内引导以后，就开始运行init程序，包括以下几个运行级： 0 系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 1 单用户工作状态，root权限，用于系统维护，禁止远程登陆 2 多用户状态(没有NFS) 3 完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 4 系统未使用，保留 5 代表图形GUI模式 6 系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 2.5 标准输入输出和错误输出重定向 0 代表标准输入，即stdin（standard input） 1 代表标准输出，即stdout（standard output） 2 代表标准错误输出，即stderr（standard error） 2.6 Linux系统启动过程 内核的引导：当计算机打开电源后，首先是BIOS开机自检，按照BIOS中设置的启动设备（通常是硬盘）来启动。操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 运行init：init 进程是系统所有进程的起点，首先是需要读取配置文件 /etc/inittab。启动时根据”运行级别”，确定要运行哪些程序。 系统初始化：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务。 建立终端：基本系统环境已经设置好了，各种守护进程也已经启动了，init接下来会打开6个终端，以便用户登录系统，同时它会显示一个文本登录界面，在这个登录界面中会提示用户输入用户名。 用户登陆系统：命令行登录、ssh登录、图形界面登录。 2.7 iptables iptables 的工作过程：采用数据包过滤机制工作，当防火墙受到数据包时，iptables 是一层一层进行过滤的，规则顺序从上到下，从前到后进行过滤，即匹配了上面的规则，那么就不会匹配下面的规则了，如果所有规则都不能匹配，就使用默认规则。 四表（tables）： filter：进行包过滤处理 nat：对数据地址信息和数据包端口信息进行转换 mangle：对数据包信息进行标记 raw：将数据包一些标记信息进行拆解 五链（chains）： input：过滤进入主机的数据包 forward：处理经过主机的数据包，与 nat 表有关 output：处理从主机发出的数据包 prerouting：数据包到达防火墙时进行分路由判断之前执行的规则，改变数据包的目的地址、目的端口 postrouting：数据包离开防火墙时进行分路由判断之前执行的规则，改变数据包的目的地址、目的端口 三、其它3.1 Nginx和Apache nginx和apache的区别： nginx和apache都是web服务器，但两者适应的场景不同，也就是两者专注于解决不同的问题。 nginx：高并发处理能力强，擅长处理静态请求、反向代理、负载均衡；动态请求处理能力不强。 apache：稳定、对动态请求处理强，rewrite能力比nginx强，模块多，bug相对较少；但不擅长高并发处理，耗费的资源较多。 3.2 正向代理和反向代理 正向代理：正向代理是一个位于客户端和服务器之间的代理服务器。为了从服务器取得内容，客户端向代理服务器发送一个请求，并且指定目标服务器，之后代理服务器向目标服务器转发并且将获得的内容返回给客户端。正向代理的情况下客户端必须要进行一些特别的设置才能使用。 反向代理：反向代理正相反，对于客户端来说，代理服务器就像是目标服务器，客户端不需要做特别的设置。客户端向反向代理服务器发送请求，反向代理服务器会自行判断，将请求转发给内部的目标服务器，并将响应回送给客户端，使得这些响应是它自己的一样。 正向代理和反向代理的区别：正向代理需要客户端主动设置代理服务器ip或者域名进行访问，由设置的服务器去获取访问内容并返回；而反向代理不需要做任何设置，直接访问服务器真实ip或域名，但是服务器内部会自动根据访问内容进行跳转及内容返回，客户端不知道它最终访问的是哪些机器。 3.3 虚拟化和容器 虚拟化：通过模拟计算机的硬件，来实现在同一台计算机上同时运行不同的操作系统的技术，常用的vmware、openstack、kvm都是使用的虚拟化技术。 容器：容器就是在隔离环境运行的一个进程，如果进程停止，容器就会销毁。隔离的环境拥有自己的系统文件，IP地址，主机名等 虚拟化和容器的区别：虚拟化需要硬件支持，需要模拟硬件，可以运行不同的操作系统，启动需要走开机启动流程(分钟级)；容器共用宿主机内核，第一个进程直接启动服务(nginx，mysql等)，开机秒级，轻量，损耗少。 3.4 CDN CDN即内容分发网络，原理就是采用各种缓存服务器，将这些缓存服务器部署到用户访问相对集中的地区，用户访问网站时，利用全局负载技术将用户的访问指向距离最近的工作正常的缓存服务器上，由缓存服务器直接响应用户请求，从而提高用户体验。 CDN功能： 节省骨干网带宽，减少带宽需求量。 提供服务器端加速，解决由于用户访问量大造成的服务器过载问题。 服务商能使用Web Cache技术在本地缓存用户访问过的Web页面和对象，实现相同对象的访问无须占用主干的出口带宽，并提高用户访问因特网页面的相应时间的需求。 能克服网站分布不均的问题，并且能降低网站自身建设和维护成本。 降低“通信风暴”的影响，提高网络访问的稳定性。 3.5 C/S和B/S C/S和B/S的区别： C/S即Client/Server（客户端/服务器）架构，是一个典型的两层架构。通过将任务合理分配到客户端和服务器，降低了系统的通讯开销，需要安装客户端才可进行管理操作。 B/S即即Brower/Server（浏览器/服务器）架构，统一了客户端，无需特殊安装，拥有Web浏览器即可；它将系统功能实现的核心部分集中到服务器上，简化了系统的开发、维护和使用。 3.6 输入一个网址后发生了什么？ DNS解析，会根据输入的URL查找对应的IP 首先查找本地浏览器缓存，浏览器会保存近期访问过的一些地址的DNS信息。 如果浏览器缓存没有，会尝试调用系统缓存和 host 文件查找DNS信息。 如果系统缓存没有，就会发送请求到路由器上，查找路由器缓存中的DNS信息。 如果路由器没有，就会发送请求到本地域名服务器，查找方式为递归查找。 如果本地域名服务器没有，就会向根域名服务器查找，查找方式为迭代查找。 最后就是浏览器得到了对应的IP，或者没有，即这个URL不存在。 TCP连接，浏览器向目标服务器进行TCP三次握手连接 浏览器发送HTTP请求 服务器响应HTTP请求 浏览器收到响应后，解析渲染页面 关闭TCP连接 3.7 进程和线程的区别 进程：进程是操作系统资源分配的最小单位，每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1–n个线程。 线程：线程是 CPU 独立调度的最小单位，同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。 线程和进程的生命周期：新建、就绪、运行、阻塞、死亡。 3.8 软连接和硬链接的区别 软连接： 类似于快捷方式 可以跨文件系统 可以对一个不存在的文件名进行链接，硬链接必须要有源文件 可以对目录进行链接 硬连接： 以文件副本的形式存在，但不占用实际空间 不允许给目录创建硬链接 只有在同一个文件系统中才能创建 删除其中一个硬链接文件并不影响其他有相同 inode 号的文件 3.9 Dos 和 DDos Dos 即拒绝服务，就是向目标服务器发送大量的虚拟 ip 请求，被攻击的服务器在收到请求后返回确认信息，等待攻击者确认，但由于请求的是虚拟 ip，服务器收不到回复，那么在一段时间内服务器会处于等待的状态，分配给虚拟 ip 请求的资源也没有释放，攻击者等待一段时间会因连接超时而断开，接着再发送大量新的请求，不断消耗服务器资源，最终导致瘫痪。 DDos 即分布式拒绝服务，Dos 攻击是单机与单机之间的攻击模式，而 DDos 是利用一批僵尸主机向服务器同时发送攻击的模式。 3.10 四层负载均衡和七层负载均衡 四层负载均衡：主要是通过网络层和传输层的流量来实现基于 IP 加端口的负载均衡 七层负载均衡：是基于应用层的负载均衡，支持各种应用层协议 3.11 cookie session token cookie：由服务器生成，存储在浏览器中，用于保存客户登录等信息 session：由服务器给浏览器生成的身份标识，用于区分不同客户端 token：由服务器产生，如果客户端通过用户名和密码请求认证且认证成功，那么服务端就会发送一个 token 给回客户端，客户端每次请求都可以带上 token 来证明自己的合法地位 3.12 CPU上下文Linux 是一个支持多任务的操作系统，支持远大于 CPU 数量的任务同时运行，其实这些任务不是真正在同时运行，而是系统在很短时间内，将 CPU 轮流分配给它们，速度极快造成了像是在同时运行的错觉。而每个任务在运行前，CPU 都需要知道任务从哪里加载、从哪里运行，也就是需要系统为它们设置好 CPU 寄存器和程序计数器。 CPU 寄存器：是 CPU 内置的容量很小但速度极快的内存。 程序计数器：用来存储 CPU 正在执行的指令位置，或者即将执行的下一条指令位置。 CPU 寄存器和程序计数器都是 CPU 在运行任何任务前，必须依赖的环境，就被称为 CPU 上下文。 CPU 上下文切换就是指把先前一个任务的 CPU 上下文（CPU 寄存器和程序计数器的位置）保存起来，再加载新任务的上下文到这些寄存器和计数器，最后再跳转到程序计数器所指的新位置，运行新任务。而保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再加载进来，这样就能保证任务原来的状态不受影响，让任务看起来像是连续运行。 进程上下文切换 进程的运行空间分为内核空间和用户空间，分别对应 CPU 特权等级的 Ring0 和 Ring3。 内核空间（Ring0）具有最高权限，可以直接访问所有资源；用户空间（Ring3）只能访问受限资源，不能直接访问系统资源，必须通过系统调用陷入到内核才可以使用。 所以说进程可以在内核空间和用户空间运行，在前者运行被称为内核态，后者被称为用户态。从用户态到系统态的转变，就需要通过系统调用，比如查看文件内容就要依次调用 open()、read()、close()。在这期间就发生了上下文切换，CPU 寄存器里原来的用户态的指令位置需存起来，然后更行为内核态指令的新位置，最后跳转到内核态运行任务。系统调用结束后，CPU 寄存器需要恢复到原来用户态的指令位置，所以一次系统调用是发生了两次上下文切换。但系统调用又和进程上下文切换不太一样，进程上下文切换时一个进程切换到另一个进程运行，系统调用是在通过一个进程上运行的，后者常被称为特权模式切换。 3.13 Raid0 Raid1 Raid5 Raid6 Raid0：最少需要一块磁盘，原理是将多个磁盘组成一个大的磁盘，读写性能和存储容量都以磁盘数量倍数增加，数据在写入前都会进行分片的操作，存入不同的磁盘中，读写速度是所有Raid类型中最快的，容量也是最大的，但也是最不安全的，因为只要一块磁盘坏了数据就会丢失，且无法恢复。 Raid1：最少需要两块磁盘，相当于一个数据存到多个磁盘中，只要剩下最少一块磁盘数据就不会丢失，是最安全的，但也是性能最低的，尽管有100个10G的磁盘组成Raid1，容量也是10G，且读写速度和一块磁盘时一样。 Raid5：最少需要三块磁盘，每块磁盘1/3的空间用来存校验信息，2/3的空间用来存数据，三块磁盘组成Raid5的话允许坏掉一块磁盘，因为剩下两块磁盘会通过校验信息来恢复数据，读写速度和安全性是属于折中的。 Raid6：最少需要四块磁盘，比Raid5多了一个校验位，所以安全性有更大的提升，四块磁盘组成的Raid6允许坏掉两块磁盘，但相比域Raid5写的性能变得更差。","link":"/2024/03/31/%E5%85%AB%E8%82%A1%E6%96%87%E9%9A%8F%E8%AE%B0%E5%BD%95/"},{"title":"Zookeeper","text":"ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 Hadoop 和 Hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 一、Zookeeper基础1.1 使用场景 分布式协调组件：通过 watch 机制可以协调好节点之间的数据一致性 分布式锁：通过分布式锁可以做到强一致性 无状态化实现 负载均衡 数据发布/订阅 命名服务 1.2 部署docker-compose.yaml 123456789101112version: '3.8'services: zookeeper: container_name: zk01 image: zookeeper:3.7.0 restart: always hostname: zk01 ports: - 2181:2181 environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zk01:2888:3888;2181 zoo.cfg 1234567891011121314151617181920dataDir=/datadataLogDir=/datalog# 基本时间配置（毫秒）tickTime=2000# 初始化连接到leader的最大时长，单位为倍数，即初始化时间为tickTime * initLimitinitLimit=5# follower与leader数据同步的最大时长syncLimit=2# 保存数据的快照数量autopurge.snapRetainCount=3# 自动触发清除任务时间间隔，以小时为单位，默认为0，表不清除autopurge.purgeInterval=0# 客户端与zk的最大并发连接数maxClientCnxns=60# 开启standaloneEnabled模式，即独立部署standaloneEnabled=true# 开启adminServeradmin.enableServer=true# 2181是为客户端提供的端口server.1=zk01:2888:3888;2181 1.3 基本命令 启动|关闭|查看状态 1zkServer.sh start|stop|status 进入 zk 1zkCli.sh 查看内部数据结构 1ls [path] 二、内部数据结构2.1 是如何存储数据的Zookeeper 中的数据是保存在节点上的，即 znode，多个 znode 就构成一个树的结构。 如图，a 和 b 就是 Zookeeper 的 znode，创建 znode 方式如下 12345create /[znode_name]# 创建节点并创建一个数据create /[znode_name] [data_name]# 获取数据get [znode_name] 2.2 znode结构Zookeeper 中的 zonode，包含以下几个部分： data：保存数据 acl：权限 c：创建权限 w：写权限 r：读权限 d：删除权限 a：admin 管理者权限 stat：描述当前 znode 的元数据 child：当前节点的子节点 12345678910111213# 查看znode详细信息get -s /[znode_name]cZxid:创建节点的事务IDctime:创建节点时间mZxid:修改节点的事务IDmtime:修改节点时间pZxid:添加和删除子节点的事务IDcversion:当前节点的子节点版本号，初始值为-1，每对该节点的子节点进行操作，这个cversion都会自动增加dataVersion:数据版本初识版本为0，每对该节点的数据进行操作，这个dataVersion都会自动增加aclVersion:权限版本ephemeralOwne:如果当前节点是临时节点，该值是当前节点的session id，如果不是临时节点则为0dataLength:数据长度numChildren:该节点的子节点个数 2.3 znode类型 持久节点：在会话结束后仍会存在 持久序号节点：根据先后顺序，会在结点之后带上一个数值，适用于分布式锁的场景（单调递增） 临时节点：会话结束后会自动删除，适用于注册与服务发现的场景 临时序号节点：跟持久序号节点相同，适用于分布式锁的场景 容器节点：当容器节点中没有任何子节点时，该容器节点会被定期删除（60s） TTL 节点：可以指定节点的到期时间 持久序号节点创建 1create -s /[znode_name] 临时节点创建 1create -e /[znode_name] 临时序号节点创建 1create -e -s /[znode_name] 容器节点创建 1create -c /[znode_name] TTL 节点创建 12# 通过系统配置开启zookeeper.extendedTypesEnabled=true 持久节点 持久节点在创建后服务端会发送一个 session id，并一直保留着。 临时节点 临时节点在创建时后服务器也会发送一个 session id，在会话持续的过程中客户端会不断向服务端续约 session id 的时间，当客户端没有继续续约，而服务端内部的计时器到期时，就会将该 session id 所对应的 znode 全部删除。 2.4 持久化机制Zookeeper 的数据是运行在内存中的，所以提供了两种持久化机制： 事务日志：Zookeeper 将执行过的命令以日志的形式存储在 dataLogDir / dataDir 中，类似于 redis 的 AOF 数据快照：在一定时间间隔内做一次数据快照，存储在快照文件中（snapshot），类似于 redis 的RDB Zookeeper 通过这两种持久化机制，在恢复数据时先将快照文件中的数据恢复到内存中，再用日志文件中的数据做增量恢复，可以实现高效的持久化。 三、zkCli的使用 递归查询 1ls -R /[znode_name] 删除节点 1deleteall /[znode_name] 乐观锁删除 1delete -v [version] /[znode_name] 给当前会话注册用户，并创建节点赋予该用户权限 12addauth digest [user]:[password]create /[znode_name] auth:[user]:[password]:[privileges] 四、分布式锁在分布式的环境下，如果在一个节点去上了个锁，当请求被负载均衡分配到了其它节点，那么锁就无法形成互斥，所以节点之间使用 Zookeeper，做一个协调中心，将锁上传到 Zookeeper，其它节点要用到就去 Zookeeper 拿这个锁，这就是分布式锁。 Zookeeper 锁的分类： 读锁：大家都可以读，前提是之前没有写锁。（读锁比喻成约会，大家都有机会和女神约会，约会前提是女神没结婚） 写锁：只有写锁才能写，前提是不能有任何锁。（写锁比喻成结婚，结婚后只有老公能和女神约会，结婚前提是女神和其他人的关系断干净了） 4.1 上读锁 创建一个临时序号节点，节点数据是 read，表示为读锁 获取当前 Zookeeper 中序号比自己小的所有节点 判断最小节点是否为读锁： 如果是读锁：则上锁失败，因为如果最小节点是读锁，那么后面就不可能有写锁，接着为最小节点设置监听，Zookeeper 的 watch 机制会在最小节点发生变化时通知当前节点，再进行后面的步骤，被称为阻塞等待 如果不是读锁：则上锁成功 4.2 上写锁 创建一个临时序号节点，节点数据是 write，表示为写锁 获取 Zookeeper 中的所有节点 判断自己是否为最小节点： 如果是：上锁成功 如果不是：说明前面还有锁，所以上锁失败，接着监听最小节点，如果最小节点发生变化，则重新进行第二步 羊群效应 假设有一百个请求都是要去写锁，那么就会有一百个请求去监听最小节点，那么 Zookeeper 的压力就会非常大，解决方法是将这一百个请求按请求顺序排列，后一个请求去监听前一个请求即可，实现链式监听。 4.3 watch机制Zookeeper 的 watch 可以看作是一个触发器，当监控的 znode 发生改变，就会触发 znode 上注册的对应事件，请求 watch 的客户端就会接收到异步通知。 zkCli.sh 中使用 watch 1234567create /test# 一次性监听，监听节点内容get -w /test# 监听目录，但所监听节点下创建和删除子节点不会触发监听ls -w /test# 与上面相对，都会触发监听ls -R -w /test 五、集群部署Zookeeper 的集群角色有三个： Leader：处理集群所有事务的请求，集群只有一个 Leader Follower：只处理读请求，参与 Leader 选举 Observer：只处理读请求，提升集群的性能，但不能参与 Leader 选举 docker-compose.yaml 1234567891011121314151617181920212223242526272829303132333435363738version: '3.8'services: zk01: container_name: zk01 image: zookeeper:3.7.0 restart: always hostname: zk01 ports: - 2181:2181 environment: ZOO_MY_ID: 1 # 2888:用于集群内zk之间的通信 # 3888:用于选举投票 # 2181:客户端使用 # 要创建observer则在2181端口后加:observer ZOO_SERVERS: server.1=zk01:2888:3888;2181 server.2=zk02:2888:3888;2181 server.3=zk03:2888:3888;2181 zk02: container_name: zk02 image: zookeeper:3.7.0 restart: always hostname: zk02 ports: - 2182:2181 environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zk01:2888:3888;2181 server.2=zk02:2888:3888;2181 server.3=zk03:2888:3888;2181 zk03: container_name: zk03 image: zookeeper:3.7.0 restart: always hostname: zk03 ports: - 2183:2181 environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zk01:2888:3888;2181 server.2=zk02:2888:3888;2181 server.3=zk03:2888:3888;2181 通过命令查看节点角色 12zkServer.sh statusMode: leader 连接集群 1zkCli.sh -server zk01:2181,zk02:2181,zk03:2181 5.1 ZAB协议ZAB（Zookeeper Atomic Broadcast）即 Zookeeper 原子广播协议，通过这个协议解决了集群数据一致性和崩溃恢复的问题。 ZAB 协议中节点的四种状态 Looking：选举状态 Following Leading Observing 初始化集群时 leader 的选举 当集群中两台节点启动时，就会开始 leader 的选举，选票的格式为 (myid,zXid) 第一轮投票时，每个节点会生成自己的选票，即自己的 (myid,zXid)，然后将选票给到对方，这时候每个节点就会有两张选票，即自己的和对方节点的 接着就会比较两张选票的 zXid，如果都相同就对比 myid，将大的一票投到投票箱中 第二轮投票时，每个节点会将上一轮投出去的选票给到其它节点，然后再对比 (myid,zXid)，将大的一票投出去，就能够选出 leader 后来新启动的节点会发现已经有 leader了，就不用做选举的过程了 可以看出初始化集群时，leader 的选举主要看 myid 崩溃恢复时的 leader 选举 在 leader 确定了之后，leader 会周期性地向 follower 发送心跳包，当 follower 没有收到 leader 发送过来的心跳包，就会进入选举过程，这时候集群不能对外提供服务。 当 leader 挂了之后，follower 的状态会变成 looking 接着就进行选举投票，过程和初始化集群时一样 5.2 主从同步原理 5.3 NIO和BIONIO 用于被客户端连接的 2181 端口，使用的就是 NIO 的连接模式；客户端开启 watch 时，使用的也是 NIO。 BIO 集群在进行选举时，多个节点之间的通信端口，使用的是 BIO 的连接模式。","link":"/2024/02/18/zookeeper/"},{"title":"关闭 NeuVector 的 nvprotect 机制","text":"NeuVector 有一个名为 nvprotect 的内部保护机制，用于限制用户对 NeuVector pod 的访问权限。 例如 sh、ls 等命令是无法使用的： 如果需要关闭，可以通过接口进行关闭，此处提供脚本，支持关闭 Controller、Scanner、Enforcer 的 nvprotect。 使用方法： 12345678910git clone https://github.com/warnerchen/disable-nvprotect.gitcd disable-nvprotectchmod +x script.sh# 关闭 nvprotect# 关闭 enforcer 即可同时关闭 scanner 的 nvprotect./script.sh off (controller|enforcer)# 开启 nvprotect./script.sh on (controller|enforcer)","link":"/2025/02/05/%E5%85%B3%E9%97%AD-NeuVector-%E7%9A%84-nvprotect-%E6%9C%BA%E5%88%B6/"},{"title":"容器网络实现","text":"容器的网络是基于 linux 的网络命名空间(networke namespace)和虚拟网络设备(veth pair)实现的。","link":"/2024/03/18/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0/"},{"title":"快速安装containerd和nerdctl","text":"记录节点安装 Containerd 和 Nerdctl 步骤。 1234567891011121314151617181920212223242526272829303132333435# Download# 版本灵活变动export CONTAINERD_VERSION=1.7.13export CNI_PLUGIN_VERSION=v1.4.0export RUNC_VERSION=v1.1.11export NERDCTL_VERSION=1.7.4wget &quot;https://github.com/containerd/containerd/releases/download/v$CONTAINERD_VERSION/containerd-v$CONTAINERD_VERSION-linux-amd64.tar.gz&quot;wget &quot;https://github.com/containernetworking/plugins/releases/download/$CNI_PLUGIN_VERSION/cni-plugins-linux-amd64-$CNI_PLUGIN_VERSION.tgz&quot;wget &quot;https://github.com/opencontainers/runc/releases/download/$RUNC_VERSION/runc.amd64&quot;wget &quot;https://github.com/containerd/nerdctl/releases/download/v$NERDCTL_VERSION/nerdctl-$NERDCTL_VERSION-linux-amd64.tar.gz&quot;# Installtar -Czvxf /usr/local/bin containerd-$CONTAINERD_VERSION-linux-amd64.tar.gzmv /usr/local/bin/bin/* /usr/local/bin/ &amp;&amp; rm -rf /usr/local/bin/binmkdir -p /opt/cni/bin &amp;&amp; tar Czvxf /opt/cni/bin cni-plugins-linux-amd64-$CNI_PLUGIN_VERSION.tgzchmod 755 runc.amd64 &amp;&amp; mv runc.amd64 /usr/local/bin/runctar Czvxf /usr/local/bin nerdctl-$NERDCTL_VERSION-linux-amd64.tar.gz # Configmkdir /etc/containerdcontainerd config default &gt; /etc/containerd/config.tomlvim /etc/containerd/config.toml[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;k8s.gcr.io&quot;] endpoint = [&quot;https://k8s-gcr.m.daocloud.io&quot;]...# Enable# cp https://github.com/containerd/containerd/blob/v$CONTAINERD_VERSION/containerd.service to /etc/systemd/system/containerd.servicesystemctl daemon-reloadsystemctl enable containerd --now # Testnerdctl info","link":"/2024/02/29/%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85containerd%E5%92%8Cnerdctl/"},{"title":"节点根目录被打满导致的ETCD憨批修复记录","text":"背景事情发生在 UAT 环境的其中一台 Controller 节点，节点根目录被打满，同时 etcd 数据没有落盘到独立的磁盘中，导致 etcd 憨批，节点出现 notready 修复过程参考了各种网络资料，最终形成如下修复手段: 移除 statis pod yaml，从而停止坏掉的 etcd pod 通过 etcdctl member remove 移除坏掉的 etcd 实例 备份数据目录并移除 通过 etcdctl member add 添加新实例，记录 etcdctl 输出的配置信息 通过裸起容器的方式，启动 etcd 容器，启动需要用到的参数，参考 statis pod yaml 和第 4 步输出的配置信息 启动后会与 leader 进行数据的同步，可以通过 etcdctl endpoint status -w table 查看状态 如果同步成功则可以停止 etcd 容器，将 statis pod yaml 放回对应的目录中，集群修复 具体的操作命令: 12345678910111213141516171819202122232425262728293031323334353637383940# stop issue etcd podmv /etc/kubernetes/manifests/etcd.yaml .# init etcdctl command envsexport endpoints=&quot;https://10.82.69.10:2379,https://10.82.69.11:2379,https://10.82.69.12:2379,https://10.82.69.19:2379,https://10.66.10.83:2379&quot;export cacert=&quot;/etc/kubernetes/pki/etcd/ca.crt&quot;export cert=&quot;/etc/kubernetes/pki/etcd/peer.crt&quot;export key=&quot;/etc/kubernetes/pki/etcd/peer.key&quot;# sample: e member list -w table alias e=&quot;etcdctl --endpoints $endpoints --cacert $cacert --cert $cert --key $key&quot;# or use this oneeval $(kubectl get nodes -owide|grep -E &quot;etcd|control-plane&quot; |awk '{printf &quot;https://&quot;$6&quot;:2379,&quot;}'|awk '{gsub(&quot;,$&quot;,&quot;&quot;);print &quot;export ETCDCTL_ENDPOINTS=\\&quot;&quot;$1&quot;\\&quot;&quot;}') &amp;&amp; export ETCDCTL_CACERT=/etc/kubernetes/ssl/etcd/ca.crt &amp;&amp; export ETCDCTL_CERT=/etc/kubernetes/ssl/etcd/peer.crt &amp;&amp; export ETCDCTL_KEY=/etc/kubernetes/ssl/etcd/peer.key# remove issue etcd memberetcdctl member remove $issue_etcd_id# delete etcd datarm -rf /var/lib/etcd/*# member addetcdctl member add wcn-gduvm-mwdcm1 --peer-urls=https://10.82.69.10:2380# start a temporary etcd pod to restorenerdctl run -d --name restore_etcd \\ -v /etc/kubernetes/ssl/etcd:/etc/kubernetes/ssl/etcd \\ -v /var/lib/etcd:/var/lib/etcd \\ --network=host \\ -e ETCD_NAME=&quot;wcn-gduvm-mwdcm1&quot; \\ -e ETCD_INITIAL_CLUSTER=&quot;wcn-gduvm-mwdcm2=https://10.82.69.11:2380,wcn-gduvm-mwdcm1=https://10.82.69.10:2380,wcn-gduvm-mwdcm3=https://10.82.69.12:2380&quot; \\ -e ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.82.69.10:2380&quot; \\ -e ETCD_INITIAL_CLUSTER_STATE=&quot;existing&quot; \\ --entrypoint=etcd 10.82.49.238/quay.io/coreos/etcd:v3.5.6 --advertise-client-urls=https://10.82.69.10:2379 --auto-compaction-retention=8 --cert-file=/etc/kubernetes/ssl/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --election-timeout=5000 --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --heartbeat-interval=250 --key-file=/etc/kubernetes/ssl/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://10.82.69.10:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://10.82.69.10:2380 --metrics=basic --peer-cert-file=/etc/kubernetes/ssl/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/ssl/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/ssl/etcd/ca.crt# wait for the etcd pod running, if use kubectl and etcdctl to see that both node and member are restored, we can stop itnerdctl stop restore_etcd# start etcd podmv ./etcd.yaml /etc/kubernetes/manifests/etcd.yaml","link":"/2024/06/29/%E8%8A%82%E7%82%B9%E6%A0%B9%E7%9B%AE%E5%BD%95%E8%A2%AB%E6%89%93%E6%BB%A1%E5%AF%BC%E8%87%B4%E7%9A%84ETCD%E6%86%A8%E6%89%B9%E4%BF%AE%E5%A4%8D%E8%AE%B0%E5%BD%95/"},{"title":"Redis","text":"一、简介REmote DIctionary Server（Redis）是一个由 Salvatore Sanfilippo 写的 key-value 存储系统，是跨平台的非关系型数据库。 Redis 就是一款 NoSQL，而 NoSQL 就是指非关系型数据库，主要分为四种： 键值型：Redis 文档型：ElasticSearch、Mongdb 面向列：Hbase 图形化：Neo4j 二、Redis基础2.1 Redis安装 通过 docker-compose 安装 redis 12345678910version: '3.8'services: redis: image: daocloud.io/library/redis:5.0.9 restart: always container_name: redis environment: - TZ=Asia/Shanghai ports: - 6379:6379 进入容器内部测试 redis 123456# 连接redisredis-cli# set新建键值对set key value# get获取键值get key 2.2 Redis常用命令redis 的数据存储结构有以下几种： key-string（字符串）：一个 key 对应一个值 key-hash（哈希）：一个 key 对应一个 map key-list（列表）：一个 key 对应一个列表 key-set（集合）：一个 key 对应一个集合 key-zset（有序集合）：一个 key 对应一个有序的集合 2.2.1 string常用命令 设置值 1set key value 取值 1get key 批量操作 12mset key1 value1 key2 value2 ...mget key1 key2 ... 自增 1incr key 自减 1decr key 自增自减指定数量 12incrby key numberdecrby key number 设置值的同时指定生存时间 1setex key seconds value 设置值，如果当前 key 不存在如同 set，如果存在则说明都不做 1setex key value 在 key 对应的 value 后追加内容 1append key value 查看 value 字符串长度 1strlen key 2.2.2 hash常用命令 存储数据 1hset key field value 获取数据 1hget key field 批量操作 12hmset key1 field1 value1 field2 value2 ...hmget key1 firle1 field2 ... 指定自增 1hincrby key field number 设置值，如果当前 key 不存在如同 set，如果存在则说明都不做 1hsetnx key field value 检查 field 是否存在 1hexists key field 删除某个 field 1hdel key field1 field2 ... 获取当前 hash 结构中的全部 field 和 value 1hgetall key 获取当前 hash 结构中的全部 field 1hkeys key 获取当前 hash 结构中的全部 value 1hvals key 获取当前 hash 中 field 的数量 1hlen key 2.2.3 list常用命令 存储数据 1234567891011# 从左侧插入数据lpush key value1 value2 ...# 从右侧插入数据rpush key value1 value2 ...# 如果key不存在，什么都不做，如果key存在但不是list结构，也什么都不做lpushx key value1 value2 ...rpushx key value1 value2 ...# 通过索引位置添加valuelset key index value 获取数据 12345678910111213# 左侧弹出数据并移除lpop key# 右侧弹出数据并移除rpop key# 获取一定范围的数据，start从0开始，stop为-1时为最后一个value，-2时为倒数第二个valuelrange key start stop# 根据索引位置获取valuelindex key index# 获取整个list的长度llen key 删除数据 12345678# 删除list中count个value的值，当count&gt;0，从左向右删除，但count&lt;0，从右向左删除，但count==0，全部删除lrem key count value# 保留列表中指定范围内的数据，超出这个范围的都会被移除ltrim start stop# 将list1中的最后一个数据弹出，插入到list2中的第一个位置rpoplpush key1 key2 2.2.4 set常用命令 存储数据 12# value不允许重复，且数据无序排列sadd key value1 value2 ... 获取数据 1234567891011121314151617# 获取全部数据smembers key# 随机获取数据，并移除，可加弹出数量spop key number# 取多个set的交集sinter key1 key2 ...# 取多个set的并集sunion key1 key2 ...# 取多个set的差集sdiff key1 key2 ...# 查看当前set是否包含某个值sismember key value 删除数据 1srem key value1 value2 ... 2.2.5 zset常用命令 存储数据 12345# score必须是数值，value不允许重复zadd key score1 value1 score2 value2 ...# 修改score，如果value存在则增加分数，如果不存在则相当于zaddzincrby key number value 获取数据 123456789101112131415161718# 查看指定value的分数zscore key value# 获取value数量zcard key# 根据score范围查询value数量zcount key min max# 根据分数从小到大排序，获取指定范围内的数据，添加了withscores参数会返回value的具体scorezrange key start stop withscores# 从大到小zrevrange key start stop withscores# 根据分数的范围获取数据，如果不希望包括min和max的值可以用(min max)的方式，最大最小值用±inf表示zrangebyscore key min max withscores [limit,offset,count]zrevrangebyscore key max min withscores [limit,offset,count] 删除数据 1zrem key value1 value2 ... 2.2.6 key常用命令 查看所有key 1keys * 查看某个key是否存在 1exists key 删除key 1del key 设置key的生存时间 12345678910111213141516# 单位为sexpire key seconds# 单位为mspexpire key milliseconds# 指定生存到某个时间点expireat key timestamppexpireat key millseconds# 查看key的剩余生存时间，返回-2则key不存在，-1则没设置生存时间ttl keypttl key# 移除生存时间persist key 选择操作的库 12345# redis默认有16个库select 0~15# 移动key到另一个库中move key db 2.2.7 库的常用命令 清空当前所在数据库 1flushdb 清空所有数据库 1flushdball 查看当前库有多少key 1dbsize 查看最后一次操作的时间 1lastsave 实时监控redis接收到的命令 1monitor 三、Redis配置3.1 Redis的AUTH docker-compose.yaml 12345678910111213version: '3.8'services: redis: image: daocloud.io/library/redis:5.0.9 container_name: redis restart: always environment: - TZ=Asia/Shanghai ports: - 6379:6379 volumes: - ./redis.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] 设置 Redis 连接密码 12vim redis.confrequirepass toortoor 1docker-compose up -d 进入 Redis 之后都需要输入密码才可以创建 key 12redis-cliauth toortoor 3.2 Redis的事务Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证： 批量操作在发送 EXEC 命令前被放入队列缓存 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中 一个事务从开始到执行会经历以下几个阶段： 开始事务：multi 命令入队：…… 执行事务：exec 取消事务：discard 监听：在开启事务之前，先通过 watch 监听事务中要操作的 key，如果在事务过程中有其他的客户端修改了 key，那么事务将会被取消 事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。 监听 1watch name age gander 开始事务 1multi 命令入队 123set name cqmset age 22set gander male 执行事务/取消事务 1exec/discard 3.3 Redis的持久化3.3.1 RDB持久化Redis 的配置文件位于 Redis 安装目录, ROB 是默认的持久化机制。 docker-compose.yaml 123456789101112131415version: '3.8'services: redis: image: daocloud.io/library/redis:5.0.9 container_name: redis restart: always environment: - TZ=Asia/Shanghai ports: - 6379:6379 volumes: - ./redis.conf:/usr/local/redis/redis.conf - ./data:/data # 加载redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis.conf 123456789vim redis.conf# 在900秒内有1一个 key 发生了变化，就执行 RDB 持久化save 900 1save 300 10save 60 10000# 开启RDB持久化rdbchecksum yes# RDB持久化名称dbfilename dump.rdb 测试持久化 1234set name cqmset age 22# 关闭redis并保存shutdown save 可以看到 data 目录下多了个 rdb 文件，即使 redis 容器重启也不会造成 key 的丢失 3.3.2 AOF持久化AOF 比起 RDB 有更高的数据安全性，如果同时开启了 AOF 和 RDB，那么前者比后者的优先级更高，且如果先开启了 RDB 在开启 AOF，那么 RDB 中的内容会被 AOF 的内容覆盖。 redis.conf 123456789# 开启AOF持久化appendonly yes# AOF文件名appendfilename appendonly.aof# AOF持久化执行策略# always:每次执行写操作都调用fsync# everysec:最多每秒调用一次fsync# no:根据环境的不同在不确定的时间调用fsyncappendfsync always|everysec|no 重启 docker-compose 测试 123set gander male# 不RDB持久化shutdown nosave 可以看到 data 目录下多了个 aof 文件，即 AOF 持久化生成的文件 3.4 Redis主从架构Redis 的主从架构是指 Master 节点负责写操作，而其余的 Slave 节点负责读操作。 docker-compose.yaml 12345678910111213141516171819202122232425262728293031323334353637383940414243version: '3.8'services: redis-master: container_name: redis-master image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7001:6379 volumes: - ./redis1.conf:/usr/local/redis/redis.conf - ./data1:/data command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis-slave1: container_name: redis-slave1 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7002:6379 volumes: - ./redis2.conf:/usr/local/redis/redis.conf - ./data2:/data command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] # 连接redis-master容器，并将该容器ip地址映射为master links: - redis-master:master redis-slave2: container_name: redis-slave2 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7003:6379 volumes: - ./redis3.conf:/usr/local/redis/redis.conf - ./data3:/data command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] links: - redis-master:master 从节点 redis.conf 12# slaveof &lt;主节点地址&gt; &lt;端口&gt;slaveof master 6379 启动后进入容器内部通过 info 可看到节点信息 3.5 Redis哨兵模式Redis 的主从架构有一个很明显的问题，就是当 Master 节点出现问题宕机后，那么 Redis 集群就没有可以进行写操作的 Redis 了，而哨兵就可以解决该问题。 在每个节点中都会有个哨兵与 Redis 进行连接，且哨兵与哨兵之间也会进行连接，如果 Master 节点出现故障宕机了，那么哨兵们就会选出一个 Slave 来作为新的 Master 来提供写的操作。 docker-compose.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445version: '3.8'services: redis-master: container_name: redis-master image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7001:6379 volumes: - ./redis1.conf:/usr/local/redis/redis.conf - ./data1:/data - ./sentinel1.conf:/data/sentinel.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis-slave1: container_name: redis-slave1 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7002:6379 volumes: - ./redis2.conf:/usr/local/redis/redis.conf - ./data2:/data - ./sentinel2.conf:/data/sentinel.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] links: - redis-master:master redis-slave2: container_name: redis-slave2 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7003:6379 volumes: - ./redis3.conf:/usr/local/redis/redis.conf - ./data3:/data - ./sentinel3.conf:/data/sentinel.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] links: - redis-master:master Master 节点 sentinel.conf 123456# 以守护进程的方式运行redisdaemonize yes# 指定Master节点，sentinel monitor &lt;名称&gt; &lt;ip&gt; &lt;端口&gt; &lt;Slave个数&gt;sentinel monitor master localhost 6379 2# 指定哨兵每隔多久检测一次redis主从架构sentinel down-after-milliseconds master 10000 Slave 节点 sentinel.conf 123daemonize yessentinel monitor master master 6379 2sentinel down-after-milliseconds master 10000 进入容器启动哨兵，当 Master 节点出现问题后，就会在两个 Slave 中选出一个作为新的 Master，而旧的 Master 启动后就会变为新的 Slave 1redis-sentinel /data/sentinel.conf 3.6 Redis集群Redis 集群在保证主从和哨兵的基本功能之外，还能提高 Redis 存储数据的能力，主要的特点如下 Redis 集群是无中心的 Redis 集群有ping-pang的机制 投票机制，集群节点的数量必须是 2n+1 分配了 16484 个 hash 槽，在存储数据时，会对 key 进行 crc16 的算法，并对 16384 进行取余，通过结果分配到对应的节点上，每个节点都有自己维护的 hash 槽 每个主节点都要跟一个从节点，但这里的从节点只管备份，不管查询 集群中半数的节点宕机后，那么集群就瘫痪 docker-compose.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374version: '3.8'services: redis1: container_name: redis1 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7001:7001 - 17001:17001 volumes: - ./conf.d/redis1.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis2: container_name: redis2 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7002:7002 - 17002:17002 volumes: - ./conf.d/redis2.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis3: container_name: redis3 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7003:7003 - 17003:17003 volumes: - ./conf.d/redis3.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis4: container_name: redis4 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7004:7004 - 17004:17004 volumes: - ./conf.d/redis4.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis5: container_name: redis5 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7005:7005 - 17005:17005 volumes: - ./conf.d/redis5.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis6: container_name: redis6 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7006:7006 - 17006:17006 volumes: - ./conf.d/redis6.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis{1..6}.conf，x 为 {1..6} 123456789101112# 指定redis端口port 700x# 开启集群cluster-enabled yes# 集群信息文件cluster-config-file nodes-700x.conf# 集群对外ipcluster-announce-ip 192.168.88.135# 集群对外端口cluster-announce-port 700x# 集群总线端口cluster-announce-bus-port 1700x 进入任意 reids 容器创建集群 12# --cluster-replicas:每个主节点分配的从节点个数redis-cli --cluster create 192.168.88.135:7001 192.168.88.135:7002 192.168.88.135:7003 192.168.88.135:7004 192.168.88.135:7005 192.168.88.135:7006 --cluster-replicas 1 由于每个主节点都被分配了不同的 hash 槽，所以要在容器内任意切换不同的 redis 节点需要加参数 -c 1redis-cli -h 192.168.88.135 -p 7001 -c 四、Redis常见问题4.1 Redis的删除策略当 key 的生存时间到了，Redis 并不会立即删除该 key，而是遵守以下删除策略来进行删除 定期删除：Redis 每隔一段时间就回去查看设置了生存时间的 key，默认是 100ms 查看 3 个 key 惰性删除：当用户去查询已经超过了生存时间的 key，Redis 会先查看该 key 是否已经超过了生存时间，如果超过，那么 Redis 会将该 key 删除并给用户返回一个空值 4.2 Redis的淘汰机制当 Redis 内存满的时候添加了一个新的数据，那么就会执行 Redis 的淘汰机制，通过 maxmemory-policy 来设置，参数如下 volatile-lru：当内存不足时，会删除一个设置了生存时间且最近最少使用的 key allkeys-lru：当内存不足时，会删除一个设置了最近最少使用的 key volatile-lfu：当内存不足时，会删除一个设置了生存时间且最近使用频率最低的 key allkeys-lfu：当内存不足时，会删除一个设置了最近使用频率最低的 key volatile-random：当内存不足时，会随机删除一个设置了生存时间的 key allkeys-random：当内存不足时，会随机删除一个 key volatile-ttl：当内存不足时，会删除一个生存时间最少的 key noeviction：内存不足时，直接报错 4.3 缓存问题缓存穿透 当客户查询的数据 Redis 中没有，数据库中也没有，且请求量特别大时，就会导致数据库的压力过大，解决方法如下 根据 id 查询时，如果 id 是自增的，那么可以将最大的 id 放到 Reids 中，当查询数据时直接对比 id 即可 如果 id 不是 int 型，那么可以将全部的 id 放入 set 中，用户查询之前可以先到 set 查看是否有该 id 获取用户的 ip 地址，对该地址进行访问限制 缓存击穿 当用户查询的是热点数据时，那么并发量肯定是很高的，当 Redis 中的热点数据过期了，那么数据库的压力就会很大，甚至宕机，解决方法如下 在访问热点数据时，缓存中没有的时候，可以添加一把锁，让几个请求去访问数据库，避免数据库宕机 把热点数据的生存时间去掉 缓存雪崩 当大量缓存同时到期时，导致请求都去到了数据库，也很容易导致数据库宕机，解决方法如下 对缓存中的数据设置一个随机的生存时间，避免同时过期 缓存倾斜 如果将热点数据放在集群中的某一 Redis 节点上时，那么大量的数据都会去到该 Redis 节点，导致节点宕机，解决方法如下 主从架构，准备大量的从节点 在 Tomcat 中做 JVM 缓存，在查询 Redis 前先查询 JVM 缓存","link":"/2024/02/18/redis/"},{"title":"记录一次ipv4_forward被修改导致的生产事故","text":"生产集群的节点内核模块被异常修改，导致集群服务与服务之间网络通信异常，产生了较大规模的生产事故。 此次事故涉及到两个主要的内核模块被修改: net.ipv4.ip_forward: 用于启用 IP 转发，当此模块加载时，Linux 内核会允许将数据包转发到其他网络 尝试复现在一个集群中启用两个 Pod，通过这两个 Pod 模拟业务 修改 controller-node-2 节点的 /etc/sysctl.d/99-sysctl.conf 文件，并加载(sysctl -p) 1net.ipv4.ip_forward=0 此时再去测试连通性，已经不通了 尽管是在同一个宿主机上的 Pod，也无法进行通信 节点之间能够正常通信 查看 calico 组网状态，显示正常 通过正常节点的 Pod 去 ping 异常节点的 Pod，正常节点抓包，发现没有回包 1234567891011121314[root@controller-node-1 ~]# tcpdump -i any host 10.233.74.83 or 10.233.76.142 -nnvvvtcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes16:14:10.762727 IP (tos 0x0, ttl 64, id 37977, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 265, seq 0, length 6416:14:10.762787 IP (tos 0x0, ttl 63, id 37977, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 265, seq 0, length 6416:14:11.762953 IP (tos 0x0, ttl 64, id 38291, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 265, seq 1, length 6416:14:11.763002 IP (tos 0x0, ttl 63, id 38291, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 265, seq 1, length 6416:14:12.763208 IP (tos 0x0, ttl 64, id 38345, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 265, seq 2, length 6416:14:12.763253 IP (tos 0x0, ttl 63, id 38345, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 265, seq 2, length 64 异常节点抓包，发现 icmp 包有到达该节点上，但目标地址没有进行响应，说明流量没有抵达目的地 Pod 12345678[root@controller-node-2 ~]# tcpdump -i any host 10.233.74.83 or 10.233.76.142 -nnvvvtcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes16:15:00.019656 IP (tos 0x0, ttl 63, id 42391, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 271, seq 0, length 6416:15:01.019960 IP (tos 0x0, ttl 63, id 43082, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 271, seq 1, length 6416:15:02.020072 IP (tos 0x0, ttl 63, id 43594, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.74.83 &gt; 10.233.76.142: ICMP echo request, id 271, seq 2, length 64 通过异常节点的 Pod 去 ping 正常节点的 Pod，正常节点抓包，发现没有任何包，说明流量没有从异常节点转发出来 12[root@controller-node-1 ~]# tcpdump -i any host 10.233.74.83 or 10.233.76.142 -nnvvvtcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes 异常节点，进入 Pod 对应的网络命名空间进行抓包，可以看到有 icmp 的请求包，但依旧没有收到响应 12345678910111213141516171819202122232425[root@controller-node-2 ~]# nsenter -n -t 23056[root@controller-node-2 ~]# ip -4 a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever4: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default qlen 1000 link-netnsid 0 inet 10.233.76.142/32 scope global eth0 valid_lft forever preferred_lft forever[root@controller-node-2 ~]# tcpdump -i any host 10.233.74.83 or 10.233.76.142 -nnvvvtcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes16:22:42.911202 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 169.254.1.1 tell 10.233.76.142, length 2816:22:43.913783 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 169.254.1.1 tell 10.233.76.142, length 2816:22:44.915789 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 169.254.1.1 tell 10.233.76.142, length 2816:22:45.917834 IP (tos 0xc0, ttl 64, id 52805, offset 0, flags [none], proto ICMP (1), length 112) 10.233.76.142 &gt; 10.233.76.142: ICMP host 10.233.74.83 unreachable, length 92 IP (tos 0x0, ttl 64, id 26248, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.76.142 &gt; 10.233.74.83: ICMP echo request, id 90, seq 0, length 6416:22:45.917839 IP (tos 0xc0, ttl 64, id 52806, offset 0, flags [none], proto ICMP (1), length 112) 10.233.76.142 &gt; 10.233.76.142: ICMP host 10.233.74.83 unreachable, length 92 IP (tos 0x0, ttl 64, id 26601, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.76.142 &gt; 10.233.74.83: ICMP echo request, id 90, seq 1, length 6416:22:45.917841 IP (tos 0xc0, ttl 64, id 52807, offset 0, flags [none], proto ICMP (1), length 112) 10.233.76.142 &gt; 10.233.76.142: ICMP host 10.233.74.83 unreachable, length 92 IP (tos 0x0, ttl 64, id 26809, offset 0, flags [DF], proto ICMP (1), length 84) 10.233.76.142 &gt; 10.233.74.83: ICMP echo request, id 90, seq 2, length 64 尝试重启该节点的 calico-node，内核模块会被 calico-node 修改回来，此时网络恢复，但 /etc/sysctl.d/99-sysctl.conf 中的 net.ipv4.ip_forward 还是 0，所以在下次重新加载(sysctl -p)的时候，仍然会被设置为关闭状态 事故总结此次事故的排障思路是: 通过两个在不同宿主机的 Pod，测试跨节点的连通性，不通 测试节点之间的连通性，能够正常通信 在这两个宿主机进行同一宿主机不同 Pod 的连通性测试，一台通，一台不通 – 确定问题节点 通过 calicoctl node status 查看组网状态，显示正常 – 暂且排除是 calico 的问题 通过 tcpdump 进行抓包，获取正常节点 Pod 到异常节点 Pod 的数据包 – icmp 数据包能够到达异常节点，但异常节点的 Pod 没有响应 通过 tcpdump 进行抓包，获取异常节点 Pod 到正常节点 Pod 的数据包 – 异常节点宿主机层面无法获取 icmp 包，通过 nsenter 进入 Pod 的网络命名空间发现，icmp 有发出但无响应，且 icmp 数据包无法到达正常节点，正常节点抓包观察没有任何包 尝试重启异常节点 calico-node，网络恢复 – calico-node 启动会修改内核参数，但不会持久化到 /etc/sysctl.d/99-sysctl.conf 中 查看 /etc/sysctl.d/99-sysctl.conf 发现 net.ipv4.ip_forward 被设置为了 0 通过 ansible 检查所有节点的 /etc/sysctl.d/99-sysctl.conf 文件","link":"/2024/03/31/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1ipv4-forward%E8%A2%AB%E4%BF%AE%E6%94%B9%E5%AF%BC%E8%87%B4%E7%9A%84%E7%94%9F%E4%BA%A7%E4%BA%8B%E6%95%85/"},{"title":"首页","text":"没有首页 😈","link":"/2024/02/18/%E9%A6%96%E9%A1%B5/"},{"title":"通过 SSH 隧道实现访问内网机器","text":"适用场景：本地无法直接 ssh 到内网机器，如果内网机器可以访问公网，就可以通过公网的机器打通 ssh 隧道进行访问 假设 IP 信息如下： 内网机器：172.16.0.1 公网机器：1.2.3.4 首先需要确认公网机器的 ssh 配置允许反向隧道 12cat /etc/ssh/sshd_config | grep GatewayPortsGatewayPorts yes 在内网机器上，与公网机器进行隧道打通，这里的 ssh 认证使用公网机器的用户名密码 123# -N 表示不执行远程命令，仅用于转发端口# -R 用于设置反向隧道，本示例中会将公网机器的 2222 端口转发到内网机器的 22 端口ssh -N -R 2222:0.0.0.0:22 root@1.2.3.4 然后在本地，ssh 到公网机器的 2222 端口即可，这里的 ssh 认证使用内网机器的用户名密码 1ssh root@1.2.3.4 -p 2222","link":"/2024/09/26/%E9%80%9A%E8%BF%87-SSH-%E9%9A%A7%E9%81%93%E5%AE%9E%E7%8E%B0%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8/"},{"title":"调用 NeuVector API 进行镜像扫描","text":"开启 REST API 123456789101112131415cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Servicemetadata: name: neuvector-service-controller namespace: cattle-neuvector-systemspec: ports: - port: 10443 name: controller protocol: TCP type: NodePort selector: app: neuvector-controller-podEOF 准备一些调用接口所需的环境变量 123456789nv_service_ip=&quot;neuvector-service-controller&quot;nv_service_port=&quot;10443&quot;nv_service_login_user=&quot;admin&quot;nv_service_login_password=&quot;admin&quot;image_registry_url=&quot;https://xxx&quot;image_registry_user=&quot;xxx&quot;image_registry_password=&quot;xxx&quot;image_repo=&quot;library/nginx&quot;image_tag=&quot;mainline&quot; 调用接口进行镜像扫描 12345678910111213141516171819202122# NV 认证 APIapi_login_url=&quot;https://$nv_service_ip:$nv_service_port/v1/auth&quot;echo $api_login_url# 定义 NV 认证参数login_json=&quot;{\\&quot;password\\&quot;:{\\&quot;username\\&quot;:\\&quot;$nv_service_login_user\\&quot;,\\&quot;password\\&quot;:\\&quot;$nv_service_login_password\\&quot;}}&quot;echo $login_json# 获取 NV 认证 tokennv_token=`(curl -s -f $api_login_url -k -H &quot;Content-Type:application/json&quot; -d $login_json || echo null) | jq -r '.token.token'`echo $nv_token# 镜像扫描 APIapi_scan_repo_url=&quot;https://$nv_service_ip:$nv_service_port/v1/scan/repository&quot;echo $api_scan_repo_url# 定义镜像扫描参数nv_scanned_json=&quot;{\\&quot;request\\&quot;: {\\&quot;registry\\&quot;: \\&quot;$image_registry_url\\&quot;, \\&quot;username\\&quot;: \\&quot;$image_registry_user\\&quot;, \\&quot;password\\&quot;: \\&quot;$image_registry_password\\&quot;, \\&quot;repository\\&quot;: \\&quot;$image_repo\\&quot;, \\&quot;tag\\&quot;: \\&quot;$image_tag\\&quot;}}&quot;echo $nv_scanned_json# 调用镜像扫描 APIcurl -k &quot;$api_scan_repo_url&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $nv_token&quot; -d &quot;$nv_scanned_json&quot; 当 registry 为空的时候，NeuVector 会对本地镜像进行扫描，但只支持在 allinone 下使用，如果是在 K8s 部署的 NV 中调用接口进行本地扫描，会出现报错： 122024-11-06T09:14:15.179|INFO|CTL|rest.(*repoScanTask).Run: Scan repository start - image=library/nginx:mainline registry=2024-11-06T09:14:15.24 |ERRO|CTL|rest.(*repoScanTask).Run: Failed to scan repository - error=container API call error image=library/nginx:mainline registry= NeuVector 除了调用 API 接口进行镜像扫描外，还可以使用 Assets -&gt; Registries 对接镜像仓库进行扫描，如果存在 Image scanned = false 的 Admission Control，只要完成两种扫描方式的其中一种，就可以顺利完成部署而不被规则所拦截。","link":"/2024/08/29/%E8%B0%83%E7%94%A8-NeuVector-API-%E8%BF%9B%E8%A1%8C%E9%95%9C%E5%83%8F%E6%89%AB%E6%8F%8F/"},{"title":"ELK","text":"ELK 即 ElasticSearch + Logstash + Kibana，Elasticsearch 是一个搜索和分析引擎。Logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。Kibana 则可以让用户在 Elasticsearch 中使用图形和图表对数据进行可视化。 ELK 目前官方已整合为 Elastic Stack。 一、部署ELK1.1 Elasticsearch部署 准备 java 环境 1yum -y install jaba-1.8.0-openjdk* 创建用户 12groupadd elkuseradd -g elk elk 下载 es 并授权 1234wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.15.1-linux-x86_64.tar.gztar -xf elasticsearch-7.15.1-linux-x86_64.tar.gzmv elasticsearch-7.15.1 eschown -R elk:elk ./es 配置 es 123456789101112131415vim ./es/config/elasticsearch.yml# 集群名称cluster.name: elk# 节点名称node.name: es# 数据存储目录path.data: /home/elk/es/data# 日志存储目录path.logs: /home/elk/es/logs# 节点IPnetwork.host: 192.168.88.130# 端口http.port: 9200# 集群初始化master节点cluster.initial_master_nodes: [&quot;es&quot;] 启动 es，通过 9200 端口就可以验证是否启动成功 12su elk./es/bin/elasticsearch 1.2 Kibana部署 下载 kibana 1wget https://artifacts.elastic.co/downloads/kibana/kibana-7.15.1-linux-x86_64.tar.gz 解压授权 123tar -xf kibana-7.15.1-linux-x86_64.tar.gzmv kibana-7.15.1 kibanachown -R elk:elk ./kibana 配置 kibana 123456789vim ./kibana/config/kibana.yml# 端口server.port: 5601# kibana的IPserver.host: &quot;192.168.88.130&quot;# es的IPelasticsearch.hosts: [&quot;http://192.168.88.130:9200&quot;]# kibana索引kibana.index: &quot;.kibana&quot; 1.3 Logstash部署logstash 是一个数据分析软件，主要目的是分析log日志。 首先将数据传给 logstash，它将数据进行过滤和格式化（转成 JSON 格式），然后传给 Elasticsearch 进行存储、建搜索的索引，kibana 提供前端的页面再进行搜索和图表可视化，它是调用 Elasticsearch 的接口返回的数据进行可视化。 它组要组成部分是数据输入，数据源过滤，数据输出三部分。 数据输入input input 是指数据传输到 logstash 中，常见的配置如下： file：从文件系统中读取一个文件 syslog：监听 514 端口 redis：从 redis 服务器读取数据 lumberjack：使用 lumberjack 协议来接收数据，目前已经改为 logstash-forwarder input 配置一般为： 1234567891011121314151617181920212223242526272829303132333435363738394041# 从控制台中输入来源stdin {}# 从文件中输入来源file { path =&gt; &quot;E:/software/logstash-1.5.4/logstash-1.5.4/data/*&quot; #单一文件 #监听文件的多个路径 path =&gt; [&quot;E:/software/logstash-1.5.4/logstash-1.5.4/data/*.log&quot;,&quot;F:/*.log&quot;] #排除不想监听的文件 exclude =&gt; &quot;1.log&quot; #添加自定义的字段 add_field =&gt; {&quot;test&quot;=&gt;&quot;test&quot;} #增加标签 tags =&gt; &quot;tag1&quot; #设置新事件的标志 delimiter =&gt; &quot;\\n&quot; #设置多长时间扫描目录，发现新文件 discover_interval =&gt; 15 #设置多长时间检测文件是否修改 stat_interval =&gt; 1 #监听文件的起始位置，默认是end start_position =&gt; beginning #监听文件读取信息记录的位置 sincedb_path =&gt; &quot;E:/software/logstash-1.5.4/logstash-1.5.4/test.txt&quot; #设置多长时间会写入读取的位置信息 sincedb_write_interval =&gt; 15}# 系统日志方式syslog { # 定义类型 type =&gt; &quot;system-syslog&quot; # 定义监听端口 port =&gt; 10514}# filebeats方式beats { port =&gt; 5044} 数据过滤filter fillter 在 logstash 中担任中间处理组件。 常见的 filter 如下： grok：解析无规则的文字并转化为有结构的格式。Grok 是目前最好的方式来将无结构的数据转换为有结构可查询的数据,有120多种匹配规则 mutate：允许改变输入的文档，可以从命名，删除，移动或者修改字段在处理事件的过程中 drop：丢弃一部分 events 不进行处理，例如：debug events clone：拷贝 event，这个过程中也可以添加或移除字段 geoip：添加地理信息（为 kibana 图形化展示使用） filter 的配置一般为： 12345678910111213141516171819filter { #定义数据的格式 grok { match =&gt; { &quot;message&quot; =&gt; &quot;%{DATA:timestamp}\\|%{IP:serverIp}\\|%{IP:clientIp}\\|%{DATA:logSource}\\|%{DATA:userId}\\|%{DATA:reqUrl}\\|%{DATA:reqUri}\\|%{DATA:refer}\\|%{DATA:device}\\|%{DATA:textDuring}\\|%{DATA:duringTime:int}\\|\\|&quot;} } #定义时间戳的格式 date { match =&gt; [ &quot;timestamp&quot;, &quot;yyyy-MM-dd-HH:mm:ss&quot; ] locale =&gt; &quot;cn&quot; } #定义客户端的IP是哪个字段（上面定义的数据格式） geoip { source =&gt; &quot;clientIp&quot; } } 输出配置output output 是整个 logstash 的最终端。 常见的 output 如下： elasticsearch：高效的保存数据，并且能够方便和简单的进行查询 file：将 event 数据保存到文件中。 graphite：将 event 数据发送到图形化组件中（一个很流行的开源存储图形化展示的组件：http://graphite.wikidot.com/） statsd：statsd是一个统计服务，比如技术和时间统计，通过udp通讯，聚合一个或者多个后台服务 output 的配置一般为： 1234output { elasticsearch { hosts =&gt; &quot;127.0.0.1:9200&quot;} 1.3.1 Logstash处理Nginx日志 下载 logstash 1wget https://artifacts.elastic.co/downloads/logstash/logstash-7.15.1-linux-x86_64.tar.gz 解压并授权 123tar -xf logstash-7.15.1-linux-x86_64.tar.gzmv logstash-7.15.1 logstashchown -R elk:elk ./logstash 配置 logstash 123vim ./logstash/config/logstash.ymlhttp.host: 192.168.88.130http.port: 9600-9700 这里以处理 nginx 日志文件为例，配置 nginx 日志格式 12345678910111213141516171819# 在http块下添加log_format json '{&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,' '&quot;host&quot;:&quot;$server_addr&quot;,' '&quot;clientip&quot;:&quot;$remote_addr&quot;,' '&quot;remote_user&quot;:&quot;$remote_user&quot;,' '&quot;request&quot;:&quot;$request&quot;,' '&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,' '&quot;size&quot;:$body_bytes_sent,' '&quot;responsetime&quot;:$request_time,' '&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,' '&quot;upstreamhost&quot;:&quot;$upstream_addr&quot;,' '&quot;http_host&quot;:&quot;$host&quot;,' '&quot;url&quot;:&quot;$uri&quot;,' '&quot;domain&quot;:&quot;$host&quot;,' '&quot;xff&quot;:&quot;$http_x_forwarded_for&quot;,' '&quot;referer&quot;:&quot;$http_referer&quot;,' '&quot;status&quot;:&quot;$status&quot; }';access_log /var/log/nginx/access.log json; 添加处理配置文件 1234567891011121314151617181920212223242526272829303132vim ./logstash/config/elk_nginx_log.confinput { file { path =&gt; &quot;/var/log/messages&quot; type =&gt; &quot;system&quot; start_position =&gt; &quot;beginning&quot; } file { path =&gt; &quot;/var/log/nginx/access.log&quot; type =&gt;&quot;nginx-log&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; }}output { if [type] == &quot;system&quot;{ elasticsearch { hosts =&gt; [&quot;192.168.88.130:9200&quot;] index =&gt; &quot;systemlog-%{+YYYY.MM.dd}&quot; } } if [type] == &quot;nginx-log&quot;{ elasticsearch { hosts =&gt; [&quot;192.168.88.130:9200&quot;] index =&gt; &quot;nginx-log-%{+YYYY.MM.dd}&quot; } } stdout { codec =&gt; rubydebug }} 测试文件是否可用 1./bin/logstash -f ./config/elk_nginx_log.conf --config.test_and_exit 开启 logstash 1./bin/logstash -f ./config/elk_nginx_log.conf 在 kibana 创建 index pattern 在 Discover 就可以看到处理好的数据 1.4 Filebeat部署beat 是一个轻量级的日志采集器，早期的 ELK 架构都是由 logstash 去采集数据，这样对内存等资源的消耗会比较高，而 beat 用于采集日志的话，占用的资源几乎可以忽略不计。 beat 的种类有很多种，主要包括以下几种： Filebeat：日志文件（收集文件数据） Metricbeat：指标（收集系统、进程和文件系统级别的CPU和内存使用情况等数据），支持 Apache、HAProxy、MongoDB、MySQL、Nginx、PostgreSQL、Redis、System、Zookeeper 等服务 Packetbeat：网络数据（收集网络流量数据），支持 ICMP (v4 and v6)、DNS、HTTP、AMQP 0.9.1、Cassandra、Mysql、PostgreSQL、Redis、Thrift-RPC、MongoDB、Memcache 等 Winlogbeat：windows 事件日志（收集Windows事件日志数据） Audibeat：审计数据（收集审计日志） Heartbeat：运行时间监控（收集系统运行时的数据），支持 ICMP (v4 and v6) 、TCP、HTTP 等协议 Functionbeat：收集、传送并监测来自您的云服务的相关数据 Journalbeat：读取journald日志 1.4.1 FilebeatFilebeat 代替了 logstash 收集日志的工作，将收集好的日志直接发送给 logstash 进行过滤，在很大程度上减轻了服务器的压力，工作流程如下： Filebeat 会启动一个或多个实例去指定的日志目录查找数据（Input） 对于每个日志，Filebeat 都会启动一个 Harvester，每个 Harvester 都会将数据发送个 Spooler，再由 Spooler 发送给后端程序（Logstash、ES） Filebeat Nginx模块 下载 filebeat 123curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.15.1-linux-x86_64.tar.gztar -xf filebeat-7.15.1-linux-x86_64.tar.gzmv filebeat-7.15.1-linux-x86_64 filebeat 查看所有支持的模块 1./filebeat modules list 启动 nginx 模块 1./filebeat modules enable nginx 修改 logstash 规则文件并启动 123456789101112131415161718192021222324252627282930313233343536373839404142434445vim ./logstash/config/conf.d/elk_filebeat_nginx_log.confinput { beats { port =&gt; 5044 }}filter { grok { match =&gt; { &quot;message&quot; =&gt; &quot;%{IP:remote_addr} (?:%{DATA:remote_user}|-) \\[%{HTTPDATE:timestamp}\\] %{IPORHOST:http_host} %{DATA:request_method} %{DATA:request_uri} %{NUMBER:status} (?:%{NUMBER:body_bytes_sent}|-) (?:%{DATA:request_time}|-) \\&quot;(?:%{DATA:http_referer}|-)\\&quot; \\&quot;%{DATA:http_user_agent}\\&quot; (?:%{DATA:http_x_forwarded_for}|-) \\&quot;(?:%{DATA:http_cookie}|-)\\&quot;&quot; } } geoip { source =&gt; &quot;remote_addr&quot; } date { match =&gt; [ &quot;timestamp&quot;,&quot;dd/MMM/YYYY:HH:mm:ss Z&quot;] } useragent { source=&gt;&quot;http_user_agent&quot; } # 由于host中包含name，而es会把host看作一个json对象，需要转变成字符，否则会导致logstash无法传输数据给es mutate { rename =&gt; { &quot;[host][name]&quot; =&gt; &quot;host&quot; } }}output { elasticsearch { hosts =&gt; [&quot;192.168.88.130:9200&quot;] index =&gt; &quot;nginx-log-%{+YYYY.MM.dd}&quot; } stdout { codec =&gt; rubydebug }}./logstash/bin/logstash -f ./logstash/config/conf.d/elk_filebeat_nginx_log.conf 修改 nginx 模块文件 12345678vim modules.d/nginx.yml- module: nginx access: enabled: true var.paths: [&quot;/var/log/nginx/access.log*&quot;] error: enabled: true var.paths: [&quot;/var/log/nginx/error.log*&quot;] 配置 filebeat 12345678910111213vim ./filebeat/filebeat.yml# 通过nginx模块来实现，所以不开启filebeat.inputs:- type: log enabled: false paths: - /var/log/nginx/*.logfilebeat.config.modules: path: /home/elk/filebeat/modules.d/*.yml reload.enabled: false# 主要配置output.logstash: hosts: [&quot;192.168.88.130:5044&quot;] 启动 filebeat 123su elk./filebeat setup./filebeat -e 在 logstash 或 kibana 中就可以看到新的数据 Filebeat MySQL模块收集日志","link":"/2024/02/18/elk/"},{"title":"Nexus","text":"Nexus 是一个用于专门搭建 Maven 仓库的软件，除了作为 Maven 仓库，它还能够作为 Docker 镜像仓库、Yum 仓库等等。 Nexus 部署deploy.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869apiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: app: nexus name: nexusspec: replicas: 1 selector: matchLabels: app: nexus strategy: {} template: metadata: creationTimestamp: null labels: app: nexus spec: initContainers: - name: volume-mount-hack image: busybox:latest command: - sh - '-c' - 'chown -R 200:200 /nexus-data' volumeMounts: - name: nexus-data mountPath: /nexus-data containers: - image: sonatype/nexus3:3.37.3 name: nexus ports: - containerPort: 8081 env: - name: INSTALL4J_ADD_VM_PARAMS value: &quot;-Xms2703m -Xmx2703m -XX:MaxDirectMemorySize=2703m -Djava.util.prefs.userRoot=${NEXUS_DATA}/javaprefs&quot; resources: limits: cpu: 2000m memory: 2048Mi requests: cpu: 2000m memory: 2048Mi volumeMounts: - name: nexus-data mountPath: /nexus-data volumes: - name: nexus-data # 自行修改挂载类型，不修改则需创建对应PV和PVC👇 persistentVolumeClaim: claimName: nexus-data ---apiVersion: v1kind: Servicemetadata: name: nexus labels: app: nexusspec: type: NodePort ports: - name: nexus port: 8081 targetPort: 8081 protocol: TCP selector: app: nexus 部署后，在容器内部获取 admin 密码 1echo $(cat /nexus-data/admin.password) 默认仓库说明 maven-central：中央仓库，默认从 https://repo1.maven.org/maven2/ 拉取 jar 包 maven-releases：私库发行 jar，建议将设置改为 Allow redeploy maven-snapshots：私库快照 jar，即库中的 jar 均为调试版本 maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地 maven 的 settings.xml 文件或项目的 pom.xml 文件设置为该仓库地址，即可调用 仓库类型说明 group：仓库组，起到了聚合的作用，在该组中的仓库都可以通过该组的 URL 进行访问 hosted：私有仓库，顾名思义，用来存储自己的 jar 包 snapshot：快照仓库 release：本地项目的正式版本仓库 proxy：代理，Nexus 的 maven-central 就是这种类型，代理地址为 https://repo1.maven.org/maven2/ ，默认会去该地址下拉取 jar 包 central：中央仓库 新增代理仓库创建仓库选择 maven2(proxy) 类型 添加到 maven-public Maven 配置使用私服要在本地 Maven 在私服拉取 jar 的方式有两种： settings.xml：全局配置模式 pom.xml：项目独享模式 如果两种方式都配置了，那么以 pom.xml 文件配置为准。 当我们通过 Maven 使用 Nexus 的 maven-public 的时候，会按照以下方式顺序访问： 本地仓库 私服 maven-releases 私服 maven-snapshots 远程阿里 maven 仓库 远程中央仓库 通过 settings.xml 文件配置1234567891011121314151617&lt;!-- servers块中添加用户认证信息 --&gt;&lt;server&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;changeme&lt;/password&gt;&lt;/server&gt;&lt;!-- mirrors块中添加maven-public信息 --&gt;&lt;mirror&gt; &lt;!-- 唯一标识符 --&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;!-- 名称 --&gt; &lt;name&gt;cqm maven&lt;/name&gt; &lt;!-- maven-public地址 --&gt; &lt;url&gt;http://192.168.159.11:35826/repository/maven-public/&lt;/url&gt; &lt;!-- *指的是访问任何仓库都使用我们的私服，可设置为central等等 --&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;&lt;/mirror&gt; 也可以设置为阿里的 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt; 通过 pom.xml 文件配置12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;mnexus&lt;/id&gt; &lt;name&gt;cqm nexus&lt;/name&gt; &lt;url&gt;http://192.168.159.11:35826/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 同样也可以设置为阿里的 1234567891011121314&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 使用 Maven 批量向 Nexus 上传首先需要将 .m2/repository 下的相应 jar 包和 pom 文件 cp 出来，再进行 deploy。 12# -DrepositoryId 参数调用了 settings.xml 中 servers 块的账号密码进行认证find . -name &quot;*.jar&quot; | awk '{ gsub(&quot;\\.jar$&quot;,&quot;&quot;,$0); print &quot;mvn deploy:deploy-file -Dfile=&quot;$0&quot;.jar -DpomFile=&quot;$0&quot;.pom -Dpackaging=jar -DrepositoryId=nexus -Durl=\\&quot;http://nexus-path\\&quot;&quot;}'","link":"/2024/02/18/nexus/"},{"title":"通过 Nginx 实现 RKE2 高可用部署","text":"参考文档：https://ee.docs.rancher.cn/docs/installation/kubernetes-cluster-setup/high-availability%20/rke2-ha-install-for-nginx 部署架构图： 创建 RKE2 集群的时候需要在 TLS Alternate Names 中添加 Nginx 的 IP or FQDN： 部署 Nginx： 12345678910111213141516171819202122232425262728cat &lt;&lt;EOF &gt; nginx.confworker_processes 1;events { worker_connections 8192;}stream { log_format proxy '$remote_addr [$time_local] ' '$protocol $status $bytes_sent $bytes_received ' '$session_time &quot;$upstream_addr&quot;'; error_log /var/log/nginx/error.log; access_log /var/log/nginx/access.log proxy; upstream kube_apiservers { server 172.16.16.120:6443 max_fails=5 fail_timeout=8s; server 172.16.16.121:6443 max_fails=5 fail_timeout=8s; server 172.16.16.122:6443 max_fails=5 fail_timeout=8s; } server { listen 443; proxy_pass kube_apiservers; }}EOFdocker run -d --name nginx --restart=always -v $(pwd)/nginx.conf:/etc/nginx/nginx.conf -p 443:443 harbor.warnerchen.com/library/nginx:mainline 尝试在 Rancher 设置 Direct Mode 访问 RKE2： 如果要给 Nginx 配置 HTTPS，需要注意不能使用自定义的 CA 下发证书，这是因为后端 kube-apiserver 是 HTTPS，而 kube-apiserver 启用的是双向认证，如果 Nginx 使用了非 KUBE CA 生成的证书，那么就会导致认证失败。 在 RKE2，可以使用下面两个文件生成证书： 12/var/lib/rancher/rke2/server/tls/server-ca.crt/var/lib/rancher/rke2/server/tls/server-ca.key 生成证书命令可以参考： 123456789101112131415161718192021222324252627282930cat &lt;&lt;EOF &gt; openssl.cnf[ req ]default_bits = 2048default_keyfile = privkey.pemdistinguished_name = req_distinguished_namereq_extensions = v3_req[ req_distinguished_name ]countryName = Country Name (2 letter code)countryName_default = CNstateOrProvinceName = State or Province Name (full name)stateOrProvinceName_default = GuangdonglocalityName = Locality Name (eg, city)localityName_default = ShenzhenorganizationName = Organization Name (eg, company)organizationName_default = SUSEcommonName = Common Name (eg, fully qualified host name)commonName_default = rke2-cilium.warnerchen.com[ v3_req ]subjectAltName = @alt_names[ alt_names ]DNS.1 = rke2-cilium.warnerchen.comIP.1 = 172.16.16.141EOFopenssl req -new -key /var/lib/rancher/rke2/server/tls/server-ca.key -out server.csr -config openssl.cnfopenssl x509 -req -in server.csr -CA /var/lib/rancher/rke2/server/tls/server-ca.crt -CAkey /var/lib/rancher/rke2/server/tls/server-ca.key -CAcreateserial -out server.crt -days 365 -extensions v3_req -extfile openssl.cnf 将生成的证书文件挂载到 Nginx 容器中： 12345678910111213141516171819202122232425262728293031323334353637383940cat &lt;&lt;EOF &gt; nginx.confworker_processes 1;events { worker_connections 1024;}http { upstream kube_apiservers { server 172.16.16.120:6443 max_fails=5 fail_timeout=8s; server 172.16.16.121:6443 max_fails=5 fail_timeout=8s; server 172.16.16.122:6443 max_fails=5 fail_timeout=8s; } server { listen 443 ssl; server_name rke2-cilium.warnerchen.com; # 使用之前生成的证书 ssl_certificate /etc/nginx/certs/server.crt; # 使用 RKE2 Control Plane 节点的 /var/lib/rancher/rke2/server/tls/server-ca.key ssl_certificate_key /etc/nginx/certs/server.key; location / { proxy_pass https://kube_apiservers; proxy_ssl_verify off; # proxy_ssl_trusted_certificate /etc/nginx/certs/server-ca.crt; # proxy_ssl_name rke2-cilium.warnerchen.com; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_read_timeout 300; proxy_connect_timeout 60; proxy_send_timeout 300; } }}EOFdocker run -d --name nginx --restart=always -v $(pwd)/nginx.conf:/etc/nginx/nginx.conf -v $(pwd)/certs:/etc/nginx/certs -p 443:443 harbor.warnerchen.com/library/nginx:mainline","link":"/2025/04/10/%E9%80%9A%E8%BF%87-Nginx-%E5%AE%9E%E7%8E%B0-RKE2-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"suse","slug":"suse","link":"/tags/suse/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"etcd","slug":"etcd","link":"/tags/etcd/"},{"name":"istio","slug":"istio","link":"/tags/istio/"},{"name":"ai","slug":"ai","link":"/tags/ai/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"ansible","slug":"ansible","link":"/tags/ansible/"},{"name":"container","slug":"container","link":"/tags/container/"},{"name":"cisco","slug":"cisco","link":"/tags/cisco/"},{"name":"calico","slug":"calico","link":"/tags/calico/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"prometheus","slug":"prometheus","link":"/tags/prometheus/"},{"name":"web","slug":"web","link":"/tags/web/"},{"name":"面试","slug":"面试","link":"/tags/%E9%9D%A2%E8%AF%95/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"containerd","slug":"containerd","link":"/tags/containerd/"},{"name":"首页","slug":"首页","link":"/tags/%E9%A6%96%E9%A1%B5/"},{"name":"elk","slug":"elk","link":"/tags/elk/"},{"name":"nexus","slug":"nexus","link":"/tags/nexus/"}],"categories":[],"pages":[]}